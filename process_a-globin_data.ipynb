{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import feather\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"/proj/yunligrp/users/minzhi/custom_lib\" not in sys.path:\n",
    "    sys.path.insert(0, \"/proj/yunligrp/users/minzhi/custom_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_process_data_eqtl import *\n",
    "from function_asso import *\n",
    "from function_mesa_cca import *\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Freeze8 Kinship Matrix for Later Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 5min 14s, total: 6min 44s\n",
      "Wall time: 10min 38s\n"
     ]
    }
   ],
   "source": [
    "predata_dir = os.path.join(\"..\", \"prepro_data\", \"kinship\")\n",
    "freeze8_kinship_filename = \"freeze8_kinship.feather\"\n",
    "freeze8_kinship_dir_filename = os.path.join(predata_dir, freeze8_kinship_filename)\n",
    "%time freeze8_kinship_df = pd.read_feather(freeze8_kinship_dir_filename, use_threads = True)\n",
    "freeze8_sample_list = list(freeze8_kinship_df)\n",
    "freeze8_sample_df = pd.DataFrame(data=freeze8_sample_list, columns=[\"NWDID\"])\n",
    "freeze8_kinship_df_ID = pd.concat(objs=[freeze8_sample_df, freeze8_kinship_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan All Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_common_df(common_0, common_1, common_col, save_dir, file_type):\n",
    "    common_df_list = [common_0, common_1]\n",
    "    common_df = merge_df_list(common_df_list, common_col, merge_method='first')\n",
    "    common_df_filename = \"common_%s.tsv\"%file_type\n",
    "    common_df_dir_filename = os.path.join(save_dir, common_df_filename)\n",
    "    common_df.to_csv(common_df_dir_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_snp_pc_cn_pheno_kinship(cohort, freeze_ver, pc_num, common_col, snp_list, kinship_df_ID, cn_var = \"cn\", pheno_filename = None):\n",
    "    if pheno_filename == None:\n",
    "        pheno_filename = \"pheno_%s.tsv\"%cohort\n",
    "    save_dir = os.path.join(\"..\", \"cohort\", cohort, \"ready_data\")\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok = True) \n",
    "    snp_dir_filename_list = [os.path.join(\"..\", \"prepro_data\", \"snp\", \"%s_%s_hetero.tsv\"%(freeze_ver, snp_i)) for snp_i in snp_list]\n",
    "    pc_dir_filename = os.path.join(\"..\", \"prepro_data\", \"pc\", \"%s_pc%d_pcair.tsv\"%(freeze_ver, pc_num))\n",
    "    cn_dir_filename = os.path.join(\"..\", \"cohort\", cohort, cn_var, \"%s_%s.tsv\"%(cohort, cn_var))\n",
    "    pheno_dir_filename = os.path.join(\"..\", \"cohort\", cohort, \"pre_data\", pheno_filename)\n",
    "    kinship_dir_filename = os.path.join(\"..\", \"prepro_data\", \"kinship\", \"%s_kinship_sample.tsv\"%freeze_ver)\n",
    "    snp_pc_cn_dir_filename_list = snp_dir_filename_list + [pc_dir_filename, cn_dir_filename, pheno_dir_filename, kinship_dir_filename]\n",
    "    \n",
    "    snp_pc_cn_df = read2df_list(snp_pc_cn_dir_filename_list)\n",
    "    common_snp_0 = merge_df_list(snp_pc_cn_df, common_col, merge_method='first', how = 'inner')\n",
    "    \n",
    "    common_snp_0_filename = \"common_%s.tsv\"%snp_list[0]\n",
    "    common_snp_0_dir_filename = os.path.join(save_dir, common_snp_0_filename)\n",
    "    common_snp_0.to_csv(common_snp_0_dir_filename, sep=\"\\t\", index=False)\n",
    "    \n",
    "    sample_df = common_snp_0[[\"NWDID\"]]\n",
    "    sample_filename = \"common_sample.tsv\"\n",
    "    sample_dir_filename = os.path.join(save_dir, sample_filename)\n",
    "    sample_df.to_csv(sample_dir_filename, sep=\"\\t\", index=False)\n",
    "    \n",
    "    snp_num = len(snp_list)\n",
    "    for snp_idx in range(1, snp_num):\n",
    "        common_0 = snp_pc_cn_df[snp_idx]\n",
    "        common_1 = common_snp_0\n",
    "        file_type = snp_list[snp_idx]\n",
    "        save_common_df(common_0, common_1, common_col, save_dir, file_type)\n",
    "    \n",
    "    save_common_df(snp_pc_cn_df[snp_num], common_snp_0, common_col, save_dir, \"pc\")\n",
    "    save_common_df(snp_pc_cn_df[snp_num + 1], common_snp_0, common_col, save_dir, cn_var)\n",
    "    save_common_df(snp_pc_cn_df[snp_num + 2], common_snp_0, common_col, save_dir, \"pheno\")\n",
    "    if cn_var == \"cn\":\n",
    "        common_del = cn2del(snp_pc_cn_df[snp_num + 1])\n",
    "        save_common_df(common_del, common_snp_0, common_col, save_dir, \"del\")\n",
    "    \n",
    "    kinship_sample_selected = kinship_select_sample(kinship_df_ID, sample_df)\n",
    "    print(kinship_sample_selected.shape)\n",
    "    common_kinship_filename_prefix = \"common_kinship\"\n",
    "    save_kinship_simple(save_dir, common_kinship_filename_prefix, kinship_sample_selected, sample_col = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_df(df, cat_col):\n",
    "    df_raw = df.copy()\n",
    "    df_raw[cat_col].fillna(\"Missing Values\", inplace = True) \n",
    "    category = list(set(df_raw[cat_col].values.tolist()))\n",
    "    cat_num_list = []\n",
    "    for cat_i in category:\n",
    "        tmp_cat_df = df.loc[df_raw[cat_col] == cat_i, :]\n",
    "        tmp_cat_num = tmp_cat_df.shape[0]\n",
    "        cat_num_list.append(tmp_cat_num)\n",
    "    category_summary_tuple = list(zip(category, cat_num_list))\n",
    "    category_summary_df = pd.DataFrame(category_summary_tuple, columns = [cat_col, \"sample_size\"])\n",
    "    category_summary_df.sort_values(axis = 0, by = cat_col, inplace = True)\n",
    "    return category_summary_df, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genxgen(gen_1, gen_2, gen_1_name, gen_2_name, common_col, save_dir):\n",
    "#     gen_gen = pd.concat(objs=[gen_1, gen_2[[gen_2_name]]], axis=1)\n",
    "    gen_gen = gen_1.merge(gen_2, on = common_col, how = \"inner\")\n",
    "    gen_gen[\"%s-%s\"%(gen_1_name, gen_2_name)] = gen_gen[gen_1_name] * gen_gen[gen_2_name]\n",
    "    rearrange_list = [list(gen_gen)[0]] + [\"%s-%s\"%(gen_1_name, gen_2_name), gen_1_name, gen_2_name]\n",
    "    gen_gen_rearrange = gen_gen.copy()\n",
    "    gen_gen_rearrange = gen_gen[rearrange_list]\n",
    "    gen_gen_rearrange_filename = \"common_%s-%s.tsv\"%(gen_1_name, gen_2_name)\n",
    "    gen_gen_rearrange_dir_filename = os.path.join(save_dir, gen_gen_rearrange_filename)\n",
    "    gen_gen_rearrange.to_csv(gen_gen_rearrange_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return gen_gen_rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table1(cohort, cn_var):\n",
    "    cn_orig_filename = \"common_%s.tsv\"%cn_var\n",
    "    cn_orig_dir = os.path.join(\"..\", \"cohort\", cohort, \"ready_data\")\n",
    "    cn_orig_dir_filename = os.path.join(cn_orig_dir, cn_orig_filename)\n",
    "    cn_orig_df = pd.read_csv(cn_orig_dir_filename, sep=\"\\t\")\n",
    "    if cn_var == \"cn\":\n",
    "        cn_binary_df_full = cn2binary_df(cn_orig_df)\n",
    "        cn_binary_df = cn_binary_df_full[[\"NWDID\", \"cn0\", \"cn1\", \"cn3\", \"cn4\"]]\n",
    "        cn_type_list = [\"cn0\", \"cn1\", \"cn3\"]\n",
    "        for cn_type in cn_type_list:\n",
    "            cn_binary_df_move = move_cn(cn_binary_df, cn_type)\n",
    "            cn_binary_df_move_filename = \"common_cn-binary_%s.tsv\"%cn_type\n",
    "            cn_binary_df_move_dir_filename = os.path.join(cn_orig_dir, cn_binary_df_move_filename)\n",
    "            cn_binary_df_move.to_csv(cn_binary_df_move_dir_filename, sep=\"\\t\", index=False)\n",
    "    # to do: add the case of cn_var == \"status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table2(cohort, gen_name):\n",
    "#     Load del\n",
    "    load_dir = os.path.join(\"..\", \"cohort\", cohort, \"ready_data\")\n",
    "    save_dir = load_dir\n",
    "    del_cn_filename = \"common_del.tsv\"\n",
    "    del_cn_dir_filename = os.path.join(load_dir, del_cn_filename)\n",
    "    del_cn = pd.read_csv(del_cn_dir_filename, sep=\"\\t\", header = 0, index_col = None)\n",
    "    \n",
    "#     del x gen\n",
    "    gen_filename = \"common_%s.tsv\"%gen_name\n",
    "    gen_dir_filename = os.path.join(load_dir, gen_filename)\n",
    "    gen = pd.read_csv(gen_dir_filename, sep=\"\\t\")\n",
    "    common_col = \"NWDID\"\n",
    "    del_gen = genxgen(del_cn, gen, \"del\", gen_name, common_col, save_dir)\n",
    "    \n",
    "#     Stratify gen by # del\n",
    "    del_val_tuple = ([0], [1], [2])\n",
    "    var_name = \"del\"\n",
    "    for del_val_list in del_val_tuple:\n",
    "        strati_gen(del_gen, del_val_list, var_name, gen_name, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strati_gen(var_gen, var_val_list, var_name, gen_name, save_dir):\n",
    "    gen_var_val = var_gen.loc[var_gen.loc[:, var_name].isin(var_val_list), [\"NWDID\", gen_name]]\n",
    "    var_val_name = \"\".join(map(str, var_val_list))\n",
    "    gen_var_val_filename = \"common_%s_%s%s.tsv\"%(gen_name, var_name, var_val_name)\n",
    "    gen_var_val_dir_filename = os.path.join(save_dir, gen_var_val_filename)\n",
    "    gen_var_val.to_csv(gen_var_val_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table3(cohort, gen_name, table3_del_val_tuple, table3_cn_val_tuple):\n",
    "#     Load del and cn\n",
    "    load_dir = os.path.join(\"..\", \"cohort\", cohort, \"ready_data\")\n",
    "    save_dir = load_dir\n",
    "    del_cn_filename = \"common_del.tsv\"\n",
    "    del_cn_dir_filename = os.path.join(load_dir, del_cn_filename)\n",
    "    del_cn = pd.read_csv(del_cn_dir_filename, sep=\"\\t\", header = 0, index_col = None)\n",
    "    \n",
    "    cn_filename = \"common_cn.tsv\"\n",
    "    cn_dir_filename = os.path.join(load_dir, cn_filename)\n",
    "    cn = pd.read_csv(cn_dir_filename, sep=\"\\t\", header = 0, index_col = None)\n",
    "    \n",
    "#     cn x gen, del x gen\n",
    "    gen_filename = \"common_%s.tsv\"%gen_name\n",
    "    gen_dir_filename = os.path.join(load_dir, gen_filename)\n",
    "    gen = pd.read_csv(gen_dir_filename, sep=\"\\t\")\n",
    "    common_col = \"NWDID\"\n",
    "    cn_gen = genxgen(cn, gen, \"cn\", gen_name, common_col, save_dir)\n",
    "    del_gen = genxgen(del_cn, gen, \"del\", gen_name, common_col, save_dir)\n",
    "    \n",
    "#     Stratify gen by # del\n",
    "    var_name = \"del\"\n",
    "    for del_val_list in table3_del_val_tuple:\n",
    "        strati_gen(del_gen, del_val_list, var_name, gen_name, save_dir)\n",
    "        \n",
    "#     Stratify gen by # cn\n",
    "    var_name = \"cn\"\n",
    "    for cn_val_list in table3_cn_val_tuple:\n",
    "        strati_gen(cn_gen, cn_val_list, var_name, gen_name, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pheno_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "full_pheno_filename = \"freeze8_anno04_af02_btc01-coh03_ddimer_egfr03_adad01.tsv\"\n",
    "full_pheno_dir_filename = os.path.join(full_pheno_dir, full_pheno_filename)\n",
    "full_pheno = pd.read_csv(full_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "_, cohort_list = categorize_df(full_pheno, \"study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FHS', 'COPDGene', 'BioMe', 'ARIC', 'GeneSTAR', 'GenSalt', 'CARDIA', 'HyperGEN', 'DHS', 'HCHS_SOL', 'CHS', 'WHI', 'MESA', 'GENOA', 'JHS']\n"
     ]
    }
   ],
   "source": [
    "print(cohort_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2594, 2595)\n",
      "FHS common samples found.\n",
      "(4699, 4700)\n",
      "COPDGene common samples found.\n",
      "(8029, 8030)\n",
      "BioMe common samples found.\n",
      "(3015, 3016)\n",
      "ARIC common samples found.\n",
      "(1266, 1267)\n",
      "GeneSTAR common samples found.\n",
      "(1717, 1718)\n",
      "GenSalt common samples found.\n",
      "(2596, 2597)\n",
      "CARDIA common samples found.\n",
      "(1700, 1701)\n",
      "HyperGEN common samples found.\n",
      "(344, 345)\n",
      "DHS common samples found.\n",
      "(3382, 3383)\n",
      "HCHS_SOL common samples found.\n",
      "(2768, 2769)\n",
      "CHS common samples found.\n",
      "(8646, 8647)\n",
      "WHI common samples found.\n",
      "(3972, 3973)\n",
      "MESA common samples found.\n",
      "(1013, 1014)\n",
      "GENOA common samples found.\n",
      "(2933, 2934)\n",
      "JHS common samples found.\n"
     ]
    }
   ],
   "source": [
    "freeze_ver = \"freeze8\"\n",
    "common_col = \"NWDID\"\n",
    "cn_var = \"cn\"\n",
    "pc_num = 11\n",
    "snp_list = [\"rs334\", \"rs33930165\", \"rs11248850\"]\n",
    "pheno_prefix = \"freeze8_anno04_af02_btc03-coh03_egfr03-ckd_adad01\"\n",
    "for cohort in cohort_list:\n",
    "    pheno_filename = \"%s_%s.tsv\"%(pheno_prefix, cohort)\n",
    "    common_snp_pc_cn_pheno_kinship(cohort, freeze_ver, pc_num, common_col, snp_list, freeze8_kinship_df_ID, cn_var, pheno_filename)\n",
    "    print(\"%s common samples found.\"%cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Each Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kinship_simple(load_dir, filename_prefix):\n",
    "    load_dir_filename_prefix = os.path.join(load_dir, filename_prefix)\n",
    "    load_dir_filename_tsv = \"%s.tsv\"%load_dir_filename_prefix\n",
    "    load_dir_filename_feather = \"%s.feather\"%load_dir_filename_prefix\n",
    "    if os.path.exists(load_dir_filename_tsv):\n",
    "        kinship = pd.read_csv(load_dir_filename_tsv, sep = \"\\t\", header = 0, index_col = None)\n",
    "    elif os.path.exists(load_dir_filename_feather):\n",
    "        kinship = pd.read_feather(load_dir_filename_feather, use_threads = True)\n",
    "    return kinship\n",
    "\n",
    "def save_kinship_simple(save_dir, filename_prefix, kinship, sample_col = True):\n",
    "    save_dir_filename_prefix = os.path.join(save_dir, filename_prefix)\n",
    "    save_dir_filename_tsv = \"%s.tsv\"%save_dir_filename_prefix\n",
    "    save_dir_filename_feather = \"%s.feather\"%save_dir_filename_prefix\n",
    "    if kinship.shape[0] > 5000:\n",
    "        if sample_col == False:\n",
    "            kinship.drop(axis = 1, labels = [\"NWDID\"], inplace = True)\n",
    "        feather.write_dataframe(kinship, save_dir_filename_feather)\n",
    "    elif kinship.shape[0] <= 5000:\n",
    "        kinship.to_csv(save_dir_filename_tsv, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_matrix_pheno_adad_in_pheno(cohort, phenotype, pheno_full_df, var_name, var_df, table_name, kinship_df_ID, pc10_df, adad_header):\n",
    "    if not os.path.exists(os.path.join(\"..\", \"cohort\", cohort, table_name)):\n",
    "        os.mkdir(os.path.join(\"..\", \"cohort\", cohort, table_name))\n",
    "    if not os.path.exists(os.path.join(\"..\", \"cohort\", cohort, table_name, var_name)):\n",
    "        os.mkdir(os.path.join(\"..\", \"cohort\", cohort, table_name, var_name))\n",
    "        \n",
    "    save_dir = os.path.join(\"..\", \"cohort\", cohort, table_name, var_name)\n",
    "    \n",
    "    pheno_df_raw = pheno_full_df[[\"NWDID\", \"sex\", phenotype, \"age_at_%s\"%phenotype] + adad_header].copy()\n",
    "    pheno_df_raw.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    adad_header_used = intersection_list(list(pheno_df_raw), adad_header)\n",
    "    if phenotype not in list(pheno_df_raw):\n",
    "        return None\n",
    "    \n",
    "    pheno_agesex_df = pheno_df_raw.dropna(axis=0)\n",
    "    common_col = \"NWDID\"\n",
    "    pheno_agesex_var_df_list = [pheno_agesex_df, var_df]\n",
    "    common_pheno_agesex = merge_df_list(pheno_agesex_var_df_list, common_col, merge_method='first')\n",
    "    common_var = merge_df_list(pheno_agesex_var_df_list, common_col, merge_method='second')\n",
    "    sample_df = common_pheno_agesex[[\"NWDID\"]].copy()\n",
    "    \n",
    "    common_pheno = common_pheno_agesex[[\"NWDID\", phenotype]].copy()\n",
    "    common_pheno_filename = \"common_pheno_%s_%s_%s.tsv\"%(table_name, var_name, phenotype)\n",
    "    common_pheno_dir_filename = os.path.join(save_dir, common_pheno_filename)\n",
    "    common_pheno.to_csv(common_pheno_dir_filename, sep=\"\\t\", index=False)\n",
    "    \n",
    "    common_var_filename = \"common_var_%s_%s_%s.tsv\"%(table_name, var_name, phenotype)\n",
    "    common_var_dir_filename = os.path.join(save_dir, common_var_filename)\n",
    "    common_var.to_csv(common_var_dir_filename, sep=\"\\t\", index=False)\n",
    "\n",
    "    common_kinship = kinship_select_sample(kinship_df_ID, sample_df)\n",
    "    common_kinship_filename_prefix = \"common_kinship_%s_%s_%s\"%(table_name, var_name, phenotype)\n",
    "    save_kinship_simple(save_dir, common_kinship_filename_prefix, common_kinship, sample_col = False)\n",
    "\n",
    "    common_age_sex = common_pheno_agesex[[\"NWDID\", \"age_at_%s\"%phenotype, \"sex\"] + adad_header_used].copy()\n",
    "    common_age_sex_filename = \"common_age-sex_%s_%s_%s.tsv\"%(table_name, var_name, phenotype)\n",
    "    common_age_sex_dir_filename = os.path.join(save_dir, common_age_sex_filename)\n",
    "    common_age_sex.to_csv(common_age_sex_dir_filename, sep=\"\\t\", index=False)\n",
    "    \n",
    "    common_pc10 = merge_df_list([pc10_df, sample_df], common_col, merge_method='first')\n",
    "    common_pc10_filename = \"common_pc10_%s_%s_%s.tsv\"%(table_name, var_name, phenotype)\n",
    "    common_pc10_dir_filename = os.path.join(save_dir, common_pc10_filename)\n",
    "    common_pc10.to_csv(common_pc10_dir_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_prepare_matrix_pheno_adad_in_pheno(phenotype_list, table_dict, load_dir, adad_dict):\n",
    "    for table_name in table_dict:\n",
    "        pheno_full_filename = \"common_pheno.tsv\"\n",
    "        pheno_full_dir_filename = os.path.join(load_dir, pheno_full_filename)\n",
    "        pheno_full_df = pd.read_csv(pheno_full_dir_filename, sep=\"\\t\")\n",
    "        if bool(adad_dict):\n",
    "            pheno_full_df_raw = pheno_full_df.copy()\n",
    "            del pheno_full_df\n",
    "            pheno_full_df, adad_header = adad_header_conversion(pheno_full_df_raw, adad_dict)\n",
    "            pheno_dummy_adad_filename = \"common_pheno_adad_dummy.tsv\"\n",
    "            pheno_dummy_adad_dir_filename = os.path.join(load_dir, pheno_dummy_adad_filename)\n",
    "            pheno_full_df.to_csv(pheno_dummy_adad_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        else:\n",
    "            adad_header = []\n",
    "        \n",
    "        kinship_filename_prefix = \"common_kinship\"\n",
    "        kinship_df_ID = load_kinship_simple(load_dir, kinship_filename_prefix)\n",
    "\n",
    "        pc_filename = \"common_pc.tsv\"\n",
    "        pc_dir_filename = os.path.join(load_dir, pc_filename)\n",
    "        pc_df = pd.read_csv(pc_dir_filename, sep=\"\\t\")\n",
    "        \n",
    "        var_name_list = table_dict[table_name]\n",
    "        for var_name in var_name_list:\n",
    "            var_filename = \"common_%s.tsv\"%var_name\n",
    "            var_dir_filename = os.path.join(load_dir, var_filename)\n",
    "            var_df = pd.read_csv(var_dir_filename, sep=\"\\t\")\n",
    "        \n",
    "            for phenotype, i in zip(phenotype_list, range(len(phenotype_list))):\n",
    "                prepare_matrix_pheno_adad_in_pheno(cohort, phenotype, pheno_full_df, var_name, var_df, table_name, kinship_df_ID, pc_df, adad_header)\n",
    "#                print(\"%s (%d/%d) completed.\"%(phenotype, i + 1, len(phenotype_list)))\n",
    "        print(\"%s completed.\"%table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adad_header_conversion(df_original, adad_dict):\n",
    "#     adad_quan_list = adad_dict[\"quan\"]\n",
    "#     adad_cati_list = adad_dict[\"cati\"]\n",
    "#     df, adad_cati_dummy_list = cati2dummy_df(df_original, adad_cati_list)\n",
    "#     adad_header = adad_quan_list + adad_cati_dummy_list\n",
    "    adad_cati_list = adad_dict[\"cati\"]\n",
    "    df, adad_header = cati2dummy_df(df_original, adad_cati_list)\n",
    "    return df, adad_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "FHS completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "COPDGene completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "BioMe completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "ARIC completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "GeneSTAR completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "GenSalt completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "CARDIA completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "HyperGEN completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "DHS completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "HCHS_SOL completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "CHS completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "WHI completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "MESA completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "GENOA completed.\n",
      "table1 completed.\n",
      "table2 completed.\n",
      "table3 completed.\n",
      "JHS completed.\n"
     ]
    }
   ],
   "source": [
    "cohort_list = ['FHS', 'COPDGene', 'BioMe', 'ARIC', 'GeneSTAR', 'GenSalt', 'CARDIA', 'HyperGEN', 'DHS',\n",
    "               'HCHS_SOL', 'CHS', 'WHI', 'MESA', 'GENOA', 'JHS']\n",
    "# cohort_list = ['HCHS_SOL']\n",
    "gen_name_1 = \"rs334\"\n",
    "gen_name_2 = \"rs33930165\"\n",
    "gen_name_3 = \"rs11248850\"\n",
    "table3_del_val_tuple = ([0], [1, 2])\n",
    "table3_cn_val_tuple = ([2], [3, 4])\n",
    "cn_var = \"cn\"\n",
    "\n",
    "phenotype_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"DDIMER\",\n",
    "                  \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\", \"rdw_ratio_rbc_1\",\n",
    "                  \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\",\n",
    "                  \"eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"lnHBA1C\", \n",
    "                  \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"EGFRCKDEPI\", \"CKD\", \"microcytosis\", \"anemia\"]\n",
    "table_dict = {\"table1\":[\"cn-binary_cn0\", \"cn-binary_cn1\", \"cn-binary_cn3\", \"rs334\", \"rs33930165\"],\n",
    "              \"table2\":[\"rs334_del0\", \"rs334_del1\", \"rs334_del2\", \"del-rs334\"],\n",
    "              \"table3\":[\"rs11248850\", \"rs11248850_del0\", \"rs11248850_del12\", \"rs11248850_cn2\", \"rs11248850_cn34\", \"del-rs11248850\"]}\n",
    "adad_dict = {\"quan\":[\"WEIGHT_FINAL_NORM_OVERALL\"], \"cati\":[\"gengrp6\", \"CENTER\"]}\n",
    "\n",
    "for cohort in cohort_list:\n",
    "    table1(cohort, cn_var)\n",
    "    table2(cohort, gen_name_1)\n",
    "    table3(cohort, gen_name_3, table3_del_val_tuple, table3_cn_val_tuple)\n",
    "    load_dir = os.path.join(\"..\", \"cohort\", cohort, \"ready_data\")\n",
    "    wrap_prepare_matrix_pheno_adad_in_pheno(phenotype_list, table_dict, load_dir, adad_dict)\n",
    "    print(\"%s completed.\"%cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MESA (from Methylation) Samples to Kinship Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1890, 21)\n",
      "(1890, 1891)\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"/pine/scr/m/i/minzhi/mesa_multiomics/methylation_normalized\"\n",
    "sample_filename = \"TOPMed_MESA.methylomics.samplesheet_with_feno.mixup_fix.only_BIS.pass_QC.txt\"\n",
    "sample_dir_filename = os.path.join(save_dir, sample_filename)\n",
    "sample_df_raw = pd.read_csv(sample_dir_filename, sep=\"\\t\")\n",
    "print(sample_df_raw.shape)\n",
    "sample_df = sample_df_raw[[\"NWDID\"]]\n",
    "kinship_sample_selected = kinship_select_sample(freeze8_kinship_df_ID, sample_df)\n",
    "print(kinship_sample_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(936, 937)\n"
     ]
    }
   ],
   "source": [
    "sample_df_unique = sample_df.drop_duplicates(subset =\"NWDID\", keep = \"first\", inplace = False)\n",
    "kinship_sample_unique_selected = kinship_select_sample(freeze8_kinship_df_ID, sample_df_unique)\n",
    "print(kinship_sample_unique_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906, 1)\n"
     ]
    }
   ],
   "source": [
    "unrelated_dir = \"/pine/scr/m/i/minzhi/mesa_multiomics/methylation_normalized\"\n",
    "unrelated_filename = \"TOPMed_MESA.1331_samples_unrelated.txt\"\n",
    "unrelated_dir_filename = os.path.join(unrelated_dir, unrelated_filename)\n",
    "unrelated = pd.read_csv(unrelated_dir_filename, sep = \"\\t\", header = None, index_col = None)\n",
    "unrelated.rename(columns = {0:\"NWDID\"}, inplace = True)\n",
    "overlap_unrelated_methy = sample_df_unique.merge(unrelated, on = \"NWDID\", how = \"inner\")\n",
    "print(overlap_unrelated_methy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_kinship_filename = \"mesa_methy_kinship_unique.tsv\"\n",
    "common_kinship_dir_filename = os.path.join(save_dir, common_kinship_filename)\n",
    "kinship_sample_unique_selected.to_csv(common_kinship_dir_filename, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_kinship_filename = \"mesa_methy_kinship_unique.tsv\"\n",
    "common_kinship_dir_filename = os.path.join(save_dir, common_kinship_filename)\n",
    "kinship_sample_unique_selected_IDidx = pd.read_csv(common_kinship_dir_filename, sep=\"\\t\", index_col=0, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kinship_filtering(kinship_matrix, thres):\n",
    "\toriginal_list = list(kinship_matrix.columns)\n",
    "\tsample_num = len(original_list)\n",
    "\tcorrelated_sample_index = []\n",
    "\tuncorrelation_sample_list = []\n",
    "\tfor sample_i in range(1, sample_num):\n",
    "\t\tfor ortho_sample_i in range(sample_i):\n",
    "\t\t\tif kinship_matrix.iloc[sample_i, ortho_sample_i] > thres:\n",
    "\t\t\t\tcorrelated_sample_index.append(ortho_sample_i)\n",
    "\tfor sample_i in range(sample_num):\n",
    "\t\tif sample_i not in correlated_sample_index:\n",
    "\t\t\tuncorrelation_sample_list.append(original_list[sample_i])\n",
    "\treturn uncorrelation_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n",
      "925\n",
      "917\n",
      "912\n",
      "910\n",
      "905\n",
      "629\n"
     ]
    }
   ],
   "source": [
    "thres_list = [1, 0.2, 0.1, 0.05, 0.03125, 0.025, 0.005]\n",
    "for thres in thres_list:\n",
    "    uncorrelation_sample_list = kinship_filtering(kinship_sample_unique_selected_IDidx, thres)\n",
    "    print(len(uncorrelation_sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kinship_sample_selected_noID = kinship_sample_selected.drop(axis = 1, labels = \"NWDID\")\n",
    "kinship_sample_selected_noID_array = kinship_sample_selected_noID.values\n",
    "inds = np.tril_indices(kinship_sample_selected_noID_array.shape[0], -1) \n",
    "vals = kinship_sample_selected_noID_array[inds]\n",
    "_ = plt.hist(vals, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785105,)\n",
      "1784083\n",
      "1784041\n",
      "1784015\n",
      "1783993\n",
      "1783967\n",
      "1781725\n"
     ]
    }
   ],
   "source": [
    "thres_list = [0.2, 0.1, 0.05, 0.03125, 0.025, 0.005]\n",
    "print(vals.shape)\n",
    "for thres in thres_list:\n",
    "    tmp_vals = vals[vals <= thres]\n",
    "    tmp_sample_num = len(tmp_vals)\n",
    "    print(tmp_sample_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association between Blood Cell Trait and SOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = \"SOL\"\n",
    "save_dir = os.path.join(\"..\", \"cohort\", cohort, \"pre_data\")\n",
    "pheno_filename = \"pheno_%s.tsv\"%cohort\n",
    "common_pheno_anno_cn_egfr = pheno_prep(cohort, egfr_df, anno_df, pheno_df)\n",
    "pheno_wckd_wmicrocytosis_anemia_df = pheno_adding_features_recode_sex(common_pheno_anno_cn_egfr, save_dir, pheno_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D-Dimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmier_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddmier_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddmier_dir_filename = os.path.join(ddmier_dir, ddmier_filename)\n",
    "ddmier = pd.read_csv(ddmier_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "ddmier_selected = ddmier[[\"sample.id\", \"sample_remove_DDIMER\", \"DDIMER\", \"AGE_DDIMER\"]] #, \"study_race\", \"sex\"\n",
    "ddmier_selected.rename(columns={\"sample.id\":\"NWDID\", \"AGE_DDIMER\":\"age_at_DDIMER\"}, inplace=True)\n",
    "ddmier_selected_filename = \"SOL_ddimer.tsv\"\n",
    "ddmier_selected_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "ddmier_selected_dir_filename = os.path.join(ddmier_selected_dir, ddmier_selected_filename)\n",
    "ddmier_selected.to_csv(ddmier_selected_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gengrp6\n",
    "After testing, to each sample, if s/he has non-NaN SUBJECT_ID and gengrp6 value, then it must have non-NaN CONSENT_text and INTERNAL_USE_ONLY value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "gengrp6_filename = \"page-harmonized-phenotypes-pca-freeze2-candidate2-2016-12-14.GWASid_fid_22May2018internalPCs.SOLv2consent.txt\"\n",
    "gengrp6_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\", \"gengrp6\")\n",
    "gengrp6_dir_filename = os.path.join(gengrp6_dir, gengrp6_filename)\n",
    "gengrp6 = pd.read_csv(gengrp6_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6.replace(\".\", np.nan, inplace=True)\n",
    "gengrp6_select = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\",\n",
    "                          \"gengrp6\"]].dropna(axis=0, subset=[\"analysis_id\",\"gengrp6\"],how=\"any\")\n",
    "gengrp6_select.rename(columns = {\"analysis_id\":\"SUBJECT_ID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight & Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_center_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\")\n",
    "weight_center_filename = \"bloodcell_output.csv\"\n",
    "weight_center_dir_filename = os.path.join(weight_center_dir, weight_center_filename)\n",
    "weight_center = pd.read_csv(weight_center_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "weight_center_select = weight_center[[\"ID\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\"]].dropna(axis = 0, how = \"any\")\n",
    "weight_center_select.rename(columns = {\"ID\":\"z_sol_id\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Weight & Center Table with Gengrp6 Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_col = \"z_sol_id\"\n",
    "gengrp6_weight_center_list = [gengrp6_select, weight_center_select]\n",
    "gengrp6_weight_center = merge_df_list(gengrp6_weight_center_list, common_col, merge_method='merge', how = 'inner')\n",
    "gengrp6_weight_center.drop(axis = 1, columns = [\"z_sol_id\"], inplace = True)\n",
    "gengrp6_weight_center_filename = \"SOL_gengrp6_weight_center.tsv\"\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center.to_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Weight, Center, Gengrp6 and D-Dimer into Phenotype Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gengrp6_weight_center_filename = \"SOL_gengrp6_weight_center.tsv\"\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center = pd.read_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "#display(gengrp6_weight_center)\n",
    "\n",
    "ddimer_filename = \"SOL_ddimer.tsv\"\n",
    "ddimer_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer = pd.read_csv(ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "#display(ddimer)\n",
    "\n",
    "common_col = \"SUBJECT_ID\"\n",
    "pheno_gengrp6_weight_center_list = [pheno_wckd_wmicrocytosis_anemia_df, gengrp6_weight_center]\n",
    "pheno_gengrp6_weight_center = merge_df_list(pheno_gengrp6_weight_center_list, common_col, merge_method='merge', how = 'outer')\n",
    "\n",
    "common_col = \"NWDID\"\n",
    "pheno_gengrp6_weight_center_ddimer_list = [pheno_gengrp6_weight_center, ddimer]\n",
    "pheno_gengrp6_weight_center_ddimer = merge_df_list(pheno_gengrp6_weight_center_ddimer_list, common_col, merge_method='merge', how = 'outer')\n",
    "#display(pheno_gengrp6_weight_center_ddimer)\n",
    "\n",
    "pheno_gengrp6_weight_center_ddimer.dropna(axis = 0, subset = [\"NWDID\"], inplace = True)\n",
    "pheno_gengrp6_weight_center_ddimer = pheno_gengrp6_weight_center_ddimer[pheno_gengrp6_weight_center_ddimer[\"CONSENT_text\"] != \"DROP\"]\n",
    "pheno_gengrp6_weight_center_ddimer = pheno_gengrp6_weight_center_ddimer[pheno_gengrp6_weight_center_ddimer[\"sample_remove_DDIMER\"] != 1]\n",
    "\n",
    "add_adj_dict = {\"quant\":[\"WEIGHT_FINAL_NORM_OVERALL\"],\"cati\":[\"gengrp6\", \"CENTER\"]}\n",
    "pheno_gengrp6_weight_center_ddimer_dummy_adj, add_adj_list = cati2dummy_df(pheno_gengrp6_weight_center_ddimer, add_adj_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\"..\", \"cohort\", cohort, \"pre_data\")\n",
    "pheno_filename = \"pheno_%s_20190904.tsv\"%cohort\n",
    "pheno_dir_filename = os.path.join(save_dir, pheno_filename)\n",
    "pheno_gengrp6_weight_center_ddimer.to_csv(pheno_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = \"SOL\"\n",
    "cn_filename = \"%s_cn.tsv\"%cohort\n",
    "cn_dir = os.path.join(\"..\", \"cohort\", cohort, \"cn\")\n",
    "cn_dir_filename = os.path.join(cn_dir, cn_filename)\n",
    "cn_df = pd.read_csv(cn_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddmier_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddmier_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddmier_dir_filename = os.path.join(ddmier_dir, ddmier_filename)\n",
    "ddmier = pd.read_csv(ddmier_dir_filename, sep = \",\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_col = \"NWDID\"\n",
    "cn_df_ddimer_list = [cn_df, ddimer]\n",
    "cn_df_ddimer_inner = merge_df_list(cn_df_ddimer_list, common_col, merge_method='merge', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWDID</th>\n",
       "      <th>CN</th>\n",
       "      <th>sample.id</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>unique_subject_key_f6a</th>\n",
       "      <th>flag_diff_USK_F8_f6a</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>topmed_project</th>\n",
       "      <th>topmed_phase</th>\n",
       "      <th>funding</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_24</th>\n",
       "      <th>PC_25</th>\n",
       "      <th>PC_26</th>\n",
       "      <th>PC_27</th>\n",
       "      <th>PC_28</th>\n",
       "      <th>PC_29</th>\n",
       "      <th>PC_30</th>\n",
       "      <th>PC_31</th>\n",
       "      <th>PC_32</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NWDID, CN, sample.id, unique_subject_key, unique_subject_key_f6a, flag_diff_USK_F8_f6a, subject_id, topmed_project, topmed_phase, funding, seq_center, sex_dcc_f6a, Xchr.anom_f6a, Ychr.anom_f6a, exclude_f6a, id, STUDY, ETHNICITY, ETHNICITY_LET, study_race, sex, FIB, FVII, FVII_METHOD, FVII_ACTIVITY, FVII_ANTIGEN, FVIII, VWF, DDIMER, TPA, PAI1, AGE_FIB, AGE_FVII, AGE_FVII_ACTIVITY, AGE_FVII_ANTIGEN, AGE_FVIII, AGE_VWF, AGE_DDIMER, AGE_TPA, AGE_PAI1, ANTICOAG_FIB, ANTICOAG_FVII, ANTICOAG_FVII_ACTIVITY, ANTICOAG_FVII_ANTIGEN, ANTICOAG_FVIII, ANTICOAG_VWF, ANTICOAG_DDIMER, ANTICOAG_TPA, ANTICOAG_PAI1, AMISH_SSC_FIB, ARIC_SSC, CARDIA_SSC, CARDIA_SSC_FIB, CARDIA_SSC_FVII, CARDIA_SSC_FVIII, CARDIA_SSC_VWF, CFS_SSC, CHS_SSC, FHS_SSC_FIB_PAI1, GeneSTAR_SSC, GeneSTAR_SSC_FIB, GENOA_SSC, MESA_SSC, WHI_SSC, WHI_SSC_FIB, WHI_SSC_DDIMER, WHI_SSC_TPA, WHI_SSC_PAI1, MZtwinID, TechnicalRepID, BetweenStudyDupID, sample_remove_default, sample_remove_FVII, sample_remove_FVII_antigen, sample_remove_FVIII, sample_remove_VWF, sample_remove_DDIMER, sample_remove_TPA, sample_remove_PAI1, PC_1, PC_2, PC_3, PC_4, PC_5, PC_6, PC_7, PC_8, PC_9, PC_10, PC_11, PC_12, PC_13, PC_14, PC_15, PC_16, PC_17, PC_18, PC_19, PC_20, PC_21, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 112 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = cn_df.merge(ddmier, left_on=\"NWDID\", right_on=\"sample.id\")\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(duplicates_num(pheno_gengrp6_weight_center_ddimer, \"NWDID\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
