{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import feather\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"/proj/yunligrp/users/minzhi/utils/pylib\" not in sys.path:\n",
    "    sys.path.insert(0, \"/proj/yunligrp/users/minzhi/utils/pylib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_process_data_eqtl import *\n",
    "from function_asso import *\n",
    "from function_mesa_cca import *\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Alpha Globin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. cohort and 2. # samples with calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_pass_useful = cn_pass[[\"SAMPLE\", \"CN\"]]\n",
    "cn_pass_useful.rename(columns = {\"SAMPLE\":\"NWDID\", \"CN\":\"cn\"}, inplace = True)\n",
    "cn_pass_dir = os.path.join(\"..\", \"prepro_data\", \"cn\")\n",
    "cn_pass_filename = \"alpha_globin_calls_pass_useful.tsv\"\n",
    "cn_pass_dir_filename = os.path.join(cn_pass_dir, cn_pass_filename)\n",
    "cn_pass_useful.to_csv(cn_pass_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Summary of Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-10-08_useful_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118934, 15)\n",
      "['GOLDN', 'INSPIRE_AF', 'CAMP', 'GALAI', 'VU_AF', 'HCHS_SOL', 'CARDIA', 'EGCUT', 'THRV', 'PUSH_SCD', 'DECAF', 'REDS-III_Brazil', 'MGH_AF', 'AustralianFamilialAF', 'IPF', 'OMG_SCD', 'Partners', 'CHIRAH', 'BioMe', 'SAS', 'BioVU_AF', 'GENOA', 'CARE_PACT', 'CARE_BADGER', 'PharmHU', 'PCGC_CHD', 'CFS', 'GenSalt', 'PIMA', 'Sarcoidosis', 'HyperGEN', 'BAGS', 'PMBB_AF', 'WHI', 'walk_PHaSST', 'EOCOPD', 'GGAF', 'LTRC', 'WGHS', 'SARP', 'MPP', 'CARE_CLIC', 'MLOF', 'FHS', 'ARIC', 'ChildrensHS_IGERA', 'HVH', 'GeneSTAR', 'COPDGene', 'CCAF', 'SAFS', 'GENAF', 'SAGE', 'GALAII', 'DHS', 'SAPPHIRE_asthma', 'AFLMU', 'CATHGEN', 'MESA', 'VAFAR', 'miRhythm', 'JHS', 'CRA', 'JHU_AF', 'Mayo_VTE', 'CHS', 'Amish', 'ChildrensHS_MetaAir', 'CARE_TREXA', 'ChildrensHS_GAP', 'ECLIPSE']\n"
     ]
    }
   ],
   "source": [
    "cn_annotation = cn_pass.merge(annotation, left_on = \"SAMPLE\", right_on = \"NWDID\", how = \"inner\")\n",
    "cn_pass_anno_summary, cn_pass_anno_list = categorize_df(cn_annotation, \"study\")\n",
    "print(cn_annotation.shape)\n",
    "print(cn_pass_anno_list)\n",
    "cn_pass_anno_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cn_pass_anno_summary_filename = \"cn-nathan_pass_anno_cohort_summary.tsv\"\n",
    "cn_pass_anno_summary_dir_filename = os.path.join(cn_pass_anno_summary_dir, cn_pass_anno_summary_filename)\n",
    "cn_pass_anno_summary.to_csv(cn_pass_anno_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. # samples with phenotype data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ddimer_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer_raw = pd.read_csv(ddimer_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "ddimer = ddimer_raw.loc[:, [\"sample.id\", \"STUDY\", \"DDIMER\", \"AGE_DDIMER\", \"sample_remove_DDIMER\"]]\n",
    "ddimer.dropna(axis = \"index\", how = \"any\", inplace = True)\n",
    "ddimer_category_summary, _ = categorize_df(ddimer, \"STUDY\")\n",
    "ddimer_category_summary.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "ddimer_category_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_category_summary_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019_study_summary.tsv\"\n",
    "ddimer_category_summary_dir_filename = os.path.join(ddimer_category_summary_dir, ddimer_category_summary_filename)\n",
    "ddimer_category_summary.to_csv(ddimer_category_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddimer_qc = ddimer[ddimer[\"sample_remove_DDIMER\"] != 1]\n",
    "ddimer_qc_category_summary, _ = categorize_df(ddimer_qc, \"STUDY\")\n",
    "ddimer_qc_category_summary.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "ddimer_qc_category_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_qc_category_summary_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019_qc_study_summary.tsv\"\n",
    "ddimer_qc_category_summary_dir_filename = os.path.join(ddimer_qc_category_summary_dir, ddimer_qc_category_summary_filename)\n",
    "ddimer_qc_category_summary.to_csv(ddimer_qc_category_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "filename_list = os.listdir(egfr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = []\n",
    "sample_size_list = []\n",
    "for filename in filename_list[1:]:\n",
    "    cohort_name_raw = filename.split(\"_\")[3]\n",
    "    if cohort_name_raw == \"PHEN\":\n",
    "        cohort_name = \"SOL\"\n",
    "    else:\n",
    "        cohort_name = cohort_name_raw\n",
    "    dir_filename = os.path.join(egfr_dir, filename)\n",
    "    tmp_df = pd.read_csv(dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    tmp_sample_size = tmp_df.shape[0]\n",
    "    cohort_list.append(cohort_name)\n",
    "    sample_size_list.append(tmp_sample_size)\n",
    "cohort_sample_size_tuple = list(zip(cohort_list, sample_size_list))\n",
    "egfr_sample_size_df = pd.DataFrame(data = cohort_sample_size_tuple, columns = [\"cohort\", \"sample_size\"])\n",
    "egfr_sample_size_df.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "egfr_sample_size_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_sample_size_filename = \"egfr_cohort_summary.tsv\"\n",
    "egfr_sample_size_dir_filename = os.path.join(egfr_sample_size_dir, egfr_sample_size_filename)\n",
    "egfr_sample_size_df.to_csv(egfr_sample_size_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grengrp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (0,1,2,28,29,30,35,39,42,45,46,47,49,54,56,61,63,69,70,71,72,73,76,83,85,89,90,91,98,114,117,125,132,134,135,142,155,156,160,161,163,164,176,186,187,192,196,205,206,207,208,209,210,216,218,220,224,225,228,234,242,244,247,250,260,269,270,272,282,283,285,286,288,292,294,295,296,298,303,305,335,336,383,385,386,389,390,391,392,393) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 5)\n",
      "(11678, 5)\n"
     ]
    }
   ],
   "source": [
    "gengrp6_filename = \"page-harmonized-phenotypes-pca-freeze2-candidate2-2016-12-14.GWASid_fid_22May2018internalPCs.SOLv2consent.txt\"\n",
    "gengrp6_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\", \"gengrp6\")\n",
    "gengrp6_dir_filename = os.path.join(gengrp6_dir, gengrp6_filename)\n",
    "gengrp6 = pd.read_csv(gengrp6_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6.replace(\".\", np.nan, inplace=True)\n",
    "gengrp6_select = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\",\n",
    "                          \"gengrp6\"]].dropna(axis=0, subset=[\"analysis_id\",\"gengrp6\"],how=\"any\")\n",
    "gengrp6_select.rename(columns = {\"analysis_id\":\"SUBJECT_ID\"}, inplace=True)\n",
    "print(gengrp6_select.shape)\n",
    "gengrp6_select = gengrp6_select[gengrp6_select[\"CONSENT_text\"] != \"DROP\"]\n",
    "print(gengrp6_select.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight and center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16415, 3)\n",
      "(9974, 7)\n"
     ]
    }
   ],
   "source": [
    "weight_center_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\")\n",
    "weight_center_filename = \"bloodcell_output.csv\"\n",
    "weight_center_dir_filename = os.path.join(weight_center_dir, weight_center_filename)\n",
    "weight_center = pd.read_csv(weight_center_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "weight_center_select = weight_center[[\"ID\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\"]].dropna(axis = 0, how = \"any\")\n",
    "weight_center_select.rename(columns = {\"ID\":\"z_sol_id\"}, inplace = True)\n",
    "print(weight_center_select.shape)\n",
    "gengrp6_weight_center = gengrp6_select.merge(weight_center_select, on = \"z_sol_id\", how = \"inner\")\n",
    "print(gengrp6_weight_center.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blood cell traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh02_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cat_col = \"cohort\"\n",
    "pheno_cat_summary, cohort_list = categorize_df(pheno, cat_col)\n",
    "cat_list = cohort_list\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\"]\n",
    "fixed_col_list = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"]\n",
    "cohort_pheno_df = cat_col_summary(pheno, cat_col, cat_list, col_list, fixed_col_list)\n",
    "cohort_pheno_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cohort_pheno_filename = \"cohort_pheno_summary.tsv\"\n",
    "cohort_pheno_dir_filename = os.path.join(cohort_pheno_dir, cohort_pheno_filename)\n",
    "cohort_pheno_df.to_csv(cohort_pheno_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. # sample in annotation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cohort Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-07-30.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)\n",
    "annotation_cat_summary, _ = categorize_df(annotation, \"study\")\n",
    "annotation_cat_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "annotation_cat_summary_filename = \"annotation_cohort_summary_2019-07-30.tsv\"\n",
    "annotation_cat_summary_dir_filename = os.path.join(annotation_cat_summary_dir, annotation_cat_summary_filename)\n",
    "annotation_cat_summary.to_csv(annotation_cat_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Consent Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-07-30.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "sample_size = annotation.shape[0]\n",
    "annotation.dropna(axis = 0, subset = [\"consent\"], how = \"any\", inplace = True)\n",
    "sample_size_nona = annotation.shape[0]\n",
    "na_df = pd.DataFrame(data = [[\"NAN\", sample_size - sample_size_nona]], columns = [\"consent\", \"sample_size\"])\n",
    "annotation_cat_summary, _ = categorize_df(annotation, \"consent\")\n",
    "annotation_cat_summary = annotation_cat_summary.append(na_df, ignore_index=True)\n",
    "annotation_cat_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "annotation_cat_summary_filename = \"annotation_consent_summary_2019-07-30.tsv\"\n",
    "annotation_cat_summary_dir_filename = os.path.join(annotation_cat_summary_dir, annotation_cat_summary_filename)\n",
    "annotation_cat_summary.to_csv(annotation_cat_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Consent Summary by Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-10-08.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)\n",
    "_, cohort_list = categorize_df(annotation, \"study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_summary_cohort = pd.DataFrame(columns=['cohort', 'consent', 'sample_size'])\n",
    "for cohort in cohort_list:\n",
    "    tmp_annotation = annotation.loc[annotation.loc[:, \"study\"] == cohort, :]\n",
    "    tmp_summary, _ = categorize_df(tmp_annotation, \"consent\")\n",
    "    row_num = tmp_summary.shape[0]\n",
    "    tmp_cohort_series = pd.Series(data = [cohort] * row_num)\n",
    "    tmp_summary.insert(loc = 0, column = \"cohort\", value = tmp_cohort_series)\n",
    "    consent_summary_cohort = pd.concat([consent_summary_cohort, tmp_summary], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_summary_cohort_dir = os.path.join(\"..\", \"data_summary\")\n",
    "consent_summary_cohort_filename = \"annotation_consent_summary_cohort_2019-10-08.tsv\"\n",
    "consent_summary_cohort_dir_filename = os.path.join(consent_summary_cohort_dir, consent_summary_cohort_filename)\n",
    "consent_summary_cohort.to_csv(consent_summary_cohort_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Clean Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2020-03-03.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation_raw = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation_useful_header_list = [\"sample.id\", \"unique_subject_key\", \"subject_id\", \"consent\", \"study\", \"sex\", \"exclude\"]\n",
    "annotation = annotation_raw[annotation_useful_header_list]\n",
    "annotation.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "annotation.rename(columns = {\"sample.id\":\"NWDID\"}, inplace = True)\n",
    "annotation.loc[annotation.loc[:, \"sex\"] == \"M\", \"sex\"] = 1\n",
    "annotation.loc[annotation.loc[:, \"sex\"] == \"F\", \"sex\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = annotation.loc[annotation.loc[:, \"exclude\"] == False, :]\n",
    "annotation.reset_index(inplace = True, drop = True)\n",
    "sample_num = annotation.shape[0]\n",
    "consent_remove_idx = []\n",
    "cohort_list = [\"ARIC\", \"COPDGene\", \"CHS\"]\n",
    "for sample_i in range(sample_num):\n",
    "    tmp_consent = annotation.loc[sample_i, \"consent\"]\n",
    "    tmp_cohort = annotation.loc[sample_i, \"study\"]\n",
    "    tmp_consent_list = tmp_consent.split(\"-\")\n",
    "    if (tmp_cohort in cohort_list) and (tmp_consent_list[0] == \"DS\"):\n",
    "        consent_remove_idx.append(sample_i)\n",
    "annotation_remove_consent = annotation.drop(axis = 0, labels = consent_remove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_remove_consent_filename = \"freeze8_anno05_af02.tsv\"\n",
    "annotation_remove_consent_dir_filename = os.path.join(annotation_dir, annotation_remove_consent_filename)\n",
    "annotation_remove_consent.to_csv(annotation_remove_consent_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Remove Duplicates from ToPMed Duplicates List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "dup_list_filename = \"freeze8_duplicates_2019-04-19.txt\"\n",
    "dup_list_dir_filename = os.path.join(annotation_dir, dup_list_filename)\n",
    "dup_list = pd.read_csv(dup_list_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "dup_list.sort_values(by = [\"study1\"], inplace = True)\n",
    "\n",
    "annotation_filename = \"freeze8_anno05_af02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_list_control = dup_list.loc[dup_list.loc[:, \"study1\"] == \"Control\", :]\n",
    "dup_list_control_list = dup_list_control[\"ID2\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_list_norm = dup_list.loc[dup_list.loc[:, \"study1\"] != \"Control\", :]\n",
    "norm_num = dup_list_norm.shape[0]\n",
    "np.random.seed(43329)\n",
    "norm_idx_list = np.random.randint(2, size=norm_num)\n",
    "dup_list_norm_list = []\n",
    "for sample_i in range(norm_num):\n",
    "    norm_idx = norm_idx_list[sample_i]\n",
    "    tmp_sample_id = dup_list_norm.iloc[sample_i, norm_idx]\n",
    "    dup_list_norm_list.append(tmp_sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138092, 7)\n",
      "(136047, 7)\n"
     ]
    }
   ],
   "source": [
    "dup_id_list = dup_list_control_list + dup_list_norm_list\n",
    "print(annotation.shape)\n",
    "annotation_unique = annotation[~annotation['NWDID'].isin(dup_id_list)]\n",
    "print(annotation_unique.shape)\n",
    "annotation_unique_filename = \"freeze8_anno05_af02_unique01.tsv\"\n",
    "annotation_unique_dir_filename = os.path.join(annotation_dir, annotation_unique_filename)\n",
    "annotation_unique.to_csv(annotation_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Duplicates of unique_subject_key in freeze8 annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWDID</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>consent</th>\n",
       "      <th>study</th>\n",
       "      <th>sex</th>\n",
       "      <th>exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NWDID, unique_subject_key, subject_id, consent, study, sex, exclude]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique01.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dup_df = df_extraction_duplicates(annotation, \"unique_subject_key\")\n",
    "print(annotation_dup_df.shape)\n",
    "annotation_dup_df = annotation_dup_df.sort_values(by=['unique_subject_key'])\n",
    "display(annotation_dup_df)\n",
    "\n",
    "annotation_dup_filename = \"freeze8_anno05_af02_unique01_dup.tsv\"\n",
    "annotation_dup_dir_filename = os.path.join(annotation_dir, annotation_dup_filename)\n",
    "annotation_dup_df.to_csv(annotation_dup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "invar_subsets = [\"consent\", \"sex\"]\n",
    "rm_header = \"NWDID\"\n",
    "annotation_unique = remove_dup_anno(annotation, invar_subsets, rm_header)\n",
    "annotation_unique_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_unique_dir_filename = os.path.join(annotation_dir, annotation_unique_filename)\n",
    "annotation_unique.to_csv(annotation_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Duplicates of NWDID in freeze8 annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(duplicates_num(annotation, \"NWDID\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Process the overlap between 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133280, 18)\n",
      "(133280, 18)\n",
      "(138934, 18)\n",
      "(133280, 18)\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation_drop_subject_key = annotation.dropna(axis = 0, subset = [\"unique_subject_key\"], how = \"any\")\n",
    "print(annotation_drop_subject_key.shape)\n",
    "annotation_drop_subject_id = annotation.dropna(axis = 0, subset = [\"subject_id\"], how = \"any\")\n",
    "print(annotation_drop_subject_id.shape)\n",
    "annotation_drop_study = annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\")\n",
    "print(annotation_drop_study.shape)\n",
    "annotation.dropna(axis = 0, subset = [\"unique_subject_key\", \"subject_id\", \"NWDID\"], how = \"any\", inplace = True)\n",
    "print(annotation.shape)\n",
    "annotation_select = annotation[[\"NWDID\", \"subject_id\", \"unique_subject_key\", \"consent\", \"study\", \"sex\", \"exclude\"]]\n",
    "annotation_select.rename(columns = {\"subject_id\":\"SUBJECT_ID\", \"study\":\"cohort\"}, inplace = True)\n",
    "annotation_select_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_select_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_select_dir_filename = os.path.join(annotation_select_dir, annotation_select_filename)\n",
    "#annotation_select.to_csv(annotation_select_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overlap b/w 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66696, 47)\n",
      "(66696, 47)\n",
      "0\n",
      "There is no duplicates in NWDID in the merged table of phenotype and annotation.\n"
     ]
    }
   ],
   "source": [
    "pheno_filename = \"coh03_pheno02_pre.tsv\"\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.drop(axis = 1, labels = [\"SUBJECT_ID\", \"cohort\"], inplace = True)\n",
    "\n",
    "pheno_annotation = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation.shape)\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], inplace = True)\n",
    "print(pheno_annotation.shape)\n",
    "print(duplicates_num(pheno_annotation, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in the merged table of phenotype and annotation.\")\n",
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_dir, pheno_annotation_filename)\n",
    "pheno_annotation.to_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46105, 48)\n",
      "(46105, 47)\n",
      "(46105, 48)\n",
      "(55366, 49)\n",
      "(66696, 49)\n",
      "(66696, 47)\n",
      "(66696, 47)\n",
      "0\n",
      "There is no duplicates in NWDID in the merged table of phenotype and annotation.\n"
     ]
    }
   ],
   "source": [
    "pheno_filename = \"coh03_pheno02_pre.tsv\"\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_subjectid_uni = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_uni.shape)\n",
    "pheno_annotation_subjectid_cohort_uni = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_cohort_uni.shape)\n",
    "pheno_annotation_subjectid_cohort = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"cohort\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_cohort.shape)\n",
    "pheno_annotation_subjectid = pheno.merge(annotation, on = [\"SUBJECT_ID\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid.shape)\n",
    "pheno_annotation_uni = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation_uni.shape)\n",
    "\n",
    "annotation.drop(axis = 1, labels = [\"SUBJECT_ID\", \"cohort\"], inplace = True)\n",
    "\n",
    "pheno_annotation = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation.shape)\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], inplace = True)\n",
    "print(pheno_annotation.shape)\n",
    "print(duplicates_num(pheno_annotation, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in the merged table of phenotype and annotation.\")\n",
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_dir, pheno_annotation_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>cohort</th>\n",
       "      <th>hemoglobin_mcnc_bld_1</th>\n",
       "      <th>age_at_hemoglobin_mcnc_bld_1</th>\n",
       "      <th>hematocrit_vfr_bld_1</th>\n",
       "      <th>age_at_hematocrit_vfr_bld_1</th>\n",
       "      <th>rbc_ncnc_bld_1</th>\n",
       "      <th>age_at_rbc_ncnc_bld_1</th>\n",
       "      <th>mcv_entvol_rbc_1</th>\n",
       "      <th>...</th>\n",
       "      <th>gengrp6</th>\n",
       "      <th>WEIGHT_FINAL_NORM_OVERALL</th>\n",
       "      <th>CENTER</th>\n",
       "      <th>NWDID</th>\n",
       "      <th>consent</th>\n",
       "      <th>sex</th>\n",
       "      <th>exclude</th>\n",
       "      <th>age_at_DDIMER</th>\n",
       "      <th>DDIMER</th>\n",
       "      <th>sample_remove_DDIMER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53932</th>\n",
       "      <td>131150792</td>\n",
       "      <td>ARIC_131150792</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355569</td>\n",
       "      <td>HMB-IRB</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68851</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52892</th>\n",
       "      <td>131186968</td>\n",
       "      <td>ARIC_131186968</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355606</td>\n",
       "      <td>HMB-IRB</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69022</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>9229</td>\n",
       "      <td>FHS_9229</td>\n",
       "      <td>FHS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD611521</td>\n",
       "      <td>HMB-IRB-NPU-MDS</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68671</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FHS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD611521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>23500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31155</th>\n",
       "      <td>823218</td>\n",
       "      <td>WHI_823218</td>\n",
       "      <td>WHI</td>\n",
       "      <td>13.2</td>\n",
       "      <td>78.25</td>\n",
       "      <td>39.9</td>\n",
       "      <td>78.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD632820</td>\n",
       "      <td>HMB-IRB-NPU</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69026</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD632820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>25220.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SUBJECT_ID unique_subject_key cohort  hemoglobin_mcnc_bld_1  \\\n",
       "53932  131150792     ARIC_131150792   ARIC                    NaN   \n",
       "68851        NaN                NaN   ARIC                    NaN   \n",
       "52892  131186968     ARIC_131186968   ARIC                    NaN   \n",
       "69022        NaN                NaN   ARIC                    NaN   \n",
       "59496       9229           FHS_9229    FHS                    NaN   \n",
       "68671        NaN                NaN    FHS                    NaN   \n",
       "31155     823218         WHI_823218    WHI                   13.2   \n",
       "69026        NaN                NaN    WHI                    NaN   \n",
       "\n",
       "       age_at_hemoglobin_mcnc_bld_1  hematocrit_vfr_bld_1  \\\n",
       "53932                           NaN                   NaN   \n",
       "68851                           NaN                   NaN   \n",
       "52892                           NaN                   NaN   \n",
       "69022                           NaN                   NaN   \n",
       "59496                           NaN                   NaN   \n",
       "68671                           NaN                   NaN   \n",
       "31155                         78.25                  39.9   \n",
       "69026                           NaN                   NaN   \n",
       "\n",
       "       age_at_hematocrit_vfr_bld_1  rbc_ncnc_bld_1  age_at_rbc_ncnc_bld_1  \\\n",
       "53932                          NaN             NaN                    NaN   \n",
       "68851                          NaN             NaN                    NaN   \n",
       "52892                          NaN             NaN                    NaN   \n",
       "69022                          NaN             NaN                    NaN   \n",
       "59496                          NaN             NaN                    NaN   \n",
       "68671                          NaN             NaN                    NaN   \n",
       "31155                        78.25             NaN                    NaN   \n",
       "69026                          NaN             NaN                    NaN   \n",
       "\n",
       "       mcv_entvol_rbc_1  ...  gengrp6  WEIGHT_FINAL_NORM_OVERALL  CENTER  \\\n",
       "53932               NaN  ...      NaN                        NaN     NaN   \n",
       "68851               NaN  ...      NaN                        NaN     NaN   \n",
       "52892               NaN  ...      NaN                        NaN     NaN   \n",
       "69022               NaN  ...      NaN                        NaN     NaN   \n",
       "59496               NaN  ...      NaN                        NaN     NaN   \n",
       "68671               NaN  ...      NaN                        NaN     NaN   \n",
       "31155               NaN  ...      NaN                        NaN     NaN   \n",
       "69026               NaN  ...      NaN                        NaN     NaN   \n",
       "\n",
       "           NWDID          consent  sex  exclude  age_at_DDIMER   DDIMER  \\\n",
       "53932  NWD355569          HMB-IRB    M     True            NaN      NaN   \n",
       "68851  NWD355569              NaN    F      NaN           61.0  48000.0   \n",
       "52892  NWD355606          HMB-IRB    M     True            NaN      NaN   \n",
       "69022  NWD355606              NaN    F      NaN           57.0  25000.0   \n",
       "59496  NWD611521  HMB-IRB-NPU-MDS    F    False            NaN      NaN   \n",
       "68671  NWD611521              NaN    M      NaN           61.0  23500.0   \n",
       "31155  NWD632820      HMB-IRB-NPU    M    False            NaN      NaN   \n",
       "69026  NWD632820              NaN    F      NaN           78.0  25220.0   \n",
       "\n",
       "       sample_remove_DDIMER  \n",
       "53932                   NaN  \n",
       "68851                   0.0  \n",
       "52892                   NaN  \n",
       "69022                   0.0  \n",
       "59496                   NaN  \n",
       "68671                   0.0  \n",
       "31155                   NaN  \n",
       "69026                   0.0  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(load_dir, ddimer_filename)\n",
    "ddimer = pd.read_csv(ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(load_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_ddimer_comparesex = pheno_annotation.merge(ddimer, on = [\"NWDID\", \"cohort\", \"sex\"], how = \"outer\")\n",
    "print(duplicates_num(pheno_annotation_ddimer_comparesex, \"NWDID\"))\n",
    "pheno_annotation_ddimer_nocomparesex = pheno_annotation.merge(ddimer, on = [\"NWDID\", \"cohort\"], how = \"outer\")\n",
    "print(duplicates_num(pheno_annotation_ddimer_nocomparesex, \"NWDID\"))\n",
    "\n",
    "boolean_series = pheno_annotation_ddimer_comparesex[[\"NWDID\"]].duplicated(keep=False)\n",
    "pheno_annotation_ddimer_comparesex_dup = pheno_annotation_ddimer_comparesex.loc[boolean_series, :]\n",
    "pheno_annotation_ddimer_comparesex_dup.sort_values(axis = 0, by = \"NWDID\", inplace = True)\n",
    "\n",
    "ddimer_anno_sex_diff_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_anno_sex_diff_filename = \"ddimer_anno_sex_diff.tsv\"\n",
    "ddimer_anno_sex_diff_dir_filename = os.path.join(ddimer_anno_sex_diff_dir, ddimer_anno_sex_diff_filename)\n",
    "pheno_annotation_ddimer_comparesex_dup.to_csv(ddimer_anno_sex_diff_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "display(pheno_annotation_ddimer_comparesex_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually remove the samples with different sex but the same NWDID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69394, 50)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pheno_annotation_ddimer = pheno_annotation_ddimer_comparesex.drop(labels = [53932, 52892, 59496, 31155], axis = 0)\n",
    "print(pheno_annotation_ddimer.shape)\n",
    "print(duplicates_num(pheno_annotation_ddimer, \"NWDID\"))\n",
    "pheno_annotation_ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_ddimer_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_ddimer_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_ddimer_filename)\n",
    "pheno_annotation_ddimer.to_csv(pheno_annotation_ddimer_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of phenotypes that can maps to annotation file based on cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (38,40,42,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"gengrp6\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\", \"DDIMER\", \"EGFRCKDEPI\", \"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\",\n",
    "            \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\", \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\",\n",
    "            \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\",\n",
    "            \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], how = \"any\", inplace = True)\n",
    "cat_col = \"cohort\"\n",
    "pheno_annotation_summary, category_list = categorize_df(pheno_annotation, cat_col)\n",
    "pheno_annotation_category_df = pd.DataFrame(columns = col_list, index = category_list)\n",
    "for category in category_list:\n",
    "    for col in col_list:\n",
    "        tmp_df = pheno_annotation.loc[pheno_annotation[\"cohort\"] == category, col]\n",
    "        tmp_df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "        pheno_annotation_category_df.loc[category, col] = tmp_df.shape[0]\n",
    "pheno_annotation_category_df.sort_index(axis = 0, inplace = True)\n",
    "pheno_annotation_category_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pheno_annotation_category_filename = \"overlap_pheno_annotation_noexclude_summary.tsv\"\n",
    "pheno_annotation_category_dir_filename = os.path.join(pheno_annotation_category_dir, pheno_annotation_category_filename)\n",
    "pheno_annotation_category_df.to_csv(pheno_annotation_category_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overlap b/w 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess CN sample list to get list of cram of each cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = [\"freeze6-AA\", \"JHS\", \"OMG-SCD\", \"PAGE\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SCD\", \"SOL\", \"walk-PHaSST\"]\n",
    "cohort_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "for cohort in cohort_list:\n",
    "    tmp_cohort_dir_filename = os.path.join(cohort_dir, \"%s_chr16_full_sample.list\"%cohort)\n",
    "    tmp_cohort = pd.read_csv(tmp_cohort_dir_filename, sep = \"/\", header = None, index_col = None)\n",
    "    tmp_sample_list = tmp_cohort.iloc[:, -1].values.tolist()\n",
    "    tmp_sample_id_list = []\n",
    "    for tmp_sample in tmp_sample_list:\n",
    "        tmp_sample_id = tmp_sample.split(\".\")[0]\n",
    "        tmp_sample_id_list.append(tmp_sample_id)\n",
    "    tmp_sample_id_df = pd.DataFrame(data = tmp_sample_id_list)\n",
    "    tmp_sample_id_dir = cohort_dir\n",
    "    tmp_sample_id_filename = \"%s_cram.tsv\"%cohort\n",
    "    tmp_sample_id_dir_filename = os.path.join(tmp_sample_id_dir, tmp_sample_id_filename)\n",
    "    tmp_sample_id_df.to_csv(tmp_sample_id_dir_filename, sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap b/w samples we have CN and pheno_anno got in section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "col_list = [\"gengrp6\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\", \"DDIMER\", \"EGFRCKDEPI\", \"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\",\n",
    "            \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\", \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\",\n",
    "            \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\",\n",
    "            \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "\n",
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"HCHS_SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "cn_pheno_anno_summary = pd.DataFrame(columns = col_list, index = cohort_list)\n",
    "for cohort in cohort_list:\n",
    "    if cohort == \"HCHS_SOL\":\n",
    "        cohort_in_filename = \"SOL\"\n",
    "    else:\n",
    "        cohort_in_filename = cohort\n",
    "    cohort_filename = \"%s_cram.tsv\"%cohort_in_filename\n",
    "    cohort_dir_filename = os.path.join(load_dir, cohort_filename)\n",
    "    cohort_df = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = None, index_col = None)\n",
    "    cohort_df.rename(columns = {0:\"NWDID\"}, inplace = True)\n",
    "    cohort_pheno = cohort_df.merge(pheno_annotation, on = \"NWDID\", how = \"inner\")\n",
    "    for col in col_list:\n",
    "        tmp_df = cohort_pheno.loc[cohort_pheno[\"cohort\"] == cohort, col]\n",
    "        tmp_df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "        cn_pheno_anno_summary.loc[cohort, col] = tmp_df.shape[0]\n",
    "cn_pheno_anno_summary.sort_index(axis = 0, inplace = True)\n",
    "cn_pheno_anno_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cn_pheno_anno_summary_filename = \"overlap_cn_pheno_anno.tsv\"\n",
    "cn_pheno_anno_summary_dir_filename = os.path.join(cn_pheno_anno_summary_dir, cn_pheno_anno_summary_filename)\n",
    "cn_pheno_anno_summary.to_csv(cn_pheno_anno_summary_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap between CN samples and annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SCD\", \"SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "cohort_dir_list = [os.path.join(load_dir, \"%s_cram.tsv\"%cohort) for cohort in cohort_list]\n",
    "commom_col = \"col\"\n",
    "map_raw_df = annotation\n",
    "save_dir = load_dir\n",
    "cohort_cn_map_summary = cn_map_list(cohort_list, cohort_dir_list, map_raw_df, common_col, save_dir)\n",
    "cohort_cn_map_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cohort_cn_map_summary_filename = \"cohort_cn_annotation_overlap_summary.tsv\"\n",
    "cohort_cn_map_summary_dir_filename = os.path.join(cohort_cn_map_summary_dir, cohort_cn_map_summary_filename)\n",
    "cohort_cn_map_summary.to_csv(cohort_cn_map_summary_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap between (Overlap between CN samples and annotation files) and blood cell traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\"]\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh03_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "for cohort in cohort_list[:1]:\n",
    "    cohort_filename = \"%s_subject_id_cram.tsv\"%cohort\n",
    "    cohort_dir_filename = os.path.join(load_dir, cohort_filename)\n",
    "    cohort_df = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    cohort_pheno = cohort_df.merge(pheno, on = \"unique_subject_key\", how = \"inner\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare JHS phenotype and Annotated JHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3406, 36)\n",
      "(3406, 3)\n",
      "(3406, 38)\n",
      "So all the annotated JHS samples having phenotypes listed in the JHS phenotype file I used at the beginning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "jhs_pheno_filename = \"jhs_basic_phenotypes_05072019.txt\"\n",
    "jhs_pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_pheno_dir_filename = os.path.join(jhs_pheno_dir, jhs_pheno_filename)\n",
    "jhs_pheno = pd.read_csv(jhs_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(jhs_pheno.shape)\n",
    "\n",
    "jhs_anno_filename = \"JHS_subject_id_cram.tsv\"\n",
    "jhs_anno_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "jhs_anno_dir_filename = os.path.join(jhs_anno_dir, jhs_anno_filename)\n",
    "jhs_anno = pd.read_csv(jhs_anno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(jhs_anno.shape)\n",
    "\n",
    "jhs_pheno_anno_raw = jhs_pheno.merge(jhs_anno, on = \"NWDID\", how = \"inner\")\n",
    "print(jhs_pheno_anno_raw.shape)\n",
    "print(\"So all the annotated JHS samples having phenotypes listed in the JHS phenotype file I used at the beginning.\")\n",
    "\n",
    "jhs_pheno_anno_raw[\"wbc_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"neutrophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"lymphocyte_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"basophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"eosinophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"monocyte_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"cohort\"] = jhs_pheno_anno_raw.shape[0] * [\"JHS\"]\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "jhs_pheno_anno = jhs_pheno_anno_raw[[\"subject_id\", \"unique_subject_key\", \"cohort\", \"age\"] + col_list]\n",
    "jhs_pheno_anno.rename(columns = {\"subject_id\":\"SUBJECT_ID\"}, inplace = True)\n",
    "for col in col_list:\n",
    "    jhs_pheno_anno[\"age_at_%s\"%col] = jhs_pheno_anno[\"age\"]\n",
    "jhs_pheno_anno.drop(axis = 1, labels = \"age\", inplace = True)\n",
    "\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh02_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "pheno[\"lnHBA1C\"] = pheno.shape[0] * [None]\n",
    "pheno[\"age_at_lnHBA1C\"] = pheno.shape[0] * [None]\n",
    "pheno_header = list(pheno)\n",
    "jhs_pheno_anno_copy = jhs_pheno_anno[pheno_header].copy()\n",
    "\n",
    "jhs_pheno_anno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_pheno_anno_filename = \"jhs_basic_phenotypes_05072019_blood_cell_traits.tsv\"\n",
    "jhs_pheno_anno_dir_filename = os.path.join(jhs_pheno_anno_dir, jhs_pheno_anno_filename)\n",
    "jhs_pheno_anno_copy.to_csv(jhs_pheno_anno_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. uPAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uPAR is the name of the genes we are interested in, and APOL1 is the name of the diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert eGFR ID to Annotation File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Keep AA and Hispanics for eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_unique_filename = \"egfr03-01_unique.tsv\"\n",
    "egfr_unique_dir_filename = os.path.join(egfr_dir, egfr_unique_filename)\n",
    "egfr_unique = pd.read_csv(egfr_unique_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_unique[\"AA\"] = 0\n",
    "egfr_unique.loc[egfr_unique.loc[:, \"race\"] == 2, \"AA\"] = 1\n",
    "egfr_sample_num = egfr_unique.shape[0]\n",
    "rm_idx_list = []\n",
    "for sample_i in range(egfr_sample_num):\n",
    "    tmp_AA = egfr_unique.loc[sample_i, \"AA\"]\n",
    "    tmp_ethnicity = egfr_unique.loc[sample_i, \"ethnicity\"]\n",
    "    if tmp_AA == 0 and tmp_ethnicity == 0:\n",
    "        rm_idx_list.append(sample_i)\n",
    "egfr = egfr_unique.drop(labels = rm_idx_list, axis = 0)\n",
    "egfr_filename = \"egfr03-01_unique_AA_HS.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr.to_csv(egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Map eGFR to Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_filename = \"egfr03-01_unique_AA_HS.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\", \"male\"]\n",
    "annotation_col = [\"unique_subject_key\", \"subject_id\", \"study\", \"sex\"]\n",
    "pheno_prefix = \"btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex\"\n",
    "anno_prefix = \"freeze8_anno04_af02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "anno_ddimer_btc_adad_egfr = map_annotation(btc_adad_egfr, anno_ddimer, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                                           pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_col = [\"unique_subject_key\", \"male\", \"SUBJECT_ID\"]\n",
    "annotation_col = [\"unique_subject_key\", \"sex\", \"subject_id\"]\n",
    "#pheno_col = [\"unique_subject_key\", \"male\"]\n",
    "#annotation_col = [\"unique_subject_key\", \"sex\"]\n",
    "pheno_prefix = \"egfr\"\n",
    "anno_prefix = \"freeze8_anno05_af02_unique02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "\n",
    "egfr_mapped = map_annotation(egfr, annotation, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                             pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coding APOL1 Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs71785313\", \"rs73885319\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "apol1_snp = read_snp_list_together(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = apol1_snp.shape[0]\n",
    "apol1 = pd.DataFrame(columns = [\"NWDID\", \"APOL1\"])\n",
    "apol1[\"NWDID\"] = apol1_snp[\"NWDID\"]\n",
    "apol1[\"APOL1\"] = apol1_snp[\"rs71785313\"] + apol1_snp[\"rs73885319\"]\n",
    "apol1.loc[apol1.loc[:, \"APOL1\"] != 2, \"APOL1\"] = 0\n",
    "apol1.loc[apol1.loc[:, \"APOL1\"] == 2, \"APOL1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "apol1_filename = \"APOL1_status.tsv\"\n",
    "apol1_dir = os.path.join(\"..\", \"cohort\", \"APOL1\")\n",
    "apol1_dir_filename = os.path.join(apol1_dir, apol1_filename)\n",
    "apol1.to_csv(apol1_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correctness of rs334 freeze8 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103646, 3)\n",
      "(103645, 2)\n"
     ]
    }
   ],
   "source": [
    "rs334_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs334_freeze6a_filename = \"snp_rs334_hetero.tsv\"\n",
    "rs334_freeze8_filename = \"freeze8_rs334_hetero.tsv\"\n",
    "\n",
    "rs334_freeze6a_dir_filename = os.path.join(rs334_dir, rs334_freeze6a_filename)\n",
    "rs334_freeze8_dir_filename = os.path.join(rs334_dir, rs334_freeze8_filename)\n",
    "\n",
    "rs334_freeze6a = pd.read_csv(rs334_freeze6a_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "rs334_freeze8 = pd.read_csv(rs334_freeze8_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_base_NWDID = rs334_freeze6a.merge(rs334_freeze8, on = \"NWDID\", how = \"inner\")\n",
    "print(rs334_base_NWDID.shape)\n",
    "rs334_base_NWDID_filename = \"rs334_base_NWDID.tsv\"\n",
    "rs334_base_NWDID_dir_filename = os.path.join(rs334_dir, rs334_base_NWDID_filename)\n",
    "rs334_base_NWDID.to_csv(rs334_base_NWDID_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "rs334_base_NWDID_geno = rs334_freeze6a.merge(rs334_freeze8, on = [\"NWDID\", \"geno\"], how = \"inner\")\n",
    "print(rs334_base_NWDID_geno.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. rs399145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocess Raw rs399145 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs399145\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "rs399145 = read_snp_list_together(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs399145_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs399145_filename = \"%s_rs399145_hetero.tsv\"%snp_ver\n",
    "rs399145_dir_filename = os.path.join(rs399145_dir, rs399145_filename)\n",
    "rs399145.to_csv(rs399145_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 (All Population) Find Two Copies of Minor Allele Individuals of rs399145 while rs334 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs399145_filename = \"freeze8_rs399145_hetero.tsv\"\n",
    "rs399145_dir_filename = os.path.join(snp_dir, rs399145_filename)\n",
    "rs399145 = pd.read_csv(rs399145_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_filename = \"freeze8_rs334_hetero.tsv\"\n",
    "rs334_dir_filename = os.path.join(snp_dir, rs334_filename)\n",
    "rs334 = pd.read_csv(rs334_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 3)\n"
     ]
    }
   ],
   "source": [
    "rs334_rs399145 = rs334.merge(rs399145, on = \"NWDID\", how = \"inner\")\n",
    "rs334_heter_rs399145 = rs334_rs399145.loc[rs334_rs399145.loc[:, \"rs334\"] == 1, :]\n",
    "rs334_heter_rs399145_homo = rs334_heter_rs399145.loc[rs334_heter_rs399145.loc[:, \"rs399145\"] == 2, :]\n",
    "print(rs334_heter_rs399145_homo.shape)\n",
    "rs334_heter_rs399145_homo_dir = os.path.join(\"..\", \"data_summary\")\n",
    "rs334_heter_rs399145_homo_filename = \"freeze8_rs334-heter_rs399145-2.tsv\"\n",
    "rs334_heter_rs399145_homo_dir_filename = os.path.join(rs334_heter_rs399145_homo_dir, rs334_heter_rs399145_homo_filename)\n",
    "rs334_heter_rs399145_homo.to_csv(rs334_heter_rs399145_homo_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4)\n"
     ]
    }
   ],
   "source": [
    "rs334_rs399145_dir = os.path.join(\"..\", \"cohort\", \"APOL1\", \"ready_data\")\n",
    "rs334_rs399145_filename = \"common_rs399145-rs334.tsv\"\n",
    "rs334_rs399145_dir_filename = os.path.join(rs334_rs399145_dir, rs334_rs399145_filename)\n",
    "rs334_rs399145 = pd.read_csv(rs334_rs399145_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_heter_rs399145 = rs334_rs399145.loc[rs334_rs399145.loc[:, \"rs334\"] == 1, :]\n",
    "rs334_heter_rs399145_homo = rs334_heter_rs399145.loc[rs334_heter_rs399145.loc[:, \"rs399145\"] == 2, :]\n",
    "print(rs334_heter_rs399145_homo.shape)\n",
    "\n",
    "rs334_heter_rs399145_homo_dir = os.path.join(\"..\", \"data_summary\")\n",
    "rs334_heter_rs399145_homo_filename = \"freeze8_rs334-heter_rs399145-2_used.tsv\"\n",
    "rs334_heter_rs399145_homo_dir_filename = os.path.join(rs334_heter_rs399145_homo_dir, rs334_heter_rs399145_homo_filename)\n",
    "rs334_heter_rs399145_homo.to_csv(rs334_heter_rs399145_homo_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs2302524\", \"rs2633317\", \"rs4251805\", \"rs4760\", \"rs73935023\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "snp_list = read_snp_list_each(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "There is no duplicates in NWDID in DDIMER I processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ddimer_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer_raw = pd.read_csv(ddimer_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "ddimer = ddimer_raw.loc[:, [\"sample.id\", \"STUDY\", \"sex\", \"AGE_DDIMER\", \"DDIMER\", \"sample_remove_DDIMER\"]]\n",
    "ddimer.dropna(axis = \"index\", how = \"any\", inplace = True)\n",
    "ddimer.rename(columns = {\"sample.id\":\"NWDID\", \"STUDY\":\"cohort\", \"AGE_DDIMER\":\"age_at_DDIMER\"}, inplace = True)\n",
    "sex_dict = {\"M\":1, \"F\":0}\n",
    "ddimer.replace({\"sex\": sex_dict}, inplace = True)\n",
    "save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(save_dir, ddimer_filename)\n",
    "ddimer.to_csv(ddimer_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "print(duplicates_num(ddimer, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in DDIMER I processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A\n",
       "0   0.0\n",
       "1   0.0\n",
       "2   1.0\n",
       "3   1.0\n",
       "4   0.0\n",
       "5   0.0\n",
       "6   1.0\n",
       "7   0.0\n",
       "8   1.0\n",
       "9   0.0\n",
       "10  1.0\n",
       "11  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(df1.apply(func, axis = 1), columns = ['A'])\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B\n",
       "0   0.0  0\n",
       "1   0.0  1\n",
       "2   1.0  1\n",
       "3   1.0  1\n",
       "4   0.0  0\n",
       "5   0.0  0\n",
       "6   1.0  1\n",
       "7   1.0  0\n",
       "8   1.0  1\n",
       "9   0.0  0\n",
       "10  NaN  0\n",
       "11  0.0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B\n",
       "0   0.0  0\n",
       "1   0.0  1\n",
       "2   1.0  1\n",
       "3   1.0  1\n",
       "4   0.0  0\n",
       "5   0.0  0\n",
       "6   1.0  1\n",
       "7   1.0  0\n",
       "8   1.0  1\n",
       "9   0.0  0\n",
       "10  0.0  0\n",
       "11  0.0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = pd.DataFrame({'A': [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, None, 0], 'B': [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0]})\n",
    "display(df4)\n",
    "df4.iloc[:, 0] = df4.apply(func, axis = 1)\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sample_lost = df4[df4.iloc[:, 0].isnull()].index.tolist()\n",
    "print(sample_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  a  3\n",
       "1  g  4\n",
       "2  c  5\n",
       "3  d  6\n",
       "4  k  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B      C\n",
       "0  a NaN   nice\n",
       "1  b NaN   good\n",
       "2  c NaN   exce\n",
       "3  d NaN    ama\n",
       "4  e NaN    awe\n",
       "5  f NaN    NaN\n",
       "6  g NaN   perf\n",
       "7  h NaN  terri\n",
       "8  i NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B      C\n",
       "A            \n",
       "a  3.0   nice\n",
       "b  NaN   good\n",
       "c  5.0   exce\n",
       "d  6.0    ama\n",
       "e  NaN    awe\n",
       "f  NaN    NaN\n",
       "g  4.0   perf\n",
       "h  NaN  terri\n",
       "i  NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B      C\n",
       "A            \n",
       "a  3.0   nice\n",
       "b  NaN   good\n",
       "c  5.0   exce\n",
       "d  6.0    ama\n",
       "e  NaN    awe\n",
       "f  NaN    NaN\n",
       "g  4.0   perf\n",
       "h  NaN  terri\n",
       "i  NaN   well\n",
       "k  7.0    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A    B      C\n",
       "0  a  3.0   nice\n",
       "1  b  NaN   good\n",
       "2  c  5.0   exce\n",
       "3  d  6.0    ama\n",
       "4  e  NaN    awe\n",
       "5  f  NaN    NaN\n",
       "6  g  4.0   perf\n",
       "7  h  NaN  terri\n",
       "8  i  NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df0 = pd.DataFrame({'A': [\"a\", \"g\", \"c\", \"d\", \"k\"],\n",
    "                    'B': [3, 4, 5, 6, 7]})\n",
    "df1 = pd.DataFrame({'A': [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "                    'B': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "                    'C': [\"nice\", \"good\", \"exce\", \"ama\", \"awe\", np.nan, \"perf\", \"terri\", \"well\"]})\n",
    "display(df0)\n",
    "display(df1)\n",
    "df1 = df1.set_index('A')\n",
    "df0 = df0.set_index('A')\n",
    "df3 = df1.fillna(df0)\n",
    "df4 = df1.combine_first(df0)\n",
    "display(df3)\n",
    "display(df4)\n",
    "df3 = df3.reset_index()\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df0.merge(df1, left_on = ['B'], right_on = ['C'], how = \"inner\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation(pheno, annotation, pheno_col, annotation_col, pivot_col, how_merge, pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir):\n",
    "    pheno_mapped = annotation.merge(pheno, left_on = annotation_col[0], right_on = pheno_col[0], how = merge_how)\n",
    "    col_num = len(pheno_col[1:])\n",
    "    for col_i in range(1, col_num + 1):\n",
    "        if pheno_col[col_i] != annotation_col[col_i]:\n",
    "            pheno_mapped.drop(axis = 1, columns = [pheno_col[col_i], annotation_col[col_i]], inplace = True)\n",
    "        else:\n",
    "            tmp_pheno_col = \"%s_y\"%pheno_col[col_i]\n",
    "            tmp_annotation_col = \"%s_x\"%annotation_col[col_i]\n",
    "            pheno_mapped.drop(axis = 1, columns = [tmp_pheno_col, tmp_annotation_col], inplace = True)\n",
    "    if len(annotation_col) > 1:\n",
    "        depre_sample_list, pivot_col_df = map_deprecation_list(pheno, annotation, pheno_col, annotation_col,\n",
    "                                                               pivot_col, pheno_prefix, anno_prefix, diff_save_dir)\n",
    "        if depre_sample_list != []:\n",
    "            pheno_mapped_raw = pheno_mapped.copy()\n",
    "            del pheno_mapped\n",
    "            pheno_mapped = pheno_mapped_raw[~pheno_mapped_raw[pivot_col].isin(depre_sample_list)]\n",
    "        pheno_mapped = pheno_mapped.merge(pivot_col_df, on = pivot_col, how = \"inner\")\n",
    "    pheno_mapped_filename = \"%s_%s.tsv\"%(anno_prefix, pheno_prefix)\n",
    "    pheno_mapped_dir_filename = os.path.join(mapped_save_dir, pheno_mapped_filename)\n",
    "    pheno_mapped.to_csv(pheno_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return pheno_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation(df, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    df_pivot = df[[pivot_col]]\n",
    "    df_col = df[[pheno_col, annotation_col]]\n",
    "    df_col.iloc[:, 0] = df_col.apply(func, axis = 1)\n",
    "    df_col.iloc[:, 1] = df_col.apply(func, axis = 1)\n",
    "    del df\n",
    "    df = pd.concat([df_pivot, df_col], axis = 1)\n",
    "    depre_sample_index_list = df[df.iloc[:, 0].isnull()].index.tolist()\n",
    "    if depre_sample_index_list == []:\n",
    "        depre_sample_list = []\n",
    "    else:\n",
    "        depre_sample_list = df.loc[depre_sample_index_list, pivot_col].values.reshape(1, -1).tolist()[0]\n",
    "    df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "    tmp_compare = df[pheno_col].eq(df[annotation_col], axis = 0)\n",
    "    df_dif = df[tmp_compare == False]\n",
    "    df_cons = df[tmp_compare == True]\n",
    "    if annotation_col.split('_x')[-1] == \"\":\n",
    "        annotation_col_propagate = annotation_col.split('_x')[0]\n",
    "        df_cons.rename(columns = {annotation_col:annotation_col_propagate}, inplace = True)\n",
    "    else:\n",
    "        annotation_col_propagate = annotation_col\n",
    "    df_cons_propagate = df_cons.loc[:, [pivot_col, annotation_col_propagate]]\n",
    "    if df_dif.shape[0] != 0:\n",
    "        print(\"%s is inconsistent.\"%pheno_col)\n",
    "        df_dif_filename = \"%s_%s_%s.tsv\"%(anno_prefix, pheno_prefix, annotation_col)\n",
    "        df_dif_dir_filename = os.path.join(save_dir, df_dif_filename)\n",
    "        df_dif.to_csv(df_dif_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        depre_sample_list = df_dif[[pivot_col]].values.reshape(1, -1).tolist()[0]\n",
    "    return depre_sample_list, df_cons_propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
