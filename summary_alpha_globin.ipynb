{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import feather\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"/proj/yunligrp/users/minzhi/custom_lib\" not in sys.path:\n",
    "    sys.path.insert(0, \"/proj/yunligrp/users/minzhi/custom_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_process_data_eqtl import *\n",
    "from function_asso import *\n",
    "from function_mesa_cca import *\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_map(cn, map_df, common_col):\n",
    "    cn_mapped = map_df.merge(cn, on = common_col, how = \"inner\")\n",
    "    return cn_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_map_list(cohort_list, cohort_dir_list, map_raw_df, common_col, save_dir):\n",
    "    cohort_cn_map_summary = pd.DataFrame(columns = [\"annotation\"], index = cohort_list)\n",
    "    for cohort_dir_filename, cohort_i in zip(cohort_dir_list, cohort_list):\n",
    "        cohort = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = None, index_col = None)\n",
    "        cohort.rename(columns={0:'NWDID'}, inplace = True)\n",
    "        map_df = map_raw_df[[\"NWDID\", \"unique_subject_key\", \"subject_id\"]]\n",
    "        map_df.dropna(axis = 0, subset = [\"unique_subject_key\", \"subject_id\"], how = \"any\", inplace = True)\n",
    "        common_col = \"NWDID\"\n",
    "        tmp_cn_mapped = cn_map(cohort, map_df, common_col)\n",
    "        tmp_cn_mapped_filename = \"%s_subject_id_cram.tsv\"%cohort_i\n",
    "        tmp_cn_mapped_dir_filename = os.path.join(save_dir, tmp_cn_mapped_filename)\n",
    "        tmp_cn_mapped.to_csv(tmp_cn_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        cohort_cn_map_summary.loc[cohort_i, \"annotation\"] = tmp_cn_mapped.shape[0]\n",
    "    cohort_cn_map_summary.sort_index(axis = 0, inplace = True)\n",
    "    return cohort_cn_map_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, cat_col, category_list, file_prefix, save_dir):\n",
    "    for category in category_list:\n",
    "        tmp_df = df.loc[df[cat_col] == category, :]\n",
    "        tmp_filename = \"%s_%s.tsv\"%(file_prefix, category)\n",
    "        tmp_dir_filename = os.path.join(save_dir, tmp_filename)\n",
    "        tmp_df.to_csv(tmp_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_df_pair(df1, df2, common_col, df1_col, df2_col):\n",
    "    df1_df2 = df1.merge(df2, on = common_col, how = \"inner\")\n",
    "    overlap_num = df1_df2.shape[0]\n",
    "    df1_num = df1.shape[0]\n",
    "    df2_num = df2.shape[0]\n",
    "    df1_nodf2_num = df1_num - overlap_num\n",
    "    df2_nodf1_num = df2_num - overlap_num\n",
    "    if df1_col == df2_col:\n",
    "        df1_col_x = \"%s_x\"%df1_col\n",
    "        df2_col_y = \"%s_y\"%df2_col\n",
    "    if (df1[df1_col].dtypes == \"float64\" or df1[df1_col].dtypes == \"int32\") and (df2[df2_col].dtypes == \"float64\" or df2[df2_col].dtypes == \"int32\"):\n",
    "        cor = pearsonr(df1_df2[df1_col_x], df1_df2[df2_col_y])[0]\n",
    "        return df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num, cor\n",
    "    else:\n",
    "        return df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_extraction_duplicates(df, col):\n",
    "    boolean_series = df[[col]].duplicated(keep = False)\n",
    "    duplicated_df = df.loc[boolean_series, :]\n",
    "    return duplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_egfr(egfr_dir, cohort_list, header_selection):\n",
    "    egfr_list = []\n",
    "    for cohort in cohort_list:\n",
    "        egfr_dir_filename = os.path.join(egfr_dir, \"egfr_calculated_%s.tsv\"%cohort)\n",
    "        egfr_raw = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "        egfr = egfr_raw[header_selection]\n",
    "        cohort_pheno_table = \"_\".join(cohort.split(\"-\"))\n",
    "        egfr[\"cohort\"] = egfr.shape[0] * [cohort_pheno_table]\n",
    "        egfr[\"unique_subject_key\"] = egfr.shape[0] * [None]\n",
    "        egfr_num = egfr.shape[0]\n",
    "        for egfr_i in range(egfr_num):\n",
    "            egfr.loc[egfr_i, \"unique_subject_key\"] = \"%s_%s\"%(cohort_pheno_table, egfr.loc[egfr_i, \"id\"])\n",
    "        egfr.rename(columns = {\"id\":\"SUBJECT_ID\", \"age\":\"age_at_EGFRCKDEPI\", \"EGFR\":\"EGFRCKDEPI\"}, inplace = True)\n",
    "        egfr_list.append(egfr)\n",
    "        print(\"%s completed.\"%cohort)\n",
    "    egfr_full = pd.concat(egfr_list, axis = 0)\n",
    "    egfr_full.reset_index(drop = True, inplace = True)\n",
    "    return egfr_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invar(df, col):\n",
    "    check_invar_list = df[col].values.tolist()\n",
    "    if len(set(check_invar_list)) == 1:\n",
    "        invar_bool = True\n",
    "    else:\n",
    "        invar_bool = False\n",
    "    return invar_bool\n",
    "\n",
    "def check_list_invar(df, col_list):\n",
    "    invar_list = []\n",
    "    for col in col_list:\n",
    "        invar_bool = check_invar(df, col)\n",
    "        invar_list.append(invar_bool)\n",
    "    return invar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup_anno(annotation, invar_subsets, rm_header = None):\n",
    "    if rm_header == None:\n",
    "        annotation[\"aux_rm\"] = np.arange(annotation.shape[0])\n",
    "        rm_header = \"aux_rm\"\n",
    "    annotation_dup = df_extraction_duplicates(annotation, \"unique_subject_key\")\n",
    "    _, usk_list = categorize_df(annotation_dup, \"unique_subject_key\")\n",
    "    remove_header_list = []\n",
    "    for usk in usk_list:\n",
    "        tmp_annotation = annotation_dup.loc[annotation_dup.loc[:, \"unique_subject_key\"] == usk, :]\n",
    "        tmp_invar_list = check_list_invar(tmp_annotation, invar_subsets)\n",
    "        tmp_rm_list = tmp_annotation[rm_header].values.tolist()\n",
    "        if False in tmp_invar_list:\n",
    "            remove_header_list = remove_header_list + tmp_rm_list\n",
    "        else:\n",
    "            tmp_rm_num = len(tmp_rm_list)\n",
    "            selection = random.randint(0, tmp_rm_num - 1)\n",
    "            remove_header_list = remove_header_list + [tmp_rm_list[selection]]\n",
    "    annotation_unique = annotation[~annotation[rm_header].isin(remove_header_list)]\n",
    "    if rm_header == \"aux_rm\":\n",
    "        annotation_unique.drop(axis = 1, labels = [\"aux_rm\"], inplace = True)\n",
    "    return annotation_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_replace_nan(df0_col, df1_col, df0, df1):\n",
    "    df0 = df0.set_index(df0_col)\n",
    "    df1 = df1.set_index(df1_col)\n",
    "    df0_filled = df0.fillna(df1)\n",
    "    df0_filled = df0_filled.reset_index()\n",
    "    return df0_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    if x.first_valid_index() is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x[x.first_valid_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation(pheno, annotation, pheno_col, annotation_col, pivot_col, how_merge, pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir):\n",
    "    pheno_mapped = annotation.merge(pheno, left_on = annotation_col[0], right_on = pheno_col[0], how = merge_how)\n",
    "    col_num = len(pheno_col[1:])\n",
    "    for col_i in range(1, col_num + 1):\n",
    "        if pheno_col[col_i] != annotation_col[col_i]:\n",
    "            pheno_mapped.drop(axis = 1, columns = [pheno_col[col_i], annotation_col[col_i]], inplace = True)\n",
    "        else:\n",
    "            tmp_pheno_col = \"%s_y\"%pheno_col[col_i]\n",
    "            tmp_annotation_col = \"%s_x\"%annotation_col[col_i]\n",
    "            pheno_mapped.drop(axis = 1, columns = [tmp_pheno_col, tmp_annotation_col], inplace = True)\n",
    "    if len(annotation_col) > 1:\n",
    "        depre_sample_list, pivot_col_df = map_deprecation_list(pheno, annotation, pheno_col, annotation_col,\n",
    "                                                               pivot_col, pheno_prefix, anno_prefix, diff_save_dir)\n",
    "        if depre_sample_list != []:\n",
    "            pheno_mapped_raw = pheno_mapped.copy()\n",
    "            del pheno_mapped\n",
    "            pheno_mapped = pheno_mapped_raw[~pheno_mapped_raw[pivot_col].isin(depre_sample_list)]\n",
    "        pheno_mapped = pheno_mapped.merge(pivot_col_df, on = pivot_col, how = \"inner\")\n",
    "    pheno_mapped_filename = \"%s_%s.tsv\"%(anno_prefix, pheno_prefix)\n",
    "    pheno_mapped_dir_filename = os.path.join(mapped_save_dir, pheno_mapped_filename)\n",
    "    pheno_mapped.to_csv(pheno_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return pheno_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation(df, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    df_pivot = df[[pivot_col]]\n",
    "    df_col = df[[pheno_col, annotation_col]]\n",
    "    df_col.iloc[:, 0] = df_col.apply(func, axis = 1)\n",
    "    df_col.iloc[:, 1] = df_col.apply(func, axis = 1)\n",
    "    del df\n",
    "    df = pd.concat([df_pivot, df_col], axis = 1)\n",
    "    depre_sample_index_list = df[df.iloc[:, 0].isnull()].index.tolist()\n",
    "    if depre_sample_index_list == []:\n",
    "        depre_sample_list = []\n",
    "    else:\n",
    "        depre_sample_list = df.loc[depre_sample_index_list, pivot_col].values.reshape(1, -1).tolist()[0]\n",
    "    df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "    tmp_compare = df[pheno_col].eq(df[annotation_col], axis = 0)\n",
    "    df_dif = df[tmp_compare == False]\n",
    "    df_cons = df[tmp_compare == True]\n",
    "    if annotation_col.split('_x')[-1] == \"\":\n",
    "        annotation_col_propagate = annotation_col.split('_x')[0]\n",
    "        df_cons.rename(columns = {annotation_col:annotation_col_propagate}, inplace = True)\n",
    "    else:\n",
    "        annotation_col_propagate = annotation_col\n",
    "    df_cons_propagate = df_cons.loc[:, [pivot_col, annotation_col_propagate]]\n",
    "    if df_dif.shape[0] != 0:\n",
    "        print(\"%s is inconsistent.\"%pheno_col)\n",
    "        df_dif_filename = \"%s_%s_%s.tsv\"%(anno_prefix, pheno_prefix, annotation_col)\n",
    "        df_dif_dir_filename = os.path.join(save_dir, df_dif_filename)\n",
    "        df_dif.to_csv(df_dif_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        depre_sample_list = df_dif[[pivot_col]].values.reshape(1, -1).tolist()[0]\n",
    "    return depre_sample_list, df_cons_propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Alpha Globin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. cohort and 2. # samples with calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_dir = os.path.join(\"..\", \"raw_data\", \"cn\")\n",
    "cn_filename = \"alpha_globin_calls.txt\"\n",
    "cn_dir_filename = os.path.join(cn_dir, cn_filename)\n",
    "cn = pd.read_csv(cn_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131823, 8)\n",
      "(131003, 8)\n",
      "(130032, 8)\n"
     ]
    }
   ],
   "source": [
    "print(cn.shape)\n",
    "cn_pass = cn.loc[cn.loc[:, \"QC_FAIL\"] == \"QC_PASS\", :]\n",
    "print(cn_pass.shape)\n",
    "cn_pass = cn_pass.loc[cn_pass.loc[:, \"QC_FLAGGED\"] == \"QC_PASS\", :]\n",
    "print(cn_pass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_num(cn_pass, \"SAMPLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_pass_useful = cn_pass[[\"SAMPLE\", \"CN\"]]\n",
    "cn_pass_useful.rename(columns = {\"SAMPLE\":\"NWDID\", \"CN\":\"cn\"}, inplace = True)\n",
    "cn_pass_dir = os.path.join(\"..\", \"prepro_data\", \"cn\")\n",
    "cn_pass_filename = \"alpha_globin_calls_pass_useful.tsv\"\n",
    "cn_pass_dir_filename = os.path.join(cn_pass_dir, cn_pass_filename)\n",
    "cn_pass_useful.to_csv(cn_pass_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Summary of Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-10-08_useful_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118934, 15)\n",
      "['GOLDN', 'INSPIRE_AF', 'CAMP', 'GALAI', 'VU_AF', 'HCHS_SOL', 'CARDIA', 'EGCUT', 'THRV', 'PUSH_SCD', 'DECAF', 'REDS-III_Brazil', 'MGH_AF', 'AustralianFamilialAF', 'IPF', 'OMG_SCD', 'Partners', 'CHIRAH', 'BioMe', 'SAS', 'BioVU_AF', 'GENOA', 'CARE_PACT', 'CARE_BADGER', 'PharmHU', 'PCGC_CHD', 'CFS', 'GenSalt', 'PIMA', 'Sarcoidosis', 'HyperGEN', 'BAGS', 'PMBB_AF', 'WHI', 'walk_PHaSST', 'EOCOPD', 'GGAF', 'LTRC', 'WGHS', 'SARP', 'MPP', 'CARE_CLIC', 'MLOF', 'FHS', 'ARIC', 'ChildrensHS_IGERA', 'HVH', 'GeneSTAR', 'COPDGene', 'CCAF', 'SAFS', 'GENAF', 'SAGE', 'GALAII', 'DHS', 'SAPPHIRE_asthma', 'AFLMU', 'CATHGEN', 'MESA', 'VAFAR', 'miRhythm', 'JHS', 'CRA', 'JHU_AF', 'Mayo_VTE', 'CHS', 'Amish', 'ChildrensHS_MetaAir', 'CARE_TREXA', 'ChildrensHS_GAP', 'ECLIPSE']\n"
     ]
    }
   ],
   "source": [
    "cn_annotation = cn_pass.merge(annotation, left_on = \"SAMPLE\", right_on = \"NWDID\", how = \"inner\")\n",
    "cn_pass_anno_summary, cn_pass_anno_list = categorize_df(cn_annotation, \"study\")\n",
    "print(cn_annotation.shape)\n",
    "print(cn_pass_anno_list)\n",
    "cn_pass_anno_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cn_pass_anno_summary_filename = \"cn-nathan_pass_anno_cohort_summary.tsv\"\n",
    "cn_pass_anno_summary_dir_filename = os.path.join(cn_pass_anno_summary_dir, cn_pass_anno_summary_filename)\n",
    "cn_pass_anno_summary.to_csv(cn_pass_anno_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CN of Each Cohort Based on Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_filename = \"freeze8_anno04_af02_btc03-coh03_egfr03-ckd_adad01.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "_, pheno_cohort_list = categorize_df(pheno, \"study\")\n",
    "\n",
    "full_cn_dir = os.path.join(\"..\", \"prepro_data\", \"cn\")\n",
    "full_cn_filename = \"alpha_globin_calls_pass_useful.tsv\"\n",
    "full_cn_dir_filename = os.path.join(full_cn_dir, full_cn_filename)\n",
    "full_cn = pd.read_csv(full_cn_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenSalt (1773, 2)\n",
      "MESA (4549, 2)\n",
      "HCHS_SOL (3847, 2)\n",
      "JHS (3099, 2)\n",
      "HyperGEN (1786, 2)\n",
      "CARDIA (3038, 2)\n",
      "FHS (3329, 2)\n",
      "ARIC (3919, 2)\n",
      "CHS (3480, 2)\n",
      "WHI (10918, 2)\n",
      "GeneSTAR (1535, 2)\n",
      "COPDGene (5713, 2)\n",
      "DHS (372, 2)\n",
      "BioMe (9171, 2)\n",
      "GENOA (1058, 2)\n"
     ]
    }
   ],
   "source": [
    "for cohort in pheno_cohort_list:\n",
    "    pheno_cohort_raw = pheno.loc[pheno.loc[:, \"study\"] == cohort, :]\n",
    "    pheno_id = pheno_cohort_raw[[\"NWDID\"]]\n",
    "    cn = full_cn.merge(pheno_id, on = \"NWDID\", how = \"inner\")\n",
    "    \n",
    "    full_cn_id = full_cn[[\"NWDID\"]]\n",
    "    pheno_cohort = full_cn_id.merge(pheno_cohort_raw, on = \"NWDID\", how = \"inner\")\n",
    "    print(cohort, cn.shape)\n",
    "    cn_filename = \"%s_cn.tsv\"%cohort\n",
    "    cn_dir = os.path.join(\"..\", \"cohort\", cohort, \"cn\")\n",
    "    cn_dir_filename = os.path.join(cn_dir, cn_filename)\n",
    "    if not os.path.isdir(cn_dir):\n",
    "        os.makedirs(cn_dir, exist_ok = True) \n",
    "    cn.to_csv(cn_dir_filename, \"\\t\", header = True, index = False)\n",
    "    \n",
    "    pheno_cohort_dir = os.path.join(\"..\", \"cohort\", cohort, \"pre_data\")\n",
    "    pheno_cohort_filename = \"freeze8_anno04_af02_btc03-coh03_egfr03-ckd_adad01_%s.tsv\"%cohort\n",
    "    pheno_cohort_dir_filename = os.path.join(pheno_cohort_dir, pheno_cohort_filename)\n",
    "    if not os.path.isdir(pheno_cohort_dir):\n",
    "        os.makedirs(pheno_cohort_dir, exist_ok = True)\n",
    "    pheno_cohort.to_csv(pheno_cohort_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. # samples with phenotype data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ddimer_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer_raw = pd.read_csv(ddimer_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "ddimer = ddimer_raw.loc[:, [\"sample.id\", \"STUDY\", \"DDIMER\", \"AGE_DDIMER\", \"sample_remove_DDIMER\"]]\n",
    "ddimer.dropna(axis = \"index\", how = \"any\", inplace = True)\n",
    "ddimer_category_summary, _ = categorize_df(ddimer, \"STUDY\")\n",
    "ddimer_category_summary.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "ddimer_category_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_category_summary_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019_study_summary.tsv\"\n",
    "ddimer_category_summary_dir_filename = os.path.join(ddimer_category_summary_dir, ddimer_category_summary_filename)\n",
    "ddimer_category_summary.to_csv(ddimer_category_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddimer_qc = ddimer[ddimer[\"sample_remove_DDIMER\"] != 1]\n",
    "ddimer_qc_category_summary, _ = categorize_df(ddimer_qc, \"STUDY\")\n",
    "ddimer_qc_category_summary.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "ddimer_qc_category_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_qc_category_summary_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019_qc_study_summary.tsv\"\n",
    "ddimer_qc_category_summary_dir_filename = os.path.join(ddimer_qc_category_summary_dir, ddimer_qc_category_summary_filename)\n",
    "ddimer_qc_category_summary.to_csv(ddimer_qc_category_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "filename_list = os.listdir(egfr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = []\n",
    "sample_size_list = []\n",
    "for filename in filename_list[1:]:\n",
    "    cohort_name_raw = filename.split(\"_\")[3]\n",
    "    if cohort_name_raw == \"PHEN\":\n",
    "        cohort_name = \"SOL\"\n",
    "    else:\n",
    "        cohort_name = cohort_name_raw\n",
    "    dir_filename = os.path.join(egfr_dir, filename)\n",
    "    tmp_df = pd.read_csv(dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    tmp_sample_size = tmp_df.shape[0]\n",
    "    cohort_list.append(cohort_name)\n",
    "    sample_size_list.append(tmp_sample_size)\n",
    "cohort_sample_size_tuple = list(zip(cohort_list, sample_size_list))\n",
    "egfr_sample_size_df = pd.DataFrame(data = cohort_sample_size_tuple, columns = [\"cohort\", \"sample_size\"])\n",
    "egfr_sample_size_df.sort_values(by = \"cohort\", axis = 0, inplace = True)\n",
    "egfr_sample_size_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_sample_size_filename = \"egfr_cohort_summary.tsv\"\n",
    "egfr_sample_size_dir_filename = os.path.join(egfr_sample_size_dir, egfr_sample_size_filename)\n",
    "egfr_sample_size_df.to_csv(egfr_sample_size_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grengrp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (0,1,2,28,29,30,35,39,42,45,46,47,49,54,56,61,63,69,70,71,72,73,76,83,85,89,90,91,98,114,117,125,132,134,135,142,155,156,160,161,163,164,176,186,187,192,196,205,206,207,208,209,210,216,218,220,224,225,228,234,242,244,247,250,260,269,270,272,282,283,285,286,288,292,294,295,296,298,303,305,335,336,383,385,386,389,390,391,392,393) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 5)\n",
      "(11678, 5)\n"
     ]
    }
   ],
   "source": [
    "gengrp6_filename = \"page-harmonized-phenotypes-pca-freeze2-candidate2-2016-12-14.GWASid_fid_22May2018internalPCs.SOLv2consent.txt\"\n",
    "gengrp6_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\", \"gengrp6\")\n",
    "gengrp6_dir_filename = os.path.join(gengrp6_dir, gengrp6_filename)\n",
    "gengrp6 = pd.read_csv(gengrp6_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6.replace(\".\", np.nan, inplace=True)\n",
    "gengrp6_select = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\",\n",
    "                          \"gengrp6\"]].dropna(axis=0, subset=[\"analysis_id\",\"gengrp6\"],how=\"any\")\n",
    "gengrp6_select.rename(columns = {\"analysis_id\":\"SUBJECT_ID\"}, inplace=True)\n",
    "print(gengrp6_select.shape)\n",
    "gengrp6_select = gengrp6_select[gengrp6_select[\"CONSENT_text\"] != \"DROP\"]\n",
    "print(gengrp6_select.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight and center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16415, 3)\n",
      "(9974, 7)\n"
     ]
    }
   ],
   "source": [
    "weight_center_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\")\n",
    "weight_center_filename = \"bloodcell_output.csv\"\n",
    "weight_center_dir_filename = os.path.join(weight_center_dir, weight_center_filename)\n",
    "weight_center = pd.read_csv(weight_center_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "weight_center_select = weight_center[[\"ID\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\"]].dropna(axis = 0, how = \"any\")\n",
    "weight_center_select.rename(columns = {\"ID\":\"z_sol_id\"}, inplace = True)\n",
    "print(weight_center_select.shape)\n",
    "gengrp6_weight_center = gengrp6_select.merge(weight_center_select, on = \"z_sol_id\", how = \"inner\")\n",
    "print(gengrp6_weight_center.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blood cell traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh02_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cat_col = \"cohort\"\n",
    "pheno_cat_summary, cohort_list = categorize_df(pheno, cat_col)\n",
    "cat_list = cohort_list\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\"]\n",
    "fixed_col_list = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"]\n",
    "cohort_pheno_df = cat_col_summary(pheno, cat_col, cat_list, col_list, fixed_col_list)\n",
    "cohort_pheno_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cohort_pheno_filename = \"cohort_pheno_summary.tsv\"\n",
    "cohort_pheno_dir_filename = os.path.join(cohort_pheno_dir, cohort_pheno_filename)\n",
    "cohort_pheno_df.to_csv(cohort_pheno_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Appending JHS ln(wbc) and to phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_pheno_filename = \"jhs_basic_phenotypes_05072019.txt\"\n",
    "jhs_pheno_dir_filename = os.path.join(pheno_dir, jhs_pheno_filename)\n",
    "jhs_pheno = pd.read_csv(jhs_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhs_pheno[\"neutrophil_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lnneu\"])\n",
    "jhs_pheno[\"age_at_neutrophil_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "jhs_pheno[\"lymphocyte_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lnlym\"])\n",
    "jhs_pheno[\"age_at_lymphocyte_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "jhs_pheno[\"basophil_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lnbaso\"])\n",
    "jhs_pheno[\"age_at_basophil_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "jhs_pheno[\"eosinophil_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lneos\"])\n",
    "jhs_pheno[\"age_at_eosinophil_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "jhs_pheno[\"monocyte_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lnmono\"])\n",
    "jhs_pheno[\"age_at_monocyte_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "jhs_pheno[\"wbc_ncnc_bld_1\"] = np.exp(jhs_pheno[\"lnwbc\"])\n",
    "jhs_pheno[\"age_at_wbc_ncnc_bld_1\"] = jhs_pheno[\"age\"]\n",
    "# jhs_pheno[\"DDIMER\"] = np.exp(jhs_pheno[\"lnddimer\"])\n",
    "# jhs_pheno[\"age_at_DDIMER\"] = jhs_pheno[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_pheno_list = [\"NWDID\", \"neutrophil_ncnc_bld_1\", \"age_at_neutrophil_ncnc_bld_1\",\"lymphocyte_ncnc_bld_1\",\n",
    "                     \"age_at_lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"age_at_basophil_ncnc_bld_1\",\"eosinophil_ncnc_bld_1\",\n",
    "                     \"age_at_eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"age_at_monocyte_ncnc_bld_1\",\"wbc_ncnc_bld_1\", \"age_at_wbc_ncnc_bld_1\"]\n",
    "jhs_pheno_useful = jhs_pheno[useful_pheno_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhs_useful_filename = \"jhs_usefule_phenotype_05072019.tsv\"\n",
    "jhs_useful_dir_filename = os.path.join(pheno_dir, jhs_useful_filename)\n",
    "jhs_pheno_useful.to_csv(jhs_useful_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. # sample in annotation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cohort Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-07-30.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)\n",
    "annotation_cat_summary, _ = categorize_df(annotation, \"study\")\n",
    "annotation_cat_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "annotation_cat_summary_filename = \"annotation_cohort_summary_2019-07-30.tsv\"\n",
    "annotation_cat_summary_dir_filename = os.path.join(annotation_cat_summary_dir, annotation_cat_summary_filename)\n",
    "annotation_cat_summary.to_csv(annotation_cat_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Consent Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-07-30.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "sample_size = annotation.shape[0]\n",
    "annotation.dropna(axis = 0, subset = [\"consent\"], how = \"any\", inplace = True)\n",
    "sample_size_nona = annotation.shape[0]\n",
    "na_df = pd.DataFrame(data = [[\"NAN\", sample_size - sample_size_nona]], columns = [\"consent\", \"sample_size\"])\n",
    "annotation_cat_summary, _ = categorize_df(annotation, \"consent\")\n",
    "annotation_cat_summary = annotation_cat_summary.append(na_df, ignore_index=True)\n",
    "annotation_cat_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "annotation_cat_summary_filename = \"annotation_consent_summary_2019-07-30.tsv\"\n",
    "annotation_cat_summary_dir_filename = os.path.join(annotation_cat_summary_dir, annotation_cat_summary_filename)\n",
    "annotation_cat_summary.to_csv(annotation_cat_summary_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Consent Summary by Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-10-08.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)\n",
    "_, cohort_list = categorize_df(annotation, \"study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_summary_cohort = pd.DataFrame(columns=['cohort', 'consent', 'sample_size'])\n",
    "for cohort in cohort_list:\n",
    "    tmp_annotation = annotation.loc[annotation.loc[:, \"study\"] == cohort, :]\n",
    "    tmp_summary, _ = categorize_df(tmp_annotation, \"consent\")\n",
    "    row_num = tmp_summary.shape[0]\n",
    "    tmp_cohort_series = pd.Series(data = [cohort] * row_num)\n",
    "    tmp_summary.insert(loc = 0, column = \"cohort\", value = tmp_cohort_series)\n",
    "    consent_summary_cohort = pd.concat([consent_summary_cohort, tmp_summary], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "consent_summary_cohort_dir = os.path.join(\"..\", \"data_summary\")\n",
    "consent_summary_cohort_filename = \"annotation_consent_summary_cohort_2019-10-08.tsv\"\n",
    "consent_summary_cohort_dir_filename = os.path.join(consent_summary_cohort_dir, consent_summary_cohort_filename)\n",
    "consent_summary_cohort.to_csv(consent_summary_cohort_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Clean Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_sample_annot_2020-03-03.txt\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation_raw = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation_useful_header_list = [\"sample.id\", \"unique_subject_key\", \"subject_id\", \"consent\", \"study\", \"sex\", \"exclude\"]\n",
    "annotation = annotation_raw[annotation_useful_header_list]\n",
    "annotation.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "annotation.rename(columns = {\"sample.id\":\"NWDID\"}, inplace = True)\n",
    "annotation.loc[annotation.loc[:, \"sex\"] == \"M\", \"sex\"] = 1\n",
    "annotation.loc[annotation.loc[:, \"sex\"] == \"F\", \"sex\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = annotation.loc[annotation.loc[:, \"exclude\"] == False, :]\n",
    "annotation.reset_index(inplace = True, drop = True)\n",
    "sample_num = annotation.shape[0]\n",
    "consent_remove_idx = []\n",
    "cohort_list = [\"ARIC\", \"COPDGene\", \"CHS\"]\n",
    "for sample_i in range(sample_num):\n",
    "    tmp_consent = annotation.loc[sample_i, \"consent\"]\n",
    "    tmp_cohort = annotation.loc[sample_i, \"study\"]\n",
    "    tmp_consent_list = tmp_consent.split(\"-\")\n",
    "    if (tmp_cohort in cohort_list) and (tmp_consent_list[0] == \"DS\"):\n",
    "        consent_remove_idx.append(sample_i)\n",
    "annotation_remove_consent = annotation.drop(axis = 0, labels = consent_remove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_remove_consent_filename = \"freeze8_anno05_af02.tsv\"\n",
    "annotation_remove_consent_dir_filename = os.path.join(annotation_dir, annotation_remove_consent_filename)\n",
    "annotation_remove_consent.to_csv(annotation_remove_consent_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Remove Duplicates from ToPMed Duplicates List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "dup_list_filename = \"freeze8_duplicates_2019-04-19.txt\"\n",
    "dup_list_dir_filename = os.path.join(annotation_dir, dup_list_filename)\n",
    "dup_list = pd.read_csv(dup_list_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "dup_list.sort_values(by = [\"study1\"], inplace = True)\n",
    "\n",
    "annotation_filename = \"freeze8_anno05_af02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_list_control = dup_list.loc[dup_list.loc[:, \"study1\"] == \"Control\", :]\n",
    "dup_list_control_list = dup_list_control[\"ID2\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_list_norm = dup_list.loc[dup_list.loc[:, \"study1\"] != \"Control\", :]\n",
    "norm_num = dup_list_norm.shape[0]\n",
    "np.random.seed(43329)\n",
    "norm_idx_list = np.random.randint(2, size=norm_num)\n",
    "dup_list_norm_list = []\n",
    "for sample_i in range(norm_num):\n",
    "    norm_idx = norm_idx_list[sample_i]\n",
    "    tmp_sample_id = dup_list_norm.iloc[sample_i, norm_idx]\n",
    "    dup_list_norm_list.append(tmp_sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138092, 7)\n",
      "(136047, 7)\n"
     ]
    }
   ],
   "source": [
    "dup_id_list = dup_list_control_list + dup_list_norm_list\n",
    "print(annotation.shape)\n",
    "annotation_unique = annotation[~annotation['NWDID'].isin(dup_id_list)]\n",
    "print(annotation_unique.shape)\n",
    "annotation_unique_filename = \"freeze8_anno05_af02_unique01.tsv\"\n",
    "annotation_unique_dir_filename = os.path.join(annotation_dir, annotation_unique_filename)\n",
    "annotation_unique.to_csv(annotation_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Duplicates of unique_subject_key in freeze8 annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWDID</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>consent</th>\n",
       "      <th>study</th>\n",
       "      <th>sex</th>\n",
       "      <th>exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NWDID, unique_subject_key, subject_id, consent, study, sex, exclude]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique01.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dup_df = df_extraction_duplicates(annotation, \"unique_subject_key\")\n",
    "print(annotation_dup_df.shape)\n",
    "annotation_dup_df = annotation_dup_df.sort_values(by=['unique_subject_key'])\n",
    "display(annotation_dup_df)\n",
    "\n",
    "annotation_dup_filename = \"freeze8_anno05_af02_unique01_dup.tsv\"\n",
    "annotation_dup_dir_filename = os.path.join(annotation_dir, annotation_dup_filename)\n",
    "annotation_dup_df.to_csv(annotation_dup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "invar_subsets = [\"consent\", \"sex\"]\n",
    "rm_header = \"NWDID\"\n",
    "annotation_unique = remove_dup_anno(annotation, invar_subsets, rm_header)\n",
    "annotation_unique_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_unique_dir_filename = os.path.join(annotation_dir, annotation_unique_filename)\n",
    "annotation_unique.to_csv(annotation_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Duplicates of NWDID in freeze8 annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(duplicates_num(annotation, \"NWDID\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Process the overlap between 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133280, 18)\n",
      "(133280, 18)\n",
      "(138934, 18)\n",
      "(133280, 18)\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation_drop_subject_key = annotation.dropna(axis = 0, subset = [\"unique_subject_key\"], how = \"any\")\n",
    "print(annotation_drop_subject_key.shape)\n",
    "annotation_drop_subject_id = annotation.dropna(axis = 0, subset = [\"subject_id\"], how = \"any\")\n",
    "print(annotation_drop_subject_id.shape)\n",
    "annotation_drop_study = annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\")\n",
    "print(annotation_drop_study.shape)\n",
    "annotation.dropna(axis = 0, subset = [\"unique_subject_key\", \"subject_id\", \"NWDID\"], how = \"any\", inplace = True)\n",
    "print(annotation.shape)\n",
    "annotation_select = annotation[[\"NWDID\", \"subject_id\", \"unique_subject_key\", \"consent\", \"study\", \"sex\", \"exclude\"]]\n",
    "annotation_select.rename(columns = {\"subject_id\":\"SUBJECT_ID\", \"study\":\"cohort\"}, inplace = True)\n",
    "annotation_select_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_select_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_select_dir_filename = os.path.join(annotation_select_dir, annotation_select_filename)\n",
    "#annotation_select.to_csv(annotation_select_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overlap b/w 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66696, 47)\n",
      "(66696, 47)\n",
      "0\n",
      "There is no duplicates in NWDID in the merged table of phenotype and annotation.\n"
     ]
    }
   ],
   "source": [
    "pheno_filename = \"coh03_pheno02_pre.tsv\"\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.drop(axis = 1, labels = [\"SUBJECT_ID\", \"cohort\"], inplace = True)\n",
    "\n",
    "pheno_annotation = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation.shape)\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], inplace = True)\n",
    "print(pheno_annotation.shape)\n",
    "print(duplicates_num(pheno_annotation, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in the merged table of phenotype and annotation.\")\n",
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_dir, pheno_annotation_filename)\n",
    "pheno_annotation.to_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46105, 48)\n",
      "(46105, 47)\n",
      "(46105, 48)\n",
      "(55366, 49)\n",
      "(66696, 49)\n",
      "(66696, 47)\n",
      "(66696, 47)\n",
      "0\n",
      "There is no duplicates in NWDID in the merged table of phenotype and annotation.\n"
     ]
    }
   ],
   "source": [
    "pheno_filename = \"coh03_pheno02_pre.tsv\"\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30_useful.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_subjectid_uni = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_uni.shape)\n",
    "pheno_annotation_subjectid_cohort_uni = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_cohort_uni.shape)\n",
    "pheno_annotation_subjectid_cohort = pheno.merge(annotation, on = [\"SUBJECT_ID\", \"cohort\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid_cohort.shape)\n",
    "pheno_annotation_subjectid = pheno.merge(annotation, on = [\"SUBJECT_ID\"], how = \"inner\")\n",
    "print(pheno_annotation_subjectid.shape)\n",
    "pheno_annotation_uni = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation_uni.shape)\n",
    "\n",
    "annotation.drop(axis = 1, labels = [\"SUBJECT_ID\", \"cohort\"], inplace = True)\n",
    "\n",
    "pheno_annotation = pheno.merge(annotation, on = [\"unique_subject_key\"], how = \"inner\")\n",
    "print(pheno_annotation.shape)\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], inplace = True)\n",
    "print(pheno_annotation.shape)\n",
    "print(duplicates_num(pheno_annotation, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in the merged table of phenotype and annotation.\")\n",
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_dir, pheno_annotation_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>cohort</th>\n",
       "      <th>hemoglobin_mcnc_bld_1</th>\n",
       "      <th>age_at_hemoglobin_mcnc_bld_1</th>\n",
       "      <th>hematocrit_vfr_bld_1</th>\n",
       "      <th>age_at_hematocrit_vfr_bld_1</th>\n",
       "      <th>rbc_ncnc_bld_1</th>\n",
       "      <th>age_at_rbc_ncnc_bld_1</th>\n",
       "      <th>mcv_entvol_rbc_1</th>\n",
       "      <th>...</th>\n",
       "      <th>gengrp6</th>\n",
       "      <th>WEIGHT_FINAL_NORM_OVERALL</th>\n",
       "      <th>CENTER</th>\n",
       "      <th>NWDID</th>\n",
       "      <th>consent</th>\n",
       "      <th>sex</th>\n",
       "      <th>exclude</th>\n",
       "      <th>age_at_DDIMER</th>\n",
       "      <th>DDIMER</th>\n",
       "      <th>sample_remove_DDIMER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53932</th>\n",
       "      <td>131150792</td>\n",
       "      <td>ARIC_131150792</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355569</td>\n",
       "      <td>HMB-IRB</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68851</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52892</th>\n",
       "      <td>131186968</td>\n",
       "      <td>ARIC_131186968</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355606</td>\n",
       "      <td>HMB-IRB</td>\n",
       "      <td>M</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69022</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD355606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>9229</td>\n",
       "      <td>FHS_9229</td>\n",
       "      <td>FHS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD611521</td>\n",
       "      <td>HMB-IRB-NPU-MDS</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68671</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FHS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD611521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0</td>\n",
       "      <td>23500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31155</th>\n",
       "      <td>823218</td>\n",
       "      <td>WHI_823218</td>\n",
       "      <td>WHI</td>\n",
       "      <td>13.2</td>\n",
       "      <td>78.25</td>\n",
       "      <td>39.9</td>\n",
       "      <td>78.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD632820</td>\n",
       "      <td>HMB-IRB-NPU</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69026</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NWD632820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>25220.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SUBJECT_ID unique_subject_key cohort  hemoglobin_mcnc_bld_1  \\\n",
       "53932  131150792     ARIC_131150792   ARIC                    NaN   \n",
       "68851        NaN                NaN   ARIC                    NaN   \n",
       "52892  131186968     ARIC_131186968   ARIC                    NaN   \n",
       "69022        NaN                NaN   ARIC                    NaN   \n",
       "59496       9229           FHS_9229    FHS                    NaN   \n",
       "68671        NaN                NaN    FHS                    NaN   \n",
       "31155     823218         WHI_823218    WHI                   13.2   \n",
       "69026        NaN                NaN    WHI                    NaN   \n",
       "\n",
       "       age_at_hemoglobin_mcnc_bld_1  hematocrit_vfr_bld_1  \\\n",
       "53932                           NaN                   NaN   \n",
       "68851                           NaN                   NaN   \n",
       "52892                           NaN                   NaN   \n",
       "69022                           NaN                   NaN   \n",
       "59496                           NaN                   NaN   \n",
       "68671                           NaN                   NaN   \n",
       "31155                         78.25                  39.9   \n",
       "69026                           NaN                   NaN   \n",
       "\n",
       "       age_at_hematocrit_vfr_bld_1  rbc_ncnc_bld_1  age_at_rbc_ncnc_bld_1  \\\n",
       "53932                          NaN             NaN                    NaN   \n",
       "68851                          NaN             NaN                    NaN   \n",
       "52892                          NaN             NaN                    NaN   \n",
       "69022                          NaN             NaN                    NaN   \n",
       "59496                          NaN             NaN                    NaN   \n",
       "68671                          NaN             NaN                    NaN   \n",
       "31155                        78.25             NaN                    NaN   \n",
       "69026                          NaN             NaN                    NaN   \n",
       "\n",
       "       mcv_entvol_rbc_1  ...  gengrp6  WEIGHT_FINAL_NORM_OVERALL  CENTER  \\\n",
       "53932               NaN  ...      NaN                        NaN     NaN   \n",
       "68851               NaN  ...      NaN                        NaN     NaN   \n",
       "52892               NaN  ...      NaN                        NaN     NaN   \n",
       "69022               NaN  ...      NaN                        NaN     NaN   \n",
       "59496               NaN  ...      NaN                        NaN     NaN   \n",
       "68671               NaN  ...      NaN                        NaN     NaN   \n",
       "31155               NaN  ...      NaN                        NaN     NaN   \n",
       "69026               NaN  ...      NaN                        NaN     NaN   \n",
       "\n",
       "           NWDID          consent  sex  exclude  age_at_DDIMER   DDIMER  \\\n",
       "53932  NWD355569          HMB-IRB    M     True            NaN      NaN   \n",
       "68851  NWD355569              NaN    F      NaN           61.0  48000.0   \n",
       "52892  NWD355606          HMB-IRB    M     True            NaN      NaN   \n",
       "69022  NWD355606              NaN    F      NaN           57.0  25000.0   \n",
       "59496  NWD611521  HMB-IRB-NPU-MDS    F    False            NaN      NaN   \n",
       "68671  NWD611521              NaN    M      NaN           61.0  23500.0   \n",
       "31155  NWD632820      HMB-IRB-NPU    M    False            NaN      NaN   \n",
       "69026  NWD632820              NaN    F      NaN           78.0  25220.0   \n",
       "\n",
       "       sample_remove_DDIMER  \n",
       "53932                   NaN  \n",
       "68851                   0.0  \n",
       "52892                   NaN  \n",
       "69022                   0.0  \n",
       "59496                   NaN  \n",
       "68671                   0.0  \n",
       "31155                   NaN  \n",
       "69026                   0.0  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(load_dir, ddimer_filename)\n",
    "ddimer = pd.read_csv(ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_filename = \"coh03_pheno02_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(load_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_annotation_ddimer_comparesex = pheno_annotation.merge(ddimer, on = [\"NWDID\", \"cohort\", \"sex\"], how = \"outer\")\n",
    "print(duplicates_num(pheno_annotation_ddimer_comparesex, \"NWDID\"))\n",
    "pheno_annotation_ddimer_nocomparesex = pheno_annotation.merge(ddimer, on = [\"NWDID\", \"cohort\"], how = \"outer\")\n",
    "print(duplicates_num(pheno_annotation_ddimer_nocomparesex, \"NWDID\"))\n",
    "\n",
    "boolean_series = pheno_annotation_ddimer_comparesex[[\"NWDID\"]].duplicated(keep=False)\n",
    "pheno_annotation_ddimer_comparesex_dup = pheno_annotation_ddimer_comparesex.loc[boolean_series, :]\n",
    "pheno_annotation_ddimer_comparesex_dup.sort_values(axis = 0, by = \"NWDID\", inplace = True)\n",
    "\n",
    "ddimer_anno_sex_diff_dir = os.path.join(\"..\", \"data_summary\")\n",
    "ddimer_anno_sex_diff_filename = \"ddimer_anno_sex_diff.tsv\"\n",
    "ddimer_anno_sex_diff_dir_filename = os.path.join(ddimer_anno_sex_diff_dir, ddimer_anno_sex_diff_filename)\n",
    "pheno_annotation_ddimer_comparesex_dup.to_csv(ddimer_anno_sex_diff_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "display(pheno_annotation_ddimer_comparesex_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually remove the samples with different sex but the same NWDID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69394, 50)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pheno_annotation_ddimer = pheno_annotation_ddimer_comparesex.drop(labels = [53932, 52892, 59496, 31155], axis = 0)\n",
    "print(pheno_annotation_ddimer.shape)\n",
    "print(duplicates_num(pheno_annotation_ddimer, \"NWDID\"))\n",
    "pheno_annotation_ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_ddimer_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_ddimer_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_ddimer_filename)\n",
    "pheno_annotation_ddimer.to_csv(pheno_annotation_ddimer_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of phenotypes that can maps to annotation file based on cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (38,40,42,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"gengrp6\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\", \"DDIMER\", \"EGFRCKDEPI\", \"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\",\n",
    "            \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\", \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\",\n",
    "            \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\",\n",
    "            \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "pheno_annotation.dropna(axis = 0, subset = [\"cohort\"], how = \"any\", inplace = True)\n",
    "cat_col = \"cohort\"\n",
    "pheno_annotation_summary, category_list = categorize_df(pheno_annotation, cat_col)\n",
    "pheno_annotation_category_df = pd.DataFrame(columns = col_list, index = category_list)\n",
    "for category in category_list:\n",
    "    for col in col_list:\n",
    "        tmp_df = pheno_annotation.loc[pheno_annotation[\"cohort\"] == category, col]\n",
    "        tmp_df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "        pheno_annotation_category_df.loc[category, col] = tmp_df.shape[0]\n",
    "pheno_annotation_category_df.sort_index(axis = 0, inplace = True)\n",
    "pheno_annotation_category_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pheno_annotation_category_filename = \"overlap_pheno_annotation_noexclude_summary.tsv\"\n",
    "pheno_annotation_category_dir_filename = os.path.join(pheno_annotation_category_dir, pheno_annotation_category_filename)\n",
    "pheno_annotation_category_df.to_csv(pheno_annotation_category_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overlap b/w 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess CN sample list to get list of cram of each cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = [\"freeze6-AA\", \"JHS\", \"OMG-SCD\", \"PAGE\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SCD\", \"SOL\", \"walk-PHaSST\"]\n",
    "cohort_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "for cohort in cohort_list:\n",
    "    tmp_cohort_dir_filename = os.path.join(cohort_dir, \"%s_chr16_full_sample.list\"%cohort)\n",
    "    tmp_cohort = pd.read_csv(tmp_cohort_dir_filename, sep = \"/\", header = None, index_col = None)\n",
    "    tmp_sample_list = tmp_cohort.iloc[:, -1].values.tolist()\n",
    "    tmp_sample_id_list = []\n",
    "    for tmp_sample in tmp_sample_list:\n",
    "        tmp_sample_id = tmp_sample.split(\".\")[0]\n",
    "        tmp_sample_id_list.append(tmp_sample_id)\n",
    "    tmp_sample_id_df = pd.DataFrame(data = tmp_sample_id_list)\n",
    "    tmp_sample_id_dir = cohort_dir\n",
    "    tmp_sample_id_filename = \"%s_cram.tsv\"%cohort\n",
    "    tmp_sample_id_dir_filename = os.path.join(tmp_sample_id_dir, tmp_sample_id_filename)\n",
    "    tmp_sample_id_df.to_csv(tmp_sample_id_dir_filename, sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap b/w samples we have CN and pheno_anno got in section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_annotation_filename = \"coh03_pheno03_freeze8_anno_pre.tsv\"\n",
    "pheno_annotation_dir_filename = os.path.join(pheno_annotation_ddimer_dir, pheno_annotation_filename)\n",
    "pheno_annotation = pd.read_csv(pheno_annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "col_list = [\"gengrp6\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\", \"DDIMER\", \"EGFRCKDEPI\", \"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\",\n",
    "            \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\", \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\",\n",
    "            \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\", \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\",\n",
    "            \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "\n",
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"HCHS_SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "cn_pheno_anno_summary = pd.DataFrame(columns = col_list, index = cohort_list)\n",
    "for cohort in cohort_list:\n",
    "    if cohort == \"HCHS_SOL\":\n",
    "        cohort_in_filename = \"SOL\"\n",
    "    else:\n",
    "        cohort_in_filename = cohort\n",
    "    cohort_filename = \"%s_cram.tsv\"%cohort_in_filename\n",
    "    cohort_dir_filename = os.path.join(load_dir, cohort_filename)\n",
    "    cohort_df = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = None, index_col = None)\n",
    "    cohort_df.rename(columns = {0:\"NWDID\"}, inplace = True)\n",
    "    cohort_pheno = cohort_df.merge(pheno_annotation, on = \"NWDID\", how = \"inner\")\n",
    "    for col in col_list:\n",
    "        tmp_df = cohort_pheno.loc[cohort_pheno[\"cohort\"] == cohort, col]\n",
    "        tmp_df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "        cn_pheno_anno_summary.loc[cohort, col] = tmp_df.shape[0]\n",
    "cn_pheno_anno_summary.sort_index(axis = 0, inplace = True)\n",
    "cn_pheno_anno_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cn_pheno_anno_summary_filename = \"overlap_cn_pheno_anno.tsv\"\n",
    "cn_pheno_anno_summary_dir_filename = os.path.join(cn_pheno_anno_summary_dir, cn_pheno_anno_summary_filename)\n",
    "cn_pheno_anno_summary.to_csv(cn_pheno_anno_summary_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap between CN samples and annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "annotation_filename = \"freeze8_sample_annot_2019-05-30.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "annotation.dropna(axis = 0, subset = [\"study\"], how = \"any\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SCD\", \"SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "cohort_dir_list = [os.path.join(load_dir, \"%s_cram.tsv\"%cohort) for cohort in cohort_list]\n",
    "commom_col = \"col\"\n",
    "map_raw_df = annotation\n",
    "save_dir = load_dir\n",
    "cohort_cn_map_summary = cn_map_list(cohort_list, cohort_dir_list, map_raw_df, common_col, save_dir)\n",
    "cohort_cn_map_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "cohort_cn_map_summary_filename = \"cohort_cn_annotation_overlap_summary.tsv\"\n",
    "cohort_cn_map_summary_dir_filename = os.path.join(cohort_cn_map_summary_dir, cohort_cn_map_summary_filename)\n",
    "cohort_cn_map_summary.to_csv(cohort_cn_map_summary_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap between (Overlap between CN samples and annotation files) and blood cell traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = [\"JHS\", \"OMG-SCD\", \"PharmHU\", \"REDS-III-Brazil\", \"SAS\", \"SOL\", \"walk-PHaSST\", \"WHI\",\n",
    "               \"MESA\", \"GeneSTAR\", \"COPDGene\", \"CHS\", \"CARDIA\", \"BioMe\", \"ARIC\"]\n",
    "load_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\"]\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh03_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "for cohort in cohort_list[:1]:\n",
    "    cohort_filename = \"%s_subject_id_cram.tsv\"%cohort\n",
    "    cohort_dir_filename = os.path.join(load_dir, cohort_filename)\n",
    "    cohort_df = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    cohort_pheno = cohort_df.merge(pheno, on = \"unique_subject_key\", how = \"inner\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare JHS phenotype and Annotated JHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3406, 36)\n",
      "(3406, 3)\n",
      "(3406, 38)\n",
      "So all the annotated JHS samples having phenotypes listed in the JHS phenotype file I used at the beginning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "jhs_pheno_filename = \"jhs_basic_phenotypes_05072019.txt\"\n",
    "jhs_pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_pheno_dir_filename = os.path.join(jhs_pheno_dir, jhs_pheno_filename)\n",
    "jhs_pheno = pd.read_csv(jhs_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(jhs_pheno.shape)\n",
    "\n",
    "jhs_anno_filename = \"JHS_subject_id_cram.tsv\"\n",
    "jhs_anno_dir = os.path.join(\"..\", \"..\", \"cnv\", \"sample_list\")\n",
    "jhs_anno_dir_filename = os.path.join(jhs_anno_dir, jhs_anno_filename)\n",
    "jhs_anno = pd.read_csv(jhs_anno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "print(jhs_anno.shape)\n",
    "\n",
    "jhs_pheno_anno_raw = jhs_pheno.merge(jhs_anno, on = \"NWDID\", how = \"inner\")\n",
    "print(jhs_pheno_anno_raw.shape)\n",
    "print(\"So all the annotated JHS samples having phenotypes listed in the JHS phenotype file I used at the beginning.\")\n",
    "\n",
    "jhs_pheno_anno_raw[\"wbc_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"neutrophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"lymphocyte_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"basophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"eosinophil_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"monocyte_ncnc_bld_1\"] = jhs_pheno_anno_raw.shape[0] * [None]\n",
    "jhs_pheno_anno_raw[\"cohort\"] = jhs_pheno_anno_raw.shape[0] * [\"JHS\"]\n",
    "col_list = [\"hemoglobin_mcnc_bld_1\", \"hematocrit_vfr_bld_1\", \"rbc_ncnc_bld_1\", \"mcv_entvol_rbc_1\", \"mch_entmass_rbc_1\", \"mchc_mcnc_rbc_1\",\n",
    "            \"rdw_ratio_rbc_1\", \"neutrophil_ncnc_bld_1\", \"lymphocyte_ncnc_bld_1\", \"basophil_ncnc_bld_1\", \"eosinophil_ncnc_bld_1\",\n",
    "            \"monocyte_ncnc_bld_1\", \"wbc_ncnc_bld_1\", \"pmv_entvol_bld_1\", \"platelet_ncnc_bld_1\", \"lnHBA1C\"]\n",
    "jhs_pheno_anno = jhs_pheno_anno_raw[[\"subject_id\", \"unique_subject_key\", \"cohort\", \"age\"] + col_list]\n",
    "jhs_pheno_anno.rename(columns = {\"subject_id\":\"SUBJECT_ID\"}, inplace = True)\n",
    "for col in col_list:\n",
    "    jhs_pheno_anno[\"age_at_%s\"%col] = jhs_pheno_anno[\"age\"]\n",
    "jhs_pheno_anno.drop(axis = 1, labels = \"age\", inplace = True)\n",
    "\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh02_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "pheno[\"lnHBA1C\"] = pheno.shape[0] * [None]\n",
    "pheno[\"age_at_lnHBA1C\"] = pheno.shape[0] * [None]\n",
    "pheno_header = list(pheno)\n",
    "jhs_pheno_anno_copy = jhs_pheno_anno[pheno_header].copy()\n",
    "\n",
    "jhs_pheno_anno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_pheno_anno_filename = \"jhs_basic_phenotypes_05072019_blood_cell_traits.tsv\"\n",
    "jhs_pheno_anno_dir_filename = os.path.join(jhs_pheno_anno_dir, jhs_pheno_anno_filename)\n",
    "jhs_pheno_anno_copy.to_csv(jhs_pheno_anno_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Concatenating Annotation (Proper Phenotype), Blood Cell Traits (Proper Adjustments), and eGFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Concatenating Blood Cell Traits and eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_filename = \"egfr03_unique.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr[\"SUBJECT_ID\"] = egfr[\"SUBJECT_ID\"].astype(str)\n",
    "egfr.to_csv(egfr_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "btc_adad_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_adad_filename = \"btc01-coh03_adad01_noex.tsv\"\n",
    "btc_adad_dir_filename = os.path.join(btc_adad_dir, btc_adad_filename)\n",
    "btc_adad = pd.read_csv(btc_adad_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "btc_adad[\"SUBJECT_ID\"] = btc_adad[\"SUBJECT_ID\"].astype(str)\n",
    "btc_adad.to_csv(btc_adad_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "annotation_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "pheno_prefix = \"egfr03\"\n",
    "anno_prefix = \"btc01-coh03_adad01-noex\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"outer\"\n",
    "btc_adad_egfr = map_annotation(egfr, btc_adad, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                             pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Adding CKD, microcytosis, and anemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_adad_egfr_ckd = one_condition_conversion(btc_adad_egfr, \"EGFRCKDEPI\", 60, \"CKD\")\n",
    "btc_microcytosis_adad_egfr_ckd = one_condition_conversion(btc_adad_egfr_ckd, \"mcv_entvol_rbc_1\", 80, \"microcytosis\")\n",
    "btc02_adad_egfr_ckd = two_condition_conversion(btc_microcytosis_adad_egfr_ckd, \"hemoglobin_mcnc_bld_1\", 13, 12, \"male\", \"anemia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc02_adad_egfr_ckd_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc02_adad_egfr_ckd_filename = \"btc02-coh03_adad01-noex_egfr03-ckd.tsv\"\n",
    "btc02_adad_egfr_ckd_dir_filename = os.path.join(btc02_adad_egfr_ckd_dir, btc02_adad_egfr_ckd_filename)\n",
    "btc02_adad_egfr_ckd.to_csv(btc02_adad_egfr_ckd_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Concatenate Annotation (w DDIMER) and BCT-ADAD01_eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_adad_egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_adad_egfr_filename = \"btc02-coh03_adad01-noex_egfr03-ckd.tsv\"\n",
    "btc_adad_egfr_dir_filename = os.path.join(btc_adad_egfr_dir, btc_adad_egfr_filename)\n",
    "btc_adad_egfr = pd.read_csv(btc_adad_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "anno_ddimer_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "anno_ddimer_filename = \"freeze8_anno04_af02_unique02_ddimer.tsv\"\n",
    "anno_ddimer_dir_filename = os.path.join(anno_ddimer_dir, anno_ddimer_filename)\n",
    "anno_ddimer = pd.read_csv(anno_ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\", \"male\"]\n",
    "annotation_col = [\"unique_subject_key\", \"subject_id\", \"study\", \"sex\"]\n",
    "pheno_prefix = \"btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex\"\n",
    "anno_prefix = \"freeze8_anno04_af02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "anno_ddimer_btc_adad_egfr = map_annotation(btc_adad_egfr, anno_ddimer, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                                           pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Merging JHS blood cell traits and ddimer with processed results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_noex_adad_noex_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filename = \"freeze8_anno04_af02_btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex.tsv\"\n",
    "pheno_noex_adad_noex_dir_filename = os.path.join(pheno_noex_adad_noex_dir, pheno_noex_adad_noex_filename)\n",
    "pheno_noex_adad_noex = pd.read_csv(pheno_noex_adad_noex_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_useful_filename = \"jhs_usefule_phenotype_05072019.tsv\"\n",
    "jhs_useful_dir_filename = os.path.join(pheno_dir, jhs_useful_filename)\n",
    "jhs_pheno_useful = pd.read_csv(jhs_useful_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_col = \"NWDID\"\n",
    "df1_col = \"NWDID\"\n",
    "pheno_noex_adad_noex_filljhs = merge_replace_nan(df0_col, df1_col, pheno_noex_adad_noex, jhs_pheno_useful)\n",
    "pheno_noex_adad_noex_filljhs_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filljhs_filename = \"freeze8_anno04_af02_btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex_filljhs.tsv\"\n",
    "pheno_noex_adad_noex_filljhs_dir_filename = os.path.join(pheno_noex_adad_noex_filljhs_dir, pheno_noex_adad_noex_filljhs_filename)\n",
    "pheno_noex_adad_noex_filljhs.to_csv(pheno_noex_adad_noex_filljhs_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Apply Exclusion Strategy on Mapped Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_noex_adad_noex_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filename = \"freeze8_anno04_af02_btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex.tsv\"\n",
    "pheno_noex_adad_noex_dir_filename = os.path.join(pheno_noex_adad_noex_dir, pheno_noex_adad_noex_filename)\n",
    "pheno_noex_adad_noex = pd.read_csv(pheno_noex_adad_noex_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.1 Gengrp6: excluding samples with CONSENT_text == DROP no matter how INTERNAL_USE_ONLY is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67999, 58)\n",
      "(67999, 58)\n"
     ]
    }
   ],
   "source": [
    "print(pheno_noex_adad_noex.shape)\n",
    "pheno_noex_adad = pheno_noex_adad_noex.loc[pheno_noex_adad_noex.loc[:, \"CONSENT_text\"] != \"DROP\", :]\n",
    "print(pheno_noex_adad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.2 DDIMER: excluding samples with sample_remove_DDIMER == TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67999, 58)\n"
     ]
    }
   ],
   "source": [
    "pheno_adad = pheno_noex_adad.loc[pheno_noex_adad.loc[:, \"sample_remove_DDIMER\"] != 1, :]\n",
    "print(pheno_adad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.3 Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_adad_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_adad_filename = \"freeze8_anno04_af02_btc03-coh03_egfr03-ckd_adad01.tsv\"\n",
    "pheno_adad_dir_filename = os.path.join(pheno_adad_dir, pheno_adad_filename)\n",
    "pheno_adad.to_csv(pheno_adad_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. uPAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uPAR is the name of the genes we are interested in, and APOL1 is the name of the diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert eGFR ID to Annotation File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Keep AA and Hispanics for eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_unique_filename = \"egfr03-01_unique.tsv\"\n",
    "egfr_unique_dir_filename = os.path.join(egfr_dir, egfr_unique_filename)\n",
    "egfr_unique = pd.read_csv(egfr_unique_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_unique[\"AA\"] = 0\n",
    "egfr_unique.loc[egfr_unique.loc[:, \"race\"] == 2, \"AA\"] = 1\n",
    "egfr_sample_num = egfr_unique.shape[0]\n",
    "rm_idx_list = []\n",
    "for sample_i in range(egfr_sample_num):\n",
    "    tmp_AA = egfr_unique.loc[sample_i, \"AA\"]\n",
    "    tmp_ethnicity = egfr_unique.loc[sample_i, \"ethnicity\"]\n",
    "    if tmp_AA == 0 and tmp_ethnicity == 0:\n",
    "        rm_idx_list.append(sample_i)\n",
    "egfr = egfr_unique.drop(labels = rm_idx_list, axis = 0)\n",
    "egfr_filename = \"egfr03-01_unique_AA_HS.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr.to_csv(egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Map eGFR to Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_filename = \"egfr03-01_unique_AA_HS.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\", \"male\"]\n",
    "annotation_col = [\"unique_subject_key\", \"subject_id\", \"study\", \"sex\"]\n",
    "pheno_prefix = \"btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex\"\n",
    "anno_prefix = \"freeze8_anno04_af02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "anno_ddimer_btc_adad_egfr = map_annotation(btc_adad_egfr, anno_ddimer, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                                           pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_col = [\"unique_subject_key\", \"male\", \"SUBJECT_ID\"]\n",
    "annotation_col = [\"unique_subject_key\", \"sex\", \"subject_id\"]\n",
    "#pheno_col = [\"unique_subject_key\", \"male\"]\n",
    "#annotation_col = [\"unique_subject_key\", \"sex\"]\n",
    "pheno_prefix = \"egfr\"\n",
    "anno_prefix = \"freeze8_anno05_af02_unique02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "\n",
    "egfr_mapped = map_annotation(egfr, annotation, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                             pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coding APOL1 Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs71785313\", \"rs73885319\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "apol1_snp = read_snp_list_together(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = apol1_snp.shape[0]\n",
    "apol1 = pd.DataFrame(columns = [\"NWDID\", \"APOL1\"])\n",
    "apol1[\"NWDID\"] = apol1_snp[\"NWDID\"]\n",
    "apol1[\"APOL1\"] = apol1_snp[\"rs71785313\"] + apol1_snp[\"rs73885319\"]\n",
    "apol1.loc[apol1.loc[:, \"APOL1\"] != 2, \"APOL1\"] = 0\n",
    "apol1.loc[apol1.loc[:, \"APOL1\"] == 2, \"APOL1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "apol1_filename = \"APOL1_status.tsv\"\n",
    "apol1_dir = os.path.join(\"..\", \"cohort\", \"APOL1\")\n",
    "apol1_dir_filename = os.path.join(apol1_dir, apol1_filename)\n",
    "apol1.to_csv(apol1_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correctness of rs334 freeze8 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103646, 3)\n",
      "(103645, 2)\n"
     ]
    }
   ],
   "source": [
    "rs334_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs334_freeze6a_filename = \"snp_rs334_hetero.tsv\"\n",
    "rs334_freeze8_filename = \"freeze8_rs334_hetero.tsv\"\n",
    "\n",
    "rs334_freeze6a_dir_filename = os.path.join(rs334_dir, rs334_freeze6a_filename)\n",
    "rs334_freeze8_dir_filename = os.path.join(rs334_dir, rs334_freeze8_filename)\n",
    "\n",
    "rs334_freeze6a = pd.read_csv(rs334_freeze6a_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "rs334_freeze8 = pd.read_csv(rs334_freeze8_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_base_NWDID = rs334_freeze6a.merge(rs334_freeze8, on = \"NWDID\", how = \"inner\")\n",
    "print(rs334_base_NWDID.shape)\n",
    "rs334_base_NWDID_filename = \"rs334_base_NWDID.tsv\"\n",
    "rs334_base_NWDID_dir_filename = os.path.join(rs334_dir, rs334_base_NWDID_filename)\n",
    "rs334_base_NWDID.to_csv(rs334_base_NWDID_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "rs334_base_NWDID_geno = rs334_freeze6a.merge(rs334_freeze8, on = [\"NWDID\", \"geno\"], how = \"inner\")\n",
    "print(rs334_base_NWDID_geno.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. rs399145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocess Raw rs399145 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs399145\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "rs399145 = read_snp_list_together(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs399145_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs399145_filename = \"%s_rs399145_hetero.tsv\"%snp_ver\n",
    "rs399145_dir_filename = os.path.join(rs399145_dir, rs399145_filename)\n",
    "rs399145.to_csv(rs399145_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 (All Population) Find Two Copies of Minor Allele Individuals of rs399145 while rs334 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_dir = os.path.join(\"..\", \"prepro_data\", \"snp\")\n",
    "rs399145_filename = \"freeze8_rs399145_hetero.tsv\"\n",
    "rs399145_dir_filename = os.path.join(snp_dir, rs399145_filename)\n",
    "rs399145 = pd.read_csv(rs399145_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_filename = \"freeze8_rs334_hetero.tsv\"\n",
    "rs334_dir_filename = os.path.join(snp_dir, rs334_filename)\n",
    "rs334 = pd.read_csv(rs334_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 3)\n"
     ]
    }
   ],
   "source": [
    "rs334_rs399145 = rs334.merge(rs399145, on = \"NWDID\", how = \"inner\")\n",
    "rs334_heter_rs399145 = rs334_rs399145.loc[rs334_rs399145.loc[:, \"rs334\"] == 1, :]\n",
    "rs334_heter_rs399145_homo = rs334_heter_rs399145.loc[rs334_heter_rs399145.loc[:, \"rs399145\"] == 2, :]\n",
    "print(rs334_heter_rs399145_homo.shape)\n",
    "rs334_heter_rs399145_homo_dir = os.path.join(\"..\", \"data_summary\")\n",
    "rs334_heter_rs399145_homo_filename = \"freeze8_rs334-heter_rs399145-2.tsv\"\n",
    "rs334_heter_rs399145_homo_dir_filename = os.path.join(rs334_heter_rs399145_homo_dir, rs334_heter_rs399145_homo_filename)\n",
    "rs334_heter_rs399145_homo.to_csv(rs334_heter_rs399145_homo_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4)\n"
     ]
    }
   ],
   "source": [
    "rs334_rs399145_dir = os.path.join(\"..\", \"cohort\", \"APOL1\", \"ready_data\")\n",
    "rs334_rs399145_filename = \"common_rs399145-rs334.tsv\"\n",
    "rs334_rs399145_dir_filename = os.path.join(rs334_rs399145_dir, rs334_rs399145_filename)\n",
    "rs334_rs399145 = pd.read_csv(rs334_rs399145_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "rs334_heter_rs399145 = rs334_rs399145.loc[rs334_rs399145.loc[:, \"rs334\"] == 1, :]\n",
    "rs334_heter_rs399145_homo = rs334_heter_rs399145.loc[rs334_heter_rs399145.loc[:, \"rs399145\"] == 2, :]\n",
    "print(rs334_heter_rs399145_homo.shape)\n",
    "\n",
    "rs334_heter_rs399145_homo_dir = os.path.join(\"..\", \"data_summary\")\n",
    "rs334_heter_rs399145_homo_filename = \"freeze8_rs334-heter_rs399145-2_used.tsv\"\n",
    "rs334_heter_rs399145_homo_dir_filename = os.path.join(rs334_heter_rs399145_homo_dir, rs334_heter_rs399145_homo_filename)\n",
    "rs334_heter_rs399145_homo.to_csv(rs334_heter_rs399145_homo_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../eqtl/eqtl_script/function_process_data_eqtl.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  snp_df.dropna(axis = 0, how = \"any\", inplace = True)\n"
     ]
    }
   ],
   "source": [
    "snp_ver = \"freeze8\"\n",
    "snp_dir = os.path.join(\"..\", \"raw_data\", \"snp\")\n",
    "snp_id_list = [\"rs2302524\", \"rs2633317\", \"rs4251805\", \"rs4760\", \"rs73935023\"]\n",
    "snp_dir_filename_list = [os.path.join(snp_dir, \"%s_%s.raw\"%(snp_ver, snp_id)) for snp_id in snp_id_list]\n",
    "snp_list = read_snp_list_each(snp_dir_filename_list, snp_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "There is no duplicates in NWDID in DDIMER I processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ddimer_filename = \"TOPMED_HarmonizedPhenotypes_DDIMER_21MAY2019.csv\"\n",
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer_raw = pd.read_csv(ddimer_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "ddimer = ddimer_raw.loc[:, [\"sample.id\", \"STUDY\", \"sex\", \"AGE_DDIMER\", \"DDIMER\", \"sample_remove_DDIMER\"]]\n",
    "ddimer.dropna(axis = \"index\", how = \"any\", inplace = True)\n",
    "ddimer.rename(columns = {\"sample.id\":\"NWDID\", \"STUDY\":\"cohort\", \"AGE_DDIMER\":\"age_at_DDIMER\"}, inplace = True)\n",
    "sex_dict = {\"M\":1, \"F\":0}\n",
    "ddimer.replace({\"sex\": sex_dict}, inplace = True)\n",
    "save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(save_dir, ddimer_filename)\n",
    "ddimer.to_csv(ddimer_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "print(duplicates_num(ddimer, \"NWDID\"))\n",
    "print(\"There is no duplicates in NWDID in DDIMER I processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate DDIMER to Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer = pd.read_csv(ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno04_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex_y is inconsistent.\n"
     ]
    }
   ],
   "source": [
    "pheno_col = [\"NWDID\", \"sex\", \"cohort\"]\n",
    "annotation_col = [\"NWDID\", \"sex\", \"study\"]\n",
    "pheno_prefix = \"ddimer\"\n",
    "anno_prefix = \"freeze8_anno04_af02_unique02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"NWDID\"\n",
    "merge_how = \"left\"\n",
    "ddimer_mapped = map_annotation(ddimer, annotation, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                               pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B. Preprocess weight, center and gengrp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 6)\n",
      "(11829, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOL</td>\n",
       "      <td>11829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cohort  sample_size\n",
       "0    SOL        11829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gengrp6_filename = \"page-harmonized-phenotypes-pca-freeze2-candidate2-2016-12-14.GWASid_fid_22May2018internalPCs.SOLv2consent.txt\"\n",
    "gengrp6_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\", \"gengrp6\")\n",
    "gengrp6_dir_filename = os.path.join(gengrp6_dir, gengrp6_filename)\n",
    "gengrp6 = pd.read_csv(gengrp6_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6.replace(\".\", np.nan, inplace=True)\n",
    "gengrp6_select = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\",\n",
    "                          \"gengrp6\", \"study\"]].dropna(axis=0, subset=[\"analysis_id\",\"gengrp6\"],how=\"any\")\n",
    "print(gengrp6_select.shape)\n",
    "\n",
    "gengrp6_select_dropna = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\", \"gengrp6\", \"study\"]].dropna(axis=0, how=\"any\")\n",
    "gengrp6_select_dropna.rename(columns = {\"analysis_id\":\"SUBJECT_ID\", \"study\":\"cohort\"}, inplace=True)\n",
    "print(gengrp6_select_dropna.shape)\n",
    "cat_col = \"cohort\"\n",
    "gengrp6_category_summary, category_list = categorize_df(gengrp6_select_dropna, cat_col)\n",
    "display(gengrp6_category_summary)\n",
    "gengrp6_select_dropna[\"cohort\"] = gengrp6_select_dropna.shape[0] * [\"HCHS_SOL\"]\n",
    "gengrp6_select_dropna.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gengrp6_select_dropna_num = gengrp6_select_dropna.shape[0]\n",
    "gengrp6_select_dropna[\"unique_subject_key\"] = gengrp6_select_dropna_num * [None]\n",
    "for gengrp6_select_dropna_i in range(gengrp6_select_dropna_num):\n",
    "    gengrp6_select_dropna.loc[gengrp6_select_dropna_i, \"unique_subject_key\"] = \"%s_%s\"%(\"HCHS_SOL\", gengrp6_select_dropna.loc[gengrp6_select_dropna_i, \"SUBJECT_ID\"])\n",
    "#    if (gengrp6_select_dropna_i + 1) % 100 == 0:\n",
    "#        print(\"%d/%d\"%(gengrp6_select_dropna_i + 1, gengrp6_select_dropna_num))\n",
    "gengrp6_select_dropna_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_select_dropna_filename = \"gengrp6_sol_noexclude.tsv\"\n",
    "gengrp6_select_dropna_dir_filename = os.path.join(gengrp6_select_dropna_dir, gengrp6_select_dropna_filename)\n",
    "gengrp6_select_dropna.to_csv(gengrp6_select_dropna_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight and center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16415, 3)\n"
     ]
    }
   ],
   "source": [
    "weight_center_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\")\n",
    "weight_center_filename = \"bloodcell_output.csv\"\n",
    "weight_center_dir_filename = os.path.join(weight_center_dir, weight_center_filename)\n",
    "weight_center = pd.read_csv(weight_center_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "weight_center_select = weight_center[[\"ID\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\"]].dropna(axis = 0, how = \"any\")\n",
    "weight_center_select.rename(columns = {\"ID\":\"z_sol_id\"}, inplace = True)\n",
    "print(weight_center_select.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 9)\n"
     ]
    }
   ],
   "source": [
    "gengrp6_select_dropna[\"z_sol_id\"] = gengrp6_select_dropna[\"z_sol_id\"].astype(str)\n",
    "weight_center_select[\"z_sol_id\"] = weight_center_select[\"z_sol_id\"].astype(str)\n",
    "gengrp6_weight_center = gengrp6_select_dropna.merge(weight_center_select, on = \"z_sol_id\", how = \"outer\")\n",
    "gengrp6_weight_center.dropna(axis=0, subset=[\"SUBJECT_ID\"],how=\"any\", inplace = True)\n",
    "print(gengrp6_weight_center.shape)\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_weight_center_filename = \"gengrp6_center_weight_noex.tsv\"\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center.to_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating gengrp6_weight_center table to phenotype table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192282, 35)\n",
      "duplicated unique_subject_key 0\n",
      "duplicated unique_subject_key 0\n"
     ]
    }
   ],
   "source": [
    "btc_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_filename = \"btc01-coh03.tsv\"\n",
    "btc_dir_filename = os.path.join(btc_dir, btc_filename)\n",
    "btc = pd.read_csv(btc_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "btc[\"SUBJECT_ID\"] = btc[\"SUBJECT_ID\"].astype(str)\n",
    "print(btc.shape)\n",
    "print(\"duplicated unique_subject_key\", duplicates_num(btc, \"unique_subject_key\"))\n",
    "\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_weight_center_filename = \"gengrp6_center_weight_noex.tsv\"\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center = pd.read_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6_weight_center[\"SUBJECT_ID\"] = gengrp6_weight_center[\"SUBJECT_ID\"].astype(str)\n",
    "print(\"duplicated unique_subject_key\", duplicates_num(gengrp6_weight_center, \"unique_subject_key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc and gengrp6_weight_center in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "annotation_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "pheno_prefix = \"adad01_noex\"\n",
    "anno_prefix = \"btc01-coh03\"\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"left\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "btc_adad = map_annotation(gengrp6_weight_center, btc, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                          pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C. Preprocess eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ARIC</td>\n",
       "      <td>3555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amish</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FHS</td>\n",
       "      <td>3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENOA</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GenSalt</td>\n",
       "      <td>1666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GeneSTAR</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HyperGEN</td>\n",
       "      <td>1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JHS</td>\n",
       "      <td>3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MESA</td>\n",
       "      <td>4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHI</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort  sample_size\n",
       "8      ARIC         3555\n",
       "9     Amish         1023\n",
       "0       FHS         3087\n",
       "5     GENOA         1023\n",
       "3   GenSalt         1666\n",
       "7  GeneSTAR          190\n",
       "2  HyperGEN         1759\n",
       "4       JHS         3115\n",
       "6      MESA         4090\n",
       "1       WHI         4224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egfr_old_filename = \"data_all_2_eGFR_Jan2019.csv\"\n",
    "egfr_old_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\")\n",
    "egfr_old_dir_filename = os.path.join(egfr_old_dir, egfr_old_filename)\n",
    "egfr_old = pd.read_csv(egfr_old_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "cat_col = \"Study\"\n",
    "egfr_old_summary, category_list = categorize_df(egfr_old, cat_col)\n",
    "egfr_old_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_old_summary_filename = \"data_all_2_eGFR_Jan2019_category_summary.tsv\"\n",
    "egfr_old_summary_dir_filename = os.path.join(egfr_old_summary_dir, egfr_old_summary_filename)\n",
    "egfr_old_summary.to_csv(egfr_old_summary_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "display(egfr_old_summary)\n",
    "save_dir = egfr_old_dir\n",
    "file_prefix = \"eGFR_Jan2019\"\n",
    "split_df(egfr_old, cat_col, category_list, file_prefix, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort AMISH completed.\n",
      "Cohort ARIC completed.\n",
      "Cohort FHS completed.\n",
      "Cohort GENOA completed.\n",
      "Cohort GenSalt completed.\n",
      "Cohort GeneSTAR completed.\n",
      "Cohort HyperGEN completed.\n",
      "Cohort JHS completed.\n",
      "Cohort MESA completed.\n",
      "Cohort WHI completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># old not in new</th>\n",
       "      <th># new not in old</th>\n",
       "      <th># overlap</th>\n",
       "      <th># old</th>\n",
       "      <th># new</th>\n",
       "      <th>cor</th>\n",
       "      <th># dup old</th>\n",
       "      <th># dup new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AMISH</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>1118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIC</td>\n",
       "      <td>0</td>\n",
       "      <td>11782</td>\n",
       "      <td>3555</td>\n",
       "      <td>3555</td>\n",
       "      <td>15337</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FHS</td>\n",
       "      <td>-52</td>\n",
       "      <td>348</td>\n",
       "      <td>3139</td>\n",
       "      <td>3087</td>\n",
       "      <td>3487</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GENOA</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GenSalt</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1666</td>\n",
       "      <td>1666</td>\n",
       "      <td>1846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GeneSTAR</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HyperGEN</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1759</td>\n",
       "      <td>1759</td>\n",
       "      <td>1884</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JHS</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MESA</td>\n",
       "      <td>0</td>\n",
       "      <td>2323</td>\n",
       "      <td>4090</td>\n",
       "      <td>4090</td>\n",
       "      <td>6413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WHI</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>4224</td>\n",
       "      <td>4224</td>\n",
       "      <td>4692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # old not in new # new not in old # overlap # old  # new cor  \\\n",
       "AMISH                   0               95      1023  1023   1118   1   \n",
       "ARIC                    0            11782      3555  3555  15337   1   \n",
       "FHS                   -52              348      3139  3087   3487   1   \n",
       "GENOA                   0              560      1023  1023   1583   1   \n",
       "GenSalt                 0              180      1666  1666   1846   1   \n",
       "GeneSTAR                0               16       190   190    206   1   \n",
       "HyperGEN                0              125      1759  1759   1884   1   \n",
       "JHS                     0              275      3115  3115   3390   1   \n",
       "MESA                    0             2323      4090  4090   6413   1   \n",
       "WHI                     0              468      4224  4224   4692   1   \n",
       "\n",
       "         # dup old # dup new  \n",
       "AMISH            0         0  \n",
       "ARIC             0         0  \n",
       "FHS              0        58  \n",
       "GENOA            0         0  \n",
       "GenSalt          0         0  \n",
       "GeneSTAR         0         0  \n",
       "HyperGEN         0         0  \n",
       "JHS              0         0  \n",
       "MESA             0         0  \n",
       "WHI              0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "egfr_old_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "egfr_new_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "cohort_list = [\"AMISH\", \"ARIC\", \"FHS\", \"GENOA\", \"GenSalt\", \"GeneSTAR\", \"HyperGEN\", \"JHS\", \"MESA\", \"WHI\"]\n",
    "compare_egfr_df = pd.DataFrame(columns = [\"# old not in new\", \"# new not in old\", \"# overlap\", \"# old\", \"# new\", \"cor\", \"# dup old\", \"# dup new\"], index = cohort_list)\n",
    "for cohort in cohort_list:\n",
    "    egfr_old_dir_filename = os.path.join(egfr_old_dir, \"eGFR_Jan2019_%s.tsv\"%cohort)\n",
    "    egfr_new_dir_filename = os.path.join(egfr_new_dir, \"egfr_calculated_%s.tsv\"%cohort)\n",
    "    egfr_old = pd.read_csv(egfr_old_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    common_col = \"id\"\n",
    "    egfr_old_col = \"EGFR\"\n",
    "    egfr_new_col = \"EGFR\"\n",
    "    egfr_old[egfr_old_col] = egfr_old[egfr_old_col].astype(float)\n",
    "    egfr_new[egfr_new_col] = egfr_new[egfr_new_col].astype(float)\n",
    "    df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num, cor = compare_df_pair(egfr_old, egfr_new, common_col, egfr_old_col, egfr_new_col)\n",
    "    compare_egfr_df.loc[cohort, \"# old not in new\"] = df1_nodf2_num\n",
    "    compare_egfr_df.loc[cohort, \"# new not in old\"] = df2_nodf1_num\n",
    "    compare_egfr_df.loc[cohort, \"# overlap\"] = overlap_num\n",
    "    compare_egfr_df.loc[cohort, \"# old\"] = df1_num\n",
    "    compare_egfr_df.loc[cohort, \"# new\"] = df2_num\n",
    "    compare_egfr_df.loc[cohort, \"cor\"] = cor\n",
    "    compare_egfr_df.loc[cohort, \"# dup old\"] = duplicates_num(egfr_old, common_col)\n",
    "    compare_egfr_df.loc[cohort, \"# dup new\"] = duplicates_num(egfr_new, common_col)\n",
    "    print(\"Cohort %s completed.\"%cohort)\n",
    "display(compare_egfr_df)\n",
    "compare_egfr_dir = os.path.join(\"..\", \"data_summary\")\n",
    "compare_egfr_filename = \"compare_egft_calculated_Jan2019_summary_nodup.tsv\"\n",
    "compare_egfr_dir_filename = os.path.join(compare_egfr_dir, compare_egfr_filename)\n",
    "compare_egfr_df.to_csv(compare_egfr_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(Deprecated)** Replace the space elements in the MESA egfr table as NAN and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6429, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6413, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_MESA_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr_new.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "display(egfr_new.shape)\n",
    "egfr_new.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "display(egfr_new.shape)\n",
    "egfr_new_mod_dir_filename = os.path.join(load_dir, \"egfr_calculated_MESA.tsv\")\n",
    "egfr_new.to_csv(egfr_new_mod_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESA: Correlation between cepgfr1c and eGFR I calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993978212689216 0.9952502226206945\n",
      "3.394391478720293\n",
      "33.29065562220411\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "freeze5_egfr_filename = \"eGFR_Jan2019_MESA.tsv\"\n",
    "freeze5_egfr_dir_filename = os.path.join(freeze5_egfr_dir, freeze5_egfr_filename)\n",
    "freeze5_egfr_df = pd.read_csv(freeze5_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "freeze5_egfr = freeze5_egfr_df[\"EGFR\"].values\n",
    "\n",
    "cepgfr1c_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "cepgfr1c_filename = \"TOPMed_kidney_phenotype_MESA_exam1.tsv\"\n",
    "cepgfr1c_dir_filename = os.path.join(cepgfr1c_dir, cepgfr1c_filename)\n",
    "cepgfr1c_df = pd.read_csv(cepgfr1c_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cepgfr1c = cepgfr1c_df[\"EGFR\"].values\n",
    "\n",
    "calegfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "calegfr_filename = \"TOPMed_MESA_RenalPhenotypes_16March2018_egfr.tsv\"\n",
    "calegfr_dir_filename = os.path.join(calegfr_dir, calegfr_filename)\n",
    "calegfr_df = pd.read_csv(calegfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "calegfr = calegfr_df[\"EGFR\"].values\n",
    "\n",
    "calegfr_mod_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "calegfr_mod_filename = \"TOPMed_MESA_RenalPhenotypes_16March2018_egfr_mod.tsv\"\n",
    "calegfr_mod_dir_filename = os.path.join(calegfr_mod_dir, calegfr_mod_filename)\n",
    "calegfr_mod_df = pd.read_csv(calegfr_mod_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "calegfr_mod = calegfr_mod_df[\"EGFR\"].values\n",
    "\n",
    "cor_cepgfr1c_calegfr = pearsonr(cepgfr1c, calegfr)[0]\n",
    "cor_cepgfr1c_calegfr_mod = pearsonr(cepgfr1c, calegfr_mod)[0]\n",
    "print(cor_cepgfr1c_calegfr, cor_cepgfr1c_calegfr_mod)\n",
    "\n",
    "mse_cepgfr1c_calegfr = mean_squared_error(cepgfr1c, calegfr)\n",
    "mse_cepgfr1c_calegfr_mod = mean_squared_error(cepgfr1c, calegfr_mod)\n",
    "print(mse_cepgfr1c_calegfr)\n",
    "print(mse_cepgfr1c_calegfr_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959972669338772 0.9999999999999826\n",
      "17.445292821842674\n",
      "8.525105559709774e-12\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_df.rename(columns = {\"EGFR\":\"EGFR_freeze5\"}, inplace = True)\n",
    "calegfr_df.rename(columns = {\"EGFR\":\"EGFR_calegfr\"}, inplace = True)\n",
    "calegfr_mod_df.rename(columns = {\"EGFR\":\"EGFR_calegfr_mod\"}, inplace = True)\n",
    "egfr_list = [freeze5_egfr_df, calegfr_df, calegfr_mod_df]\n",
    "merge_egfr = merge_df_list(egfr_list, \"id\", merge_method='merge', how = 'inner')\n",
    "common_calegfr = merge_egfr[\"EGFR_calegfr\"].values\n",
    "common_freeze5_egfr = merge_egfr[\"EGFR_freeze5\"].values\n",
    "common_calegfr_mod = merge_egfr[\"EGFR_calegfr_mod\"].values\n",
    "\n",
    "cor_common_freeze5_calegfr = pearsonr(common_freeze5_egfr, common_calegfr)[0]\n",
    "cor_common_freeze5_calegfr_mod = pearsonr(common_freeze5_egfr, common_calegfr_mod)[0]\n",
    "print(cor_common_freeze5_calegfr, cor_common_freeze5_calegfr_mod)\n",
    "\n",
    "mse_common_freeze5_calegfr = mean_squared_error(common_freeze5_egfr, common_calegfr)\n",
    "mse_common_freeze5_calegfr_mod = mean_squared_error(common_freeze5_egfr, common_calegfr_mod)\n",
    "print(mse_common_freeze5_calegfr)\n",
    "print(mse_common_freeze5_calegfr_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949210599819995\n",
      "33.930496703190656\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "freeze5_egfr_filename = \"eGFR_Jan2019_MESA.tsv\"\n",
    "freeze5_egfr_dir_filename = os.path.join(freeze5_egfr_dir, freeze5_egfr_filename)\n",
    "freeze5_egfr_df = pd.read_csv(freeze5_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "cepgfr1c_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "cepgfr1c_filename = \"TOPMed_kidney_phenotype_MESA_exam1.tsv\"\n",
    "cepgfr1c_dir_filename = os.path.join(cepgfr1c_dir, cepgfr1c_filename)\n",
    "cepgfr1c_df = pd.read_csv(cepgfr1c_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "freeze5_egfr_df.rename(columns = {\"EGFR\":\"EGFR_freeze5\"}, inplace = True)\n",
    "cepgfr1c_df.rename(columns = {\"EGFR\":\"EGFR_cepgfr1c\"}, inplace = True)\n",
    "freeze5_cepgfr1c = freeze5_egfr_df.merge(cepgfr1c_df, left_on = \"id\", right_on = \"sidno\", how = \"inner\")\n",
    "common_freeze5_egfr = freeze5_cepgfr1c[\"EGFR_freeze5\"].values\n",
    "common_cepgfr1c = freeze5_cepgfr1c[\"EGFR_cepgfr1c\"].values\n",
    "\n",
    "cor_common_freeze5_cepgfr1c = pearsonr(common_freeze5_egfr, common_cepgfr1c)[0]\n",
    "print(cor_common_freeze5_cepgfr1c)\n",
    "\n",
    "mse_common_freeze5_cepgfr1c = mean_squared_error(common_freeze5_egfr, common_cepgfr1c)\n",
    "print(mse_common_freeze5_cepgfr1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process DHS Exsited eGFR Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"dhs\")\n",
    "dhs_filename = \"phs001412_AF_20190705_nda.csv\"\n",
    "dhs_dir_filename = os.path.join(dhs_dir, dhs_filename)\n",
    "dhs = pd.read_csv(dhs_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "dhs.dropna(axis = 0, subset = [\"CKD_eGFR\", \"Subject_ID\", \"Study\"], inplace = True, how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check race composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'Black'}\n"
     ]
    }
   ],
   "source": [
    "race_set = set(dhs[\"Race\"].values.tolist())\n",
    "print(len(race_set), race_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs.rename(columns = {\"Subject_ID\":\"id\", \"Sex\":\"male\", \"Age\":\"age\", \"Race\":\"race\", \"CKD_eGFR\":\"EGFR\" }, inplace = True)\n",
    "race_dict = {\"Black\":2}\n",
    "male_dict = {\"F\":0, \"M\":1}\n",
    "dhs.replace({\"male\":male_dict, \"race\":race_dict}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_egfr_sample_num = dhs.shape[0]\n",
    "dhs_egfr_col_list = [\"id\", \"scr\", \"race\", \"age\", \"male\", \"EGFR\", \"case.control\"]\n",
    "dhs_egfr = pd.DataFrame(data = np.zeros((dhs_egfr_sample_num, len(dhs_egfr_col_list))), columns = dhs_egfr_col_list)\n",
    "dhs_egfr[\"id\"] = dhs[\"id\"]\n",
    "dhs_egfr[\"race\"] = dhs[\"race\"]\n",
    "dhs_egfr[\"age\"] = dhs[\"age\"]\n",
    "dhs_egfr[\"male\"] = dhs[\"male\"]\n",
    "dhs_egfr[\"EGFR\"] = dhs[\"EGFR\"]\n",
    "dhs_egfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "dhs_egfr_filename = \"TopMed_Kidney_Phenotype_DHS_egfr.tsv\"\n",
    "dhs_egfr_dir_filename = os.path.join(dhs_egfr_dir, dhs_egfr_filename)\n",
    "dhs_egfr.to_csv(dhs_egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving FHS # old not in new negative issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "116\n",
      "Total number of all duplicates are two times of number of samples with duplicates, so each sample with duplicate having only one duplicate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "dup_num = duplicates_num(egfr_new, \"id\")\n",
    "print(dup_num)\n",
    "boolean_series = egfr_new[[\"id\"]].duplicated(keep=False)\n",
    "egfr_new_dup = egfr_new.loc[boolean_series, :]\n",
    "egfr_new_dup.sort_values(axis = 0, by = \"id\", inplace = True)\n",
    "#display(egfr_new_dup)\n",
    "duplicates_list = egfr_new_dup[\"id\"].values.tolist()\n",
    "print(len(duplicates_list))\n",
    "print(\"Total number of all duplicates are two times of number of samples with duplicates, so each sample with duplicate having only one duplicate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to check if egfr of each pair of duplicates are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "All egfr values of pairs of duplicates are identical, so I will drop any of them in each pair.\n"
     ]
    }
   ],
   "source": [
    "egfr_list = egfr_new_dup[\"EGFR\"].values.tolist()\n",
    "duplicate_egfr_diff_list = []\n",
    "egfr_num = len(egfr_list)\n",
    "for egfr_i in range(0, egfr_num, 2):\n",
    "    duplicate_egfr_diff = egfr_list[egfr_i + 1] - egfr_list[egfr_i]\n",
    "    duplicate_egfr_diff_list.append(duplicate_egfr_diff)\n",
    "print(sum(duplicate_egfr_diff_list))\n",
    "print(\"All egfr values of pairs of duplicates are identical, so I will drop any of them in each pair.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr_new.drop_duplicates(subset = \"id\", keep = \"first\", inplace = True)\n",
    "egfr_new_nodup_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS.tsv\")\n",
    "egfr_new.to_csv(egfr_new_nodup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess CHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHS SCr file did not include sex info which is needed for calculating CKS-eGFR. So I map the individual ID to the annotation file to get the sex info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "chs_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"chs\")\n",
    "chs_filename = \"TOPMed_kidney_phenotype_CHS.txt\"\n",
    "chs_dir_filename = os.path.join(chs_dir, chs_filename)\n",
    "chs = pd.read_csv(chs_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "chs['ID'] = chs['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "subject_id_sex = annotation[[\"subject_id\", \"sex\"]]\n",
    "subject_id_sex.rename(columns = {\"subject_id\":\"ID\", \"sex\":\"MALE\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs_sex = chs.merge(subject_id_sex, on = \"ID\", how = \"inner\")\n",
    "chs_sex_filename = \"TOPMed_kidney_phenotype_CHS_sex.txt\"\n",
    "chs_sex_dir = chs_dir\n",
    "chs_sex_dir_filename = os.path.join(chs_sex_dir, chs_sex_filename)\n",
    "chs_sex.to_csv(chs_sex_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess CARDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"cardia\")\n",
    "cardia_dcc_filename = \"cardia_dcc_demographic_v3.txt\"\n",
    "cardia_scr_filename = \"cardia_scr_exam6.txt\"\n",
    "cardia_race_age_filename = \"cardia_race_age_exam6.txt\"\n",
    "\n",
    "cardia_dcc_dir_filename = os.path.join(cardia_dir, cardia_dcc_filename)\n",
    "cardia_scr_dir_filename = os.path.join(cardia_dir, cardia_scr_filename)\n",
    "cardia_race_age_dir_filename = os.path.join(cardia_dir, cardia_race_age_filename)\n",
    "\n",
    "cardia_dcc = pd.read_csv(cardia_dcc_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cardia_scr = pd.read_csv(cardia_scr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cardia_race_age = pd.read_csv(cardia_race_age_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_dcc_scr_race_age_list = [cardia_dcc, cardia_scr, cardia_race_age]\n",
    "common_col = \"ID\"\n",
    "cardia = merge_df_list(cardia_dcc_scr_race_age_list, common_col, merge_method='merge', how = 'inner')\n",
    "\n",
    "cardia.loc[cardia.loc[:, \"MALE\"] == \"female\", \"MALE\"] = 0\n",
    "cardia.loc[cardia.loc[:, \"MALE\"] == \"male\", \"MALE\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_filename = \"cardia_exam6.txt\"\n",
    "cardia_dir_filename = os.path.join(cardia_dir, cardia_filename)\n",
    "cardia.to_csv(cardia_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating All eGFR Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMISH completed.\n",
      "ARIC completed.\n",
      "CARDIA completed.\n",
      "CHS completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DHS completed.\n",
      "FHS completed.\n",
      "GENOA completed.\n",
      "GenSalt completed.\n",
      "GeneSTAR completed.\n",
      "JHS completed.\n",
      "MESA completed.\n",
      "WHI completed.\n",
      "HCHS-SOL completed.\n",
      "HyperGEN completed.\n"
     ]
    }
   ],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "cohort_list = [\"AMISH\", \"ARIC\", \"CARDIA\", \"CHS\", \"DHS\", \"FHS\", \"GENOA\", \"GenSalt\", \"GeneSTAR\", \"JHS\", \"MESA\", \"WHI\", \"HCHS-SOL\", \"HyperGEN\"]\n",
    "header_selection = [\"id\", \"age\", \"race\", \"male\", \"ethnicity\", \"EGFR\"]\n",
    "egfr_full = concat_egfr(egfr_dir, cohort_list, header_selection)\n",
    "egfr_full_filename = \"egfr03-01.tsv\"\n",
    "egfr_full_dir_filename = os.path.join(egfr_dir, egfr_full_filename)\n",
    "egfr_full.to_csv(egfr_full_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing and Recording Duplicates in Comprehensive eGFR Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 8)\n"
     ]
    }
   ],
   "source": [
    "egfr_dup = df_extraction_duplicates(egfr_full, \"unique_subject_key\")\n",
    "print(egfr_dup.shape)\n",
    "egfr_dup = egfr_dup.sort_values(by=['unique_subject_key'])\n",
    "egfr_dup_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_dup_filename = \"egfr03-01_dup.tsv\"\n",
    "egfr_dup_dir_filename = os.path.join(egfr_dup_dir, egfr_dup_filename)\n",
    "egfr_dup.to_csv(egfr_dup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "invar_subsets = [\"race\", \"male\", \"cohort\"]\n",
    "egfr_unique = remove_dup_anno(egfr_full, invar_subsets)\n",
    "egfr_unique_filename = \"egfr03-01_unique.tsv\"\n",
    "egfr_unique_dir_filename = os.path.join(egfr_dir, egfr_unique_filename)\n",
    "egfr_unique.to_csv(egfr_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating eGFR to phenotype table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223838, 37)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh03_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "pheno[\"SUBJECT_ID\"] = pheno[\"SUBJECT_ID\"].astype(str)\n",
    "\n",
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_filename = \"egfr01.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr[\"SUBJECT_ID\"] = egfr[\"SUBJECT_ID\"].astype(str)\n",
    "\n",
    "pheno_egfr = pheno.merge(egfr, on = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"], how = \"outer\")\n",
    "print(pheno_egfr.shape)\n",
    "print(duplicates_num(pheno_egfr, \"unique_subject_key\"))\n",
    "pheno_egfr_filename = \"coh03_pheno01_pre.tsv\"\n",
    "pheno_egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_egfr_dir_filename = os.path.join(pheno_egfr_dir, pheno_egfr_filename)\n",
    "pheno_egfr.to_csv(pheno_egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A\n",
       "0   0.0\n",
       "1   0.0\n",
       "2   1.0\n",
       "3   1.0\n",
       "4   0.0\n",
       "5   0.0\n",
       "6   1.0\n",
       "7   0.0\n",
       "8   1.0\n",
       "9   0.0\n",
       "10  1.0\n",
       "11  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(df1.apply(func, axis = 1), columns = ['A'])\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B\n",
       "0   0.0  0\n",
       "1   0.0  1\n",
       "2   1.0  1\n",
       "3   1.0  1\n",
       "4   0.0  0\n",
       "5   0.0  0\n",
       "6   1.0  1\n",
       "7   1.0  0\n",
       "8   1.0  1\n",
       "9   0.0  0\n",
       "10  NaN  0\n",
       "11  0.0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B\n",
       "0   0.0  0\n",
       "1   0.0  1\n",
       "2   1.0  1\n",
       "3   1.0  1\n",
       "4   0.0  0\n",
       "5   0.0  0\n",
       "6   1.0  1\n",
       "7   1.0  0\n",
       "8   1.0  1\n",
       "9   0.0  0\n",
       "10  0.0  0\n",
       "11  0.0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = pd.DataFrame({'A': [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, None, 0], 'B': [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0]})\n",
    "display(df4)\n",
    "df4.iloc[:, 0] = df4.apply(func, axis = 1)\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sample_lost = df4[df4.iloc[:, 0].isnull()].index.tolist()\n",
    "print(sample_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  a  3\n",
       "1  g  4\n",
       "2  c  5\n",
       "3  d  6\n",
       "4  k  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B      C\n",
       "0  a NaN   nice\n",
       "1  b NaN   good\n",
       "2  c NaN   exce\n",
       "3  d NaN    ama\n",
       "4  e NaN    awe\n",
       "5  f NaN    NaN\n",
       "6  g NaN   perf\n",
       "7  h NaN  terri\n",
       "8  i NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B      C\n",
       "A            \n",
       "a  3.0   nice\n",
       "b  NaN   good\n",
       "c  5.0   exce\n",
       "d  6.0    ama\n",
       "e  NaN    awe\n",
       "f  NaN    NaN\n",
       "g  4.0   perf\n",
       "h  NaN  terri\n",
       "i  NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B      C\n",
       "A            \n",
       "a  3.0   nice\n",
       "b  NaN   good\n",
       "c  5.0   exce\n",
       "d  6.0    ama\n",
       "e  NaN    awe\n",
       "f  NaN    NaN\n",
       "g  4.0   perf\n",
       "h  NaN  terri\n",
       "i  NaN   well\n",
       "k  7.0    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>5.0</td>\n",
       "      <td>exce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>perf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A    B      C\n",
       "0  a  3.0   nice\n",
       "1  b  NaN   good\n",
       "2  c  5.0   exce\n",
       "3  d  6.0    ama\n",
       "4  e  NaN    awe\n",
       "5  f  NaN    NaN\n",
       "6  g  4.0   perf\n",
       "7  h  NaN  terri\n",
       "8  i  NaN   well"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df0 = pd.DataFrame({'A': [\"a\", \"g\", \"c\", \"d\", \"k\"],\n",
    "                    'B': [3, 4, 5, 6, 7]})\n",
    "df1 = pd.DataFrame({'A': [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "                    'B': [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "                    'C': [\"nice\", \"good\", \"exce\", \"ama\", \"awe\", np.nan, \"perf\", \"terri\", \"well\"]})\n",
    "display(df0)\n",
    "display(df1)\n",
    "df1 = df1.set_index('A')\n",
    "df0 = df0.set_index('A')\n",
    "df3 = df1.fillna(df0)\n",
    "df4 = df1.combine_first(df0)\n",
    "display(df3)\n",
    "display(df4)\n",
    "df3 = df3.reset_index()\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df0.merge(df1, left_on = ['B'], right_on = ['C'], how = \"inner\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation(pheno, annotation, pheno_col, annotation_col, pivot_col, how_merge, pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir):\n",
    "    pheno_mapped = annotation.merge(pheno, left_on = annotation_col[0], right_on = pheno_col[0], how = merge_how)\n",
    "    col_num = len(pheno_col[1:])\n",
    "    for col_i in range(1, col_num + 1):\n",
    "        if pheno_col[col_i] != annotation_col[col_i]:\n",
    "            pheno_mapped.drop(axis = 1, columns = [pheno_col[col_i], annotation_col[col_i]], inplace = True)\n",
    "        else:\n",
    "            tmp_pheno_col = \"%s_y\"%pheno_col[col_i]\n",
    "            tmp_annotation_col = \"%s_x\"%annotation_col[col_i]\n",
    "            pheno_mapped.drop(axis = 1, columns = [tmp_pheno_col, tmp_annotation_col], inplace = True)\n",
    "    if len(annotation_col) > 1:\n",
    "        depre_sample_list, pivot_col_df = map_deprecation_list(pheno, annotation, pheno_col, annotation_col,\n",
    "                                                               pivot_col, pheno_prefix, anno_prefix, diff_save_dir)\n",
    "        if depre_sample_list != []:\n",
    "            pheno_mapped_raw = pheno_mapped.copy()\n",
    "            del pheno_mapped\n",
    "            pheno_mapped = pheno_mapped_raw[~pheno_mapped_raw[pivot_col].isin(depre_sample_list)]\n",
    "        pheno_mapped = pheno_mapped.merge(pivot_col_df, on = pivot_col, how = \"inner\")\n",
    "    pheno_mapped_filename = \"%s_%s.tsv\"%(anno_prefix, pheno_prefix)\n",
    "    pheno_mapped_dir_filename = os.path.join(mapped_save_dir, pheno_mapped_filename)\n",
    "    pheno_mapped.to_csv(pheno_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return pheno_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation(df, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    df_pivot = df[[pivot_col]]\n",
    "    df_col = df[[pheno_col, annotation_col]]\n",
    "    df_col.iloc[:, 0] = df_col.apply(func, axis = 1)\n",
    "    df_col.iloc[:, 1] = df_col.apply(func, axis = 1)\n",
    "    del df\n",
    "    df = pd.concat([df_pivot, df_col], axis = 1)\n",
    "    depre_sample_index_list = df[df.iloc[:, 0].isnull()].index.tolist()\n",
    "    if depre_sample_index_list == []:\n",
    "        depre_sample_list = []\n",
    "    else:\n",
    "        depre_sample_list = df.loc[depre_sample_index_list, pivot_col].values.reshape(1, -1).tolist()[0]\n",
    "    df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "    tmp_compare = df[pheno_col].eq(df[annotation_col], axis = 0)\n",
    "    df_dif = df[tmp_compare == False]\n",
    "    df_cons = df[tmp_compare == True]\n",
    "    if annotation_col.split('_x')[-1] == \"\":\n",
    "        annotation_col_propagate = annotation_col.split('_x')[0]\n",
    "        df_cons.rename(columns = {annotation_col:annotation_col_propagate}, inplace = True)\n",
    "    else:\n",
    "        annotation_col_propagate = annotation_col\n",
    "    df_cons_propagate = df_cons.loc[:, [pivot_col, annotation_col_propagate]]\n",
    "    if df_dif.shape[0] != 0:\n",
    "        print(\"%s is inconsistent.\"%pheno_col)\n",
    "        df_dif_filename = \"%s_%s_%s.tsv\"%(anno_prefix, pheno_prefix, annotation_col)\n",
    "        df_dif_dir_filename = os.path.join(save_dir, df_dif_filename)\n",
    "        df_dif.to_csv(df_dif_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        depre_sample_list = df_dif[[pivot_col]].values.reshape(1, -1).tolist()[0]\n",
    "    return depre_sample_list, df_cons_propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
