{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import feather\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"/proj/yunligrp/users/minzhi/utils/pylib\" not in sys.path:\n",
    "    sys.path.insert(0, \"/proj/yunligrp/users/minzhi/utils/pylib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_process_data_eqtl import *\n",
    "from function_asso import *\n",
    "from function_mesa_cca import *\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_map(cn, map_df, common_col):\n",
    "    cn_mapped = map_df.merge(cn, on = common_col, how = \"inner\")\n",
    "    return cn_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_map_list(cohort_list, cohort_dir_list, map_raw_df, common_col, save_dir):\n",
    "    cohort_cn_map_summary = pd.DataFrame(columns = [\"annotation\"], index = cohort_list)\n",
    "    for cohort_dir_filename, cohort_i in zip(cohort_dir_list, cohort_list):\n",
    "        cohort = pd.read_csv(cohort_dir_filename, sep = \"\\t\", header = None, index_col = None)\n",
    "        cohort.rename(columns={0:'NWDID'}, inplace = True)\n",
    "        map_df = map_raw_df[[\"NWDID\", \"unique_subject_key\", \"subject_id\"]]\n",
    "        map_df.dropna(axis = 0, subset = [\"unique_subject_key\", \"subject_id\"], how = \"any\", inplace = True)\n",
    "        common_col = \"NWDID\"\n",
    "        tmp_cn_mapped = cn_map(cohort, map_df, common_col)\n",
    "        tmp_cn_mapped_filename = \"%s_subject_id_cram.tsv\"%cohort_i\n",
    "        tmp_cn_mapped_dir_filename = os.path.join(save_dir, tmp_cn_mapped_filename)\n",
    "        tmp_cn_mapped.to_csv(tmp_cn_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        cohort_cn_map_summary.loc[cohort_i, \"annotation\"] = tmp_cn_mapped.shape[0]\n",
    "    cohort_cn_map_summary.sort_index(axis = 0, inplace = True)\n",
    "    return cohort_cn_map_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, cat_col, category_list, file_prefix, save_dir):\n",
    "    for category in category_list:\n",
    "        tmp_df = df.loc[df[cat_col] == category, :]\n",
    "        tmp_filename = \"%s_%s.tsv\"%(file_prefix, category)\n",
    "        tmp_dir_filename = os.path.join(save_dir, tmp_filename)\n",
    "        tmp_df.to_csv(tmp_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_df_pair(df1, df2, common_col, df1_col, df2_col):\n",
    "    df1_df2 = df1.merge(df2, on = common_col, how = \"inner\")\n",
    "    overlap_num = df1_df2.shape[0]\n",
    "    df1_num = df1.shape[0]\n",
    "    df2_num = df2.shape[0]\n",
    "    df1_nodf2_num = df1_num - overlap_num\n",
    "    df2_nodf1_num = df2_num - overlap_num\n",
    "    if df1_col == df2_col:\n",
    "        df1_col_x = \"%s_x\"%df1_col\n",
    "        df2_col_y = \"%s_y\"%df2_col\n",
    "    if (df1[df1_col].dtypes == \"float64\" or df1[df1_col].dtypes == \"int32\") and (df2[df2_col].dtypes == \"float64\" or df2[df2_col].dtypes == \"int32\"):\n",
    "        cor = pearsonr(df1_df2[df1_col_x], df1_df2[df2_col_y])[0]\n",
    "        return df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num, cor\n",
    "    else:\n",
    "        return df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_extraction_duplicates(df, col):\n",
    "    boolean_series = df[[col]].duplicated(keep = False)\n",
    "    duplicated_df = df.loc[boolean_series, :]\n",
    "    return duplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_egfr(egfr_dir, cohort_list, header_selection):\n",
    "    egfr_list = []\n",
    "    for cohort in cohort_list:\n",
    "        egfr_dir_filename = os.path.join(egfr_dir, \"egfr_calculated_%s.tsv\"%cohort)\n",
    "        egfr_raw = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "        egfr = egfr_raw[header_selection]\n",
    "        cohort_pheno_table = \"_\".join(cohort.split(\"-\"))\n",
    "        egfr[\"cohort\"] = egfr.shape[0] * [cohort_pheno_table]\n",
    "        egfr[\"unique_subject_key\"] = egfr.shape[0] * [None]\n",
    "        egfr_num = egfr.shape[0]\n",
    "        for egfr_i in range(egfr_num):\n",
    "            egfr.loc[egfr_i, \"unique_subject_key\"] = \"%s_%s\"%(cohort_pheno_table, egfr.loc[egfr_i, \"id\"])\n",
    "        egfr.rename(columns = {\"id\":\"SUBJECT_ID\", \"age\":\"age_at_EGFRCKDEPI\", \"EGFR\":\"EGFRCKDEPI\"}, inplace = True)\n",
    "        egfr_list.append(egfr)\n",
    "        print(\"%s completed.\"%cohort)\n",
    "    egfr_full = pd.concat(egfr_list, axis = 0)\n",
    "    egfr_full.reset_index(drop = True, inplace = True)\n",
    "    return egfr_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invar(df, col):\n",
    "    check_invar_list = df[col].values.tolist()\n",
    "    if len(set(check_invar_list)) == 1:\n",
    "        invar_bool = True\n",
    "    else:\n",
    "        invar_bool = False\n",
    "    return invar_bool\n",
    "\n",
    "def check_list_invar(df, col_list):\n",
    "    invar_list = []\n",
    "    for col in col_list:\n",
    "        invar_bool = check_invar(df, col)\n",
    "        invar_list.append(invar_bool)\n",
    "    return invar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup_anno(annotation, invar_subsets, rm_header = None):\n",
    "    if rm_header == None:\n",
    "        annotation[\"aux_rm\"] = np.arange(annotation.shape[0])\n",
    "        rm_header = \"aux_rm\"\n",
    "    annotation_dup = df_extraction_duplicates(annotation, \"unique_subject_key\")\n",
    "    _, usk_list = categorize_df(annotation_dup, \"unique_subject_key\")\n",
    "    remove_header_list = []\n",
    "    for usk in usk_list:\n",
    "        tmp_annotation = annotation_dup.loc[annotation_dup.loc[:, \"unique_subject_key\"] == usk, :]\n",
    "        tmp_invar_list = check_list_invar(tmp_annotation, invar_subsets)\n",
    "        tmp_rm_list = tmp_annotation[rm_header].values.tolist()\n",
    "        if False in tmp_invar_list:\n",
    "            remove_header_list = remove_header_list + tmp_rm_list\n",
    "        else:\n",
    "            tmp_rm_num = len(tmp_rm_list)\n",
    "            selection = random.randint(0, tmp_rm_num - 1)\n",
    "            remove_header_list = remove_header_list + [tmp_rm_list[selection]]\n",
    "    annotation_unique = annotation[~annotation[rm_header].isin(remove_header_list)]\n",
    "    if rm_header == \"aux_rm\":\n",
    "        annotation_unique.drop(axis = 1, labels = [\"aux_rm\"], inplace = True)\n",
    "    return annotation_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_replace_nan(df0_col, df1_col, df0, df1):\n",
    "    df0 = df0.set_index(df0_col)\n",
    "    df1 = df1.set_index(df1_col)\n",
    "    df0_filled = df0.fillna(df1)\n",
    "    df0_filled = df0_filled.reset_index()\n",
    "    return df0_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    if x.first_valid_index() is None:\n",
    "        return None\n",
    "    else:\n",
    "        return x[x.first_valid_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation(pheno, annotation, pheno_col, annotation_col, pivot_col, how_merge, pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir):\n",
    "    pheno_mapped = annotation.merge(pheno, left_on = annotation_col[0], right_on = pheno_col[0], how = merge_how)\n",
    "    col_num = len(pheno_col[1:])\n",
    "    for col_i in range(1, col_num + 1):\n",
    "        if pheno_col[col_i] != annotation_col[col_i]:\n",
    "            pheno_mapped.drop(axis = 1, columns = [pheno_col[col_i], annotation_col[col_i]], inplace = True)\n",
    "        else:\n",
    "            tmp_pheno_col = \"%s_y\"%pheno_col[col_i]\n",
    "            tmp_annotation_col = \"%s_x\"%annotation_col[col_i]\n",
    "            pheno_mapped.drop(axis = 1, columns = [tmp_pheno_col, tmp_annotation_col], inplace = True)\n",
    "    if len(annotation_col) > 1:\n",
    "        depre_sample_list, pivot_col_df = map_deprecation_list(pheno, annotation, pheno_col, annotation_col,\n",
    "                                                               pivot_col, pheno_prefix, anno_prefix, diff_save_dir)\n",
    "        if depre_sample_list != []:\n",
    "            pheno_mapped_raw = pheno_mapped.copy()\n",
    "            del pheno_mapped\n",
    "            pheno_mapped = pheno_mapped_raw[~pheno_mapped_raw[pivot_col].isin(depre_sample_list)]\n",
    "        pheno_mapped = pheno_mapped.merge(pivot_col_df, on = pivot_col, how = \"inner\")\n",
    "    pheno_mapped_filename = \"%s_%s.tsv\"%(anno_prefix, pheno_prefix)\n",
    "    pheno_mapped_dir_filename = os.path.join(mapped_save_dir, pheno_mapped_filename)\n",
    "    pheno_mapped.to_csv(pheno_mapped_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "    return pheno_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation_list(pheno, annotation, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    pheno_annotatio_dif = annotation.merge(pheno, left_on = pheno_col[0], right_on = annotation_col[0], how = \"outer\")\n",
    "    depre_sample_list = []\n",
    "    pivot_df = pheno_annotatio_dif[[pivot_col]]\n",
    "    for pheno_col_i, annotation_col_i in zip(pheno_col[1:], annotation_col[1:]):\n",
    "        if pheno_col_i == annotation_col_i:\n",
    "            annotation_col_i = \"%s_x\"%annotation_col_i\n",
    "            pheno_col_i = \"%s_y\"%pheno_col_i\n",
    "        col_dif = pheno_annotatio_dif[[pivot_col, annotation_col_i, pheno_col_i]]\n",
    "        tmp_depre_sample_list, df_col_i = map_deprecation(col_dif, pheno_col_i, annotation_col_i,\n",
    "                                                          pivot_col, pheno_prefix, anno_prefix, save_dir)\n",
    "        pivot_df = pivot_df.merge(df_col_i, on = pivot_col, how = \"inner\")\n",
    "        depre_sample_list = depre_sample_list + tmp_depre_sample_list\n",
    "    return depre_sample_list, pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deprecation(df, pheno_col, annotation_col, pivot_col, pheno_prefix, anno_prefix, save_dir):\n",
    "    df_pivot = df[[pivot_col]]\n",
    "    df_col = df[[pheno_col, annotation_col]]\n",
    "    df_col.iloc[:, 0] = df_col.apply(func, axis = 1)\n",
    "    df_col.iloc[:, 1] = df_col.apply(func, axis = 1)\n",
    "    del df\n",
    "    df = pd.concat([df_pivot, df_col], axis = 1)\n",
    "    depre_sample_index_list = df[df.iloc[:, 0].isnull()].index.tolist()\n",
    "    if depre_sample_index_list == []:\n",
    "        depre_sample_list = []\n",
    "    else:\n",
    "        depre_sample_list = df.loc[depre_sample_index_list, pivot_col].values.reshape(1, -1).tolist()[0]\n",
    "    df.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "    tmp_compare = df[pheno_col].eq(df[annotation_col], axis = 0)\n",
    "    df_dif = df[tmp_compare == False]\n",
    "    df_cons = df[tmp_compare == True]\n",
    "    if annotation_col.split('_x')[-1] == \"\":\n",
    "        annotation_col_propagate = annotation_col.split('_x')[0]\n",
    "        df_cons.rename(columns = {annotation_col:annotation_col_propagate}, inplace = True)\n",
    "    else:\n",
    "        annotation_col_propagate = annotation_col\n",
    "    df_cons_propagate = df_cons.loc[:, [pivot_col, annotation_col_propagate]]\n",
    "    if df_dif.shape[0] != 0:\n",
    "        print(\"%s is inconsistent.\"%pheno_col)\n",
    "        df_dif_filename = \"%s_%s_%s.tsv\"%(anno_prefix, pheno_prefix, annotation_col)\n",
    "        df_dif_dir_filename = os.path.join(save_dir, df_dif_filename)\n",
    "        df_dif.to_csv(df_dif_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "        depre_sample_list = df_dif[[pivot_col]].values.reshape(1, -1).tolist()[0]\n",
    "    return depre_sample_list, df_cons_propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "1. Annotation File:\\\n",
    "(1) Including exclusion, and only keep the columns we need.\\\n",
    "(2) Check if duplication happened in the \"unique_subject_key\" and remove: a. anyone of the duplication if they are exactly the same; b. all of them if anyone has anything being different with others.\\\n",
    "(3) Removing the duplicated individuals based on the duplicates list provided by dbGaP or other organizations.\n",
    "2. Phenotype File:\\\n",
    "(1) Concatenating all phenotype file together -- including preprocessing all phenotype files and concatenating them;\\\n",
    "(2) Converting to get microcytosis and anemia;\\\n",
    "(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate DDIMER to Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddimer_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "ddimer_filename = \"DDIMER_21MAY2019_complete_useful_col.tsv\"\n",
    "ddimer_dir_filename = os.path.join(ddimer_dir, ddimer_filename)\n",
    "ddimer = pd.read_csv(ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_col = [\"NWDID\", \"sex\", \"cohort\"]\n",
    "annotation_col = [\"NWDID\", \"sex\", \"study\"]\n",
    "pheno_prefix = \"ddimer\"\n",
    "anno_prefix = \"freeze8_anno05_af02_unique02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"NWDID\"\n",
    "merge_how = \"left\"\n",
    "ddimer_mapped = map_annotation(ddimer, annotation, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                               pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get btc01-coh_adad01-noex\n",
    "Concatenate gengrp6, weight and center to btc-coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217832, 35)\n",
      "duplicated unique_subject_key 0\n",
      "duplicated unique_subject_key 0\n"
     ]
    }
   ],
   "source": [
    "btc_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_filename = \"btc01-coh04.tsv\"\n",
    "btc_dir_filename = os.path.join(btc_dir, btc_filename)\n",
    "btc = pd.read_csv(btc_dir_filename, sep = \"\\t\", header = 0, index_col = None, dtype = {\"SUBJECT_ID\":\"string\"})\n",
    "print(btc.shape)\n",
    "print(\"duplicated unique_subject_key\", duplicates_num(btc, \"unique_subject_key\"))\n",
    "\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_weight_center_filename = \"gengrp6_center_weight_noex.tsv\"\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center = pd.read_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = 0, index_col = None, dtype = {\"SUBJECT_ID\":\"string\"})\n",
    "print(\"duplicated unique_subject_key\", duplicates_num(gengrp6_weight_center, \"unique_subject_key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:1047: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Here we first check if there is confliction between btc and gengrp6_weight_center in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "annotation_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "pheno_prefix = \"adad01-noex\"\n",
    "anno_prefix = \"btc01-coh04\"\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"left\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "btc_adad = map_annotation(gengrp6_weight_center, btc, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                          pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get btc01-coh_adad01-noex_egfr\n",
    "Concatenate egfr to btc01-coh_adad01-noex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping useless columns from egfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_filename = \"egfr03_unique.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None, dtype = {\"SUBJECT_ID\":\"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfr_no_race_ethnicity = egfr.drop(columns = [\"race\", \"ethnicity\"], inplace = False)\n",
    "egfr_no_race_ethnicity_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_no_race_ethnicity_filename = \"egfr03_unique_no_race_ethnicity.tsv\"\n",
    "egfr_no_race_ethnicity_dir_filename = os.path.join(egfr_no_race_ethnicity_dir, egfr_no_race_ethnicity_filename)\n",
    "egfr_no_race_ethnicity.to_csv(egfr_no_race_ethnicity_dir_filename, sep = \"\\t\", header = True, index = False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate egfr to btc01-coh_adad01-noex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (34,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "egfr_filename = \"egfr03_unique_no_race_ethnicity.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None, dtype = {\"SUBJECT_ID\":\"string\"})\n",
    "\n",
    "btc_adad_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_adad_filename = \"btc01-coh04_adad01_noex.tsv\"\n",
    "btc_adad_dir_filename = os.path.join(btc_adad_dir, btc_adad_filename)\n",
    "btc_adad = pd.read_csv(btc_adad_dir_filename, sep = \"\\t\", header = 0, index_col = None, dtype = {\"SUBJECT_ID\":\"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:1047: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "annotation_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\"]\n",
    "pheno_prefix = \"egfr03\"\n",
    "anno_prefix = \"btc01-coh04_adad01-noex\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"outer\"\n",
    "btc_adad_egfr = map_annotation(egfr, btc_adad, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                             pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get btc03-coh_adad01-noex_egfr-ckd\n",
    "Adding CKD, microcytosis, and anemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_adad_egfr_ckd = one_condition_conversion(btc_adad_egfr, \"EGFRCKDEPI\", 60, \"CKD\")\n",
    "btc_microcytosis_adad_egfr_ckd = one_condition_conversion(btc_adad_egfr_ckd, \"mcv_entvol_rbc_1\", 80, \"microcytosis\")\n",
    "btc02_adad_egfr_ckd = two_condition_conversion(btc_microcytosis_adad_egfr_ckd, \"hemoglobin_mcnc_bld_1\", 13, 12, \"male\", \"anemia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc02_adad_egfr_ckd_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc02_adad_egfr_ckd_filename = \"btc02-coh04_adad01-noex_egfr03-ckd.tsv\"\n",
    "btc02_adad_egfr_ckd_dir_filename = os.path.join(btc02_adad_egfr_ckd_dir, btc02_adad_egfr_ckd_filename)\n",
    "btc02_adad_egfr_ckd.to_csv(btc02_adad_egfr_ckd_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Annotation and Phenotype Sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Annotation (w DDIMER) and BCT-ADAD01_eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_adad_egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "btc_adad_egfr_filename = \"btc02-coh04_adad01-noex_egfr03-ckd.tsv\"\n",
    "btc_adad_egfr_dir_filename = os.path.join(btc_adad_egfr_dir, btc_adad_egfr_filename)\n",
    "btc_adad_egfr = pd.read_csv(btc_adad_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "anno_ddimer_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "anno_ddimer_filename = \"freeze8_anno05_af02_unique02_ddimer.tsv\"\n",
    "anno_ddimer_dir_filename = os.path.join(anno_ddimer_dir, anno_ddimer_filename)\n",
    "anno_ddimer = pd.read_csv(anno_ddimer_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first check if there is confliction between btc_adad and egfr03 in the columns \"unique_subject_key\", \"cohort\", \"SUBJECT_ID\"\n",
    "pheno_col = [\"unique_subject_key\", \"SUBJECT_ID\", \"cohort\", \"male\"]\n",
    "annotation_col = [\"unique_subject_key\", \"subject_id\", \"study\", \"sex\"]\n",
    "pheno_prefix = \"btc02-coh04_ddimer-noex_egfr03-ckd_adad01-noex\"\n",
    "anno_prefix = \"freeze8_anno05_af02\"\n",
    "mapped_save_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "diff_save_dir = os.path.join(\"..\", \"data_summary\")\n",
    "pivot_col = \"unique_subject_key\"\n",
    "merge_how = \"inner\"\n",
    "anno_ddimer_btc_adad_egfr = map_annotation(btc_adad_egfr, anno_ddimer, pheno_col, annotation_col, pivot_col, merge_how,\n",
    "                                           pheno_prefix, anno_prefix, mapped_save_dir, diff_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Exclusion Strategy on Mapped Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_noex_adad_noex_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filename = \"freeze8_anno05_af02_btc02-coh04_ddimer-noex_egfr03-ckd_adad01-noex.tsv\"\n",
    "pheno_noex_adad_noex_dir_filename = os.path.join(pheno_noex_adad_noex_dir, pheno_noex_adad_noex_filename)\n",
    "pheno_noex_adad_noex = pd.read_csv(pheno_noex_adad_noex_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gengrp6: excluding samples with CONSENT_text == DROP no matter how INTERNAL_USE_ONLY is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70594, 56)\n",
      "(70594, 56)\n"
     ]
    }
   ],
   "source": [
    "print(pheno_noex_adad_noex.shape)\n",
    "pheno_noex_adad = pheno_noex_adad_noex.loc[pheno_noex_adad_noex.loc[:, \"CONSENT_text\"] != \"DROP\", :]\n",
    "print(pheno_noex_adad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDIMER: excluding samples with sample_remove_DDIMER == TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70594, 56)\n"
     ]
    }
   ],
   "source": [
    "pheno_adad = pheno_noex_adad.loc[pheno_noex_adad.loc[:, \"sample_remove_DDIMER\"] != 1, :]\n",
    "print(pheno_adad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_adad_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_adad_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01.tsv\"\n",
    "pheno_adad_dir_filename = os.path.join(pheno_adad_dir, pheno_adad_filename)\n",
    "pheno_adad.to_csv(pheno_adad_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Race and Ethnicity to Annotation & Phenotype File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_uniform_race_ethnicity(df, cohort_col, cohort, race, ethnicity, race_col, ethnicity_col):\n",
    "    df.loc[df.loc[:, cohort_col] == cohort, race_col] = race\n",
    "    df.loc[df.loc[:, cohort_col] == cohort, ethnicity_col] = ethnicity\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_pheno_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "anno_pheno_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01.tsv\"\n",
    "anno_pheno_dir_filename = os.path.join(anno_pheno_dir, anno_pheno_filename)\n",
    "anno_pheno = pd.read_csv(anno_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_dict = {\"hispanic\":0, \"white\":1, \"black\":2, \"asian\":3, \"AI_AN\":4, \"HI_PI\":5, \"other\":6, \"multiple\":7}\n",
    "ethnicity_dict = {\"non-hispanic\":0, \"hispanic\":1}\n",
    "race_col = \"race\"\n",
    "ethnicity_col = \"ethnicity\"\n",
    "anno_pheno[race_col] = np.nan\n",
    "anno_pheno[ethnicity_col] = np.nan\n",
    "cohort_race_ethnicity_dict = {\"FHS\":{\"race\":\"white\", \"ethnicity\":\"non-hispanic\"}, \"Amish\":{\"race\":\"white\", \"ethnicity\":\"non-hispanic\"},\n",
    "                              \"GenSalt\":{\"race\":\"asian\", \"ethnicity\":\"non-hispanic\"}, \"HyperGEN\":{\"race\":\"black\", \"ethnicity\":\"non-hispanic\"},\n",
    "                              \"DHS\":{\"race\":\"black\", \"ethnicity\":\"non-hispanic\"}, \"GENOA\":{\"race\":\"black\", \"ethnicity\":\"non-hispanic\"},\n",
    "                              \"JHS\":{\"race\":\"black\", \"ethnicity\":\"non-hispanic\"}, \"SAFS\":{\"race\":\"hispanic\", \"ethnicity\":\"hispanic\"}\n",
    "                             }\n",
    "cohort_col = \"study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FHS\n",
      "Amish\n",
      "GenSalt\n",
      "HyperGEN\n",
      "DHS\n",
      "GENOA\n",
      "JHS\n",
      "SAFS\n"
     ]
    }
   ],
   "source": [
    "for cohort in cohort_race_ethnicity_dict:\n",
    "    race_ethnicity_dict = cohort_race_ethnicity_dict[cohort]\n",
    "    race_name = race_ethnicity_dict[race_col]\n",
    "    ethnicity_name = race_ethnicity_dict[ethnicity_col]\n",
    "    race = race_dict[race_name]\n",
    "    ethnicity = ethnicity_dict[ethnicity_name]\n",
    "    anno_pheno = assign_uniform_race_ethnicity(anno_pheno, cohort_col, cohort, race, ethnicity, race_col, ethnicity_col)\n",
    "    print(cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethnicity_dir = os.path.join(\"..\", \"raw_data\", \"race_ethnicity\")\n",
    "race_ethnicity_filename = \"race_ethnicity.tsv\"\n",
    "race_ethnicity_dir_filename = os.path.join(race_ethnicity_dir, race_ethnicity_filename)\n",
    "race_ethnicity = pd.read_csv(race_ethnicity_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = anno_pheno.reset_index(drop = True)\n",
    "df2 = race_ethnicity\n",
    "common_col = [\"unique_subject_key\"]\n",
    "fillin_col_raw = list(df2)\n",
    "fillin_col = [x for x in fillin_col_raw if x not in common_col]\n",
    "df1_fillin = df1.combine_first(df1.drop(columns = fillin_col).merge(df2, how = 'left', on = common_col))\n",
    "df1_fillin_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "df1_fillin_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity.tsv\"\n",
    "df1_fillin_dir_filename = os.path.join(df1_fillin_dir, df1_fillin_filename)\n",
    "df1_fillin.to_csv(df1_fillin_dir_filename, sep = \"\\t\", header = True, index = False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select AA and HL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_pheno_re_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "anno_pheno_re_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity.tsv\"\n",
    "anno_pheno_re_dir_filename = os.path.join(anno_pheno_re_dir, anno_pheno_re_filename)\n",
    "anno_pheno_re = pd.read_csv(anno_pheno_re_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_pheno_re_aa = anno_pheno_re.copy()\n",
    "anno_pheno_re_aa[\"AA\"] = np.nan\n",
    "anno_pheno_re_aa.loc[anno_pheno_re_aa.loc[:, \"race\"] == 2, \"AA\"] = 1\n",
    "anno_pheno_re_aa.loc[anno_pheno_re_aa.loc[:, \"race\"] != 2, \"AA\"] = 0\n",
    "anno_pheno_re_aa[\"AA_ethnicity\"] = anno_pheno_re_aa[\"AA\"] + anno_pheno_re_aa[\"ethnicity\"]\n",
    "# anno_pheno_re_aa_select = anno_pheno_re_aa.loc[anno_pheno_re_aa.loc[:, \"AA_ethnicity\"] != 0, :]\n",
    "anno_pheno_re_aa_select_1 = anno_pheno_re_aa.loc[anno_pheno_re_aa.loc[:, \"AA_ethnicity\"] == 1, :]\n",
    "anno_pheno_re_aa_select_2 = anno_pheno_re_aa.loc[anno_pheno_re_aa.loc[:, \"AA_ethnicity\"] == 2, :]\n",
    "anno_pheno_re_aa_select = pd.concat([anno_pheno_re_aa_select_1, anno_pheno_re_aa_select_2], axis = 0)\n",
    "# display(anno_pheno_re_aa_select)\n",
    "anno_pheno_re_aa_select.drop(columns = [\"race\", \"AA_ethnicity\"], inplace = True)\n",
    "anno_pheno_re_aa_select_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "anno_pheno_re_aa_select_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity_0-2.tsv\"\n",
    "anno_pheno_re_aa_select_dir_filename = os.path.join(anno_pheno_re_aa_select_dir, anno_pheno_re_aa_select_filename)\n",
    "anno_pheno_re_aa_select.to_csv(anno_pheno_re_aa_select_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CN Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CN of Each Cohort Based on Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity_0-2.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "_, pheno_cohort_list = categorize_df(pheno, \"study\")\n",
    "\n",
    "full_cn_dir = os.path.join(\"..\", \"prepro_data\", \"cn\")\n",
    "full_cn_filename = \"alpha_globin_calls_pass_useful.tsv\"\n",
    "full_cn_dir_filename = os.path.join(full_cn_dir, full_cn_filename)\n",
    "full_cn = pd.read_csv(full_cn_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHI (1447, 2)\n",
      "COPDGene (1862, 2)\n",
      "BioMe (5448, 2)\n",
      "JHS (3099, 2)\n",
      "MESA (2106, 2)\n",
      "ARIC (365, 2)\n",
      "SAFS (1107, 2)\n",
      "HyperGEN (1786, 2)\n",
      "GENOA (1058, 2)\n",
      "CHS (690, 2)\n",
      "DHS (372, 2)\n",
      "GeneSTAR (692, 2)\n",
      "CARDIA (1354, 2)\n",
      "HCHS_SOL (3847, 2)\n"
     ]
    }
   ],
   "source": [
    "for cohort in pheno_cohort_list:\n",
    "    pheno_cohort_raw = pheno.loc[pheno.loc[:, \"study\"] == cohort, :]\n",
    "    pheno_id = pheno_cohort_raw[[\"NWDID\"]]\n",
    "    cn = full_cn.merge(pheno_id, on = \"NWDID\", how = \"inner\")\n",
    "    \n",
    "    full_cn_id = full_cn[[\"NWDID\"]]\n",
    "    pheno_cohort = full_cn_id.merge(pheno_cohort_raw, on = \"NWDID\", how = \"inner\")\n",
    "    print(cohort, cn.shape)\n",
    "    cn_filename = \"cn_%s.tsv\"%cohort\n",
    "    cn_dir = os.path.join(\"..\", \"cn\", cohort, \"pre_data\")\n",
    "    cn_dir_filename = os.path.join(cn_dir, cn_filename)\n",
    "    if not os.path.isdir(cn_dir):\n",
    "        os.makedirs(cn_dir, exist_ok = True) \n",
    "    cn.to_csv(cn_dir_filename, \"\\t\", header = True, index = False)\n",
    "    \n",
    "    pheno_cohort_dir = os.path.join(\"..\", \"cn\", cohort, \"pre_data\")\n",
    "    pheno_cohort_filename = f\"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity_0-2_{cohort}.tsv\"\n",
    "    pheno_cohort_dir_filename = os.path.join(pheno_cohort_dir, pheno_cohort_filename)\n",
    "    if not os.path.isdir(pheno_cohort_dir):\n",
    "        os.makedirs(pheno_cohort_dir, exist_ok = True)\n",
    "    pheno_cohort.to_csv(pheno_cohort_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B. Preprocess weight, center and gengrp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 6)\n",
      "(11829, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOL</td>\n",
       "      <td>11829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cohort  sample_size\n",
       "0    SOL        11829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gengrp6_filename = \"page-harmonized-phenotypes-pca-freeze2-candidate2-2016-12-14.GWASid_fid_22May2018internalPCs.SOLv2consent.txt\"\n",
    "gengrp6_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\", \"gengrp6\")\n",
    "gengrp6_dir_filename = os.path.join(gengrp6_dir, gengrp6_filename)\n",
    "gengrp6 = pd.read_csv(gengrp6_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "gengrp6.replace(\".\", np.nan, inplace=True)\n",
    "gengrp6_select = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\",\n",
    "                          \"gengrp6\", \"study\"]].dropna(axis=0, subset=[\"analysis_id\",\"gengrp6\"],how=\"any\")\n",
    "print(gengrp6_select.shape)\n",
    "\n",
    "gengrp6_select_dropna = gengrp6[[\"z_sol_id\", \"analysis_id\", \"CONSENT_text\", \"INTERNAL_USE_ONLY\", \"gengrp6\", \"study\"]].dropna(axis=0, how=\"any\")\n",
    "gengrp6_select_dropna.rename(columns = {\"analysis_id\":\"SUBJECT_ID\", \"study\":\"cohort\"}, inplace=True)\n",
    "print(gengrp6_select_dropna.shape)\n",
    "cat_col = \"cohort\"\n",
    "gengrp6_category_summary, category_list = categorize_df(gengrp6_select_dropna, cat_col)\n",
    "display(gengrp6_category_summary)\n",
    "gengrp6_select_dropna[\"cohort\"] = gengrp6_select_dropna.shape[0] * [\"HCHS_SOL\"]\n",
    "gengrp6_select_dropna.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gengrp6_select_dropna_num = gengrp6_select_dropna.shape[0]\n",
    "gengrp6_select_dropna[\"unique_subject_key\"] = gengrp6_select_dropna_num * [None]\n",
    "for gengrp6_select_dropna_i in range(gengrp6_select_dropna_num):\n",
    "    gengrp6_select_dropna.loc[gengrp6_select_dropna_i, \"unique_subject_key\"] = \"%s_%s\"%(\"HCHS_SOL\", gengrp6_select_dropna.loc[gengrp6_select_dropna_i, \"SUBJECT_ID\"])\n",
    "#    if (gengrp6_select_dropna_i + 1) % 100 == 0:\n",
    "#        print(\"%d/%d\"%(gengrp6_select_dropna_i + 1, gengrp6_select_dropna_num))\n",
    "gengrp6_select_dropna_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_select_dropna_filename = \"gengrp6_sol_noexclude.tsv\"\n",
    "gengrp6_select_dropna_dir_filename = os.path.join(gengrp6_select_dropna_dir, gengrp6_select_dropna_filename)\n",
    "gengrp6_select_dropna.to_csv(gengrp6_select_dropna_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight and center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16415, 3)\n"
     ]
    }
   ],
   "source": [
    "weight_center_dir = os.path.join(\"..\", \"raw_data\", \"adjustment\")\n",
    "weight_center_filename = \"bloodcell_output.csv\"\n",
    "weight_center_dir_filename = os.path.join(weight_center_dir, weight_center_filename)\n",
    "weight_center = pd.read_csv(weight_center_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "weight_center_select = weight_center[[\"ID\", \"WEIGHT_FINAL_NORM_OVERALL\", \"CENTER\"]].dropna(axis = 0, how = \"any\")\n",
    "weight_center_select.rename(columns = {\"ID\":\"z_sol_id\"}, inplace = True)\n",
    "print(weight_center_select.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11829, 9)\n"
     ]
    }
   ],
   "source": [
    "gengrp6_select_dropna[\"z_sol_id\"] = gengrp6_select_dropna[\"z_sol_id\"].astype(str)\n",
    "weight_center_select[\"z_sol_id\"] = weight_center_select[\"z_sol_id\"].astype(str)\n",
    "gengrp6_weight_center = gengrp6_select_dropna.merge(weight_center_select, on = \"z_sol_id\", how = \"outer\")\n",
    "gengrp6_weight_center.dropna(axis=0, subset=[\"SUBJECT_ID\"],how=\"any\", inplace = True)\n",
    "print(gengrp6_weight_center.shape)\n",
    "gengrp6_weight_center_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "gengrp6_weight_center_filename = \"gengrp6_center_weight_noex.tsv\"\n",
    "gengrp6_weight_center_dir_filename = os.path.join(gengrp6_weight_center_dir, gengrp6_weight_center_filename)\n",
    "gengrp6_weight_center.to_csv(gengrp6_weight_center_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C. Preprocess eGFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ARIC</td>\n",
       "      <td>3555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amish</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FHS</td>\n",
       "      <td>3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENOA</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GenSalt</td>\n",
       "      <td>1666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GeneSTAR</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HyperGEN</td>\n",
       "      <td>1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JHS</td>\n",
       "      <td>3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MESA</td>\n",
       "      <td>4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHI</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort  sample_size\n",
       "8      ARIC         3555\n",
       "9     Amish         1023\n",
       "0       FHS         3087\n",
       "5     GENOA         1023\n",
       "3   GenSalt         1666\n",
       "7  GeneSTAR          190\n",
       "2  HyperGEN         1759\n",
       "4       JHS         3115\n",
       "6      MESA         4090\n",
       "1       WHI         4224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egfr_old_filename = \"data_all_2_eGFR_Jan2019.csv\"\n",
    "egfr_old_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\")\n",
    "egfr_old_dir_filename = os.path.join(egfr_old_dir, egfr_old_filename)\n",
    "egfr_old = pd.read_csv(egfr_old_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "cat_col = \"Study\"\n",
    "egfr_old_summary, category_list = categorize_df(egfr_old, cat_col)\n",
    "egfr_old_summary_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_old_summary_filename = \"data_all_2_eGFR_Jan2019_category_summary.tsv\"\n",
    "egfr_old_summary_dir_filename = os.path.join(egfr_old_summary_dir, egfr_old_summary_filename)\n",
    "egfr_old_summary.to_csv(egfr_old_summary_dir_filename, sep = \"\\t\", header = True, index = False)\n",
    "display(egfr_old_summary)\n",
    "save_dir = egfr_old_dir\n",
    "file_prefix = \"eGFR_Jan2019\"\n",
    "split_df(egfr_old, cat_col, category_list, file_prefix, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort AMISH completed.\n",
      "Cohort ARIC completed.\n",
      "Cohort FHS completed.\n",
      "Cohort GENOA completed.\n",
      "Cohort GenSalt completed.\n",
      "Cohort GeneSTAR completed.\n",
      "Cohort HyperGEN completed.\n",
      "Cohort JHS completed.\n",
      "Cohort MESA completed.\n",
      "Cohort WHI completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># old not in new</th>\n",
       "      <th># new not in old</th>\n",
       "      <th># overlap</th>\n",
       "      <th># old</th>\n",
       "      <th># new</th>\n",
       "      <th>cor</th>\n",
       "      <th># dup old</th>\n",
       "      <th># dup new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AMISH</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>1118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIC</td>\n",
       "      <td>0</td>\n",
       "      <td>11782</td>\n",
       "      <td>3555</td>\n",
       "      <td>3555</td>\n",
       "      <td>15337</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FHS</td>\n",
       "      <td>-52</td>\n",
       "      <td>348</td>\n",
       "      <td>3139</td>\n",
       "      <td>3087</td>\n",
       "      <td>3487</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GENOA</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GenSalt</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1666</td>\n",
       "      <td>1666</td>\n",
       "      <td>1846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GeneSTAR</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HyperGEN</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1759</td>\n",
       "      <td>1759</td>\n",
       "      <td>1884</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JHS</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3390</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MESA</td>\n",
       "      <td>0</td>\n",
       "      <td>2323</td>\n",
       "      <td>4090</td>\n",
       "      <td>4090</td>\n",
       "      <td>6413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WHI</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>4224</td>\n",
       "      <td>4224</td>\n",
       "      <td>4692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # old not in new # new not in old # overlap # old  # new cor  \\\n",
       "AMISH                   0               95      1023  1023   1118   1   \n",
       "ARIC                    0            11782      3555  3555  15337   1   \n",
       "FHS                   -52              348      3139  3087   3487   1   \n",
       "GENOA                   0              560      1023  1023   1583   1   \n",
       "GenSalt                 0              180      1666  1666   1846   1   \n",
       "GeneSTAR                0               16       190   190    206   1   \n",
       "HyperGEN                0              125      1759  1759   1884   1   \n",
       "JHS                     0              275      3115  3115   3390   1   \n",
       "MESA                    0             2323      4090  4090   6413   1   \n",
       "WHI                     0              468      4224  4224   4692   1   \n",
       "\n",
       "         # dup old # dup new  \n",
       "AMISH            0         0  \n",
       "ARIC             0         0  \n",
       "FHS              0        58  \n",
       "GENOA            0         0  \n",
       "GenSalt          0         0  \n",
       "GeneSTAR         0         0  \n",
       "HyperGEN         0         0  \n",
       "JHS              0         0  \n",
       "MESA             0         0  \n",
       "WHI              0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "egfr_old_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "egfr_new_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "cohort_list = [\"AMISH\", \"ARIC\", \"FHS\", \"GENOA\", \"GenSalt\", \"GeneSTAR\", \"HyperGEN\", \"JHS\", \"MESA\", \"WHI\"]\n",
    "compare_egfr_df = pd.DataFrame(columns = [\"# old not in new\", \"# new not in old\", \"# overlap\", \"# old\", \"# new\", \"cor\", \"# dup old\", \"# dup new\"], index = cohort_list)\n",
    "for cohort in cohort_list:\n",
    "    egfr_old_dir_filename = os.path.join(egfr_old_dir, \"eGFR_Jan2019_%s.tsv\"%cohort)\n",
    "    egfr_new_dir_filename = os.path.join(egfr_new_dir, \"egfr_calculated_%s.tsv\"%cohort)\n",
    "    egfr_old = pd.read_csv(egfr_old_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "    common_col = \"id\"\n",
    "    egfr_old_col = \"EGFR\"\n",
    "    egfr_new_col = \"EGFR\"\n",
    "    egfr_old[egfr_old_col] = egfr_old[egfr_old_col].astype(float)\n",
    "    egfr_new[egfr_new_col] = egfr_new[egfr_new_col].astype(float)\n",
    "    df1_nodf2_num, df2_nodf1_num, overlap_num, df1_num, df2_num, cor = compare_df_pair(egfr_old, egfr_new, common_col, egfr_old_col, egfr_new_col)\n",
    "    compare_egfr_df.loc[cohort, \"# old not in new\"] = df1_nodf2_num\n",
    "    compare_egfr_df.loc[cohort, \"# new not in old\"] = df2_nodf1_num\n",
    "    compare_egfr_df.loc[cohort, \"# overlap\"] = overlap_num\n",
    "    compare_egfr_df.loc[cohort, \"# old\"] = df1_num\n",
    "    compare_egfr_df.loc[cohort, \"# new\"] = df2_num\n",
    "    compare_egfr_df.loc[cohort, \"cor\"] = cor\n",
    "    compare_egfr_df.loc[cohort, \"# dup old\"] = duplicates_num(egfr_old, common_col)\n",
    "    compare_egfr_df.loc[cohort, \"# dup new\"] = duplicates_num(egfr_new, common_col)\n",
    "    print(\"Cohort %s completed.\"%cohort)\n",
    "display(compare_egfr_df)\n",
    "compare_egfr_dir = os.path.join(\"..\", \"data_summary\")\n",
    "compare_egfr_filename = \"compare_egft_calculated_Jan2019_summary_nodup.tsv\"\n",
    "compare_egfr_dir_filename = os.path.join(compare_egfr_dir, compare_egfr_filename)\n",
    "compare_egfr_df.to_csv(compare_egfr_dir_filename, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(Deprecated)** Replace the space elements in the MESA egfr table as NAN and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6429, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6413, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_MESA_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr_new.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "display(egfr_new.shape)\n",
    "egfr_new.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "display(egfr_new.shape)\n",
    "egfr_new_mod_dir_filename = os.path.join(load_dir, \"egfr_calculated_MESA.tsv\")\n",
    "egfr_new.to_csv(egfr_new_mod_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESA: Correlation between cepgfr1c and eGFR I calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993978212689216 0.9952502226206945\n",
      "3.394391478720293\n",
      "33.29065562220411\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "freeze5_egfr_filename = \"eGFR_Jan2019_MESA.tsv\"\n",
    "freeze5_egfr_dir_filename = os.path.join(freeze5_egfr_dir, freeze5_egfr_filename)\n",
    "freeze5_egfr_df = pd.read_csv(freeze5_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "freeze5_egfr = freeze5_egfr_df[\"EGFR\"].values\n",
    "\n",
    "cepgfr1c_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "cepgfr1c_filename = \"TOPMed_kidney_phenotype_MESA_exam1.tsv\"\n",
    "cepgfr1c_dir_filename = os.path.join(cepgfr1c_dir, cepgfr1c_filename)\n",
    "cepgfr1c_df = pd.read_csv(cepgfr1c_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cepgfr1c = cepgfr1c_df[\"EGFR\"].values\n",
    "\n",
    "calegfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "calegfr_filename = \"TOPMed_MESA_RenalPhenotypes_16March2018_egfr.tsv\"\n",
    "calegfr_dir_filename = os.path.join(calegfr_dir, calegfr_filename)\n",
    "calegfr_df = pd.read_csv(calegfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "calegfr = calegfr_df[\"EGFR\"].values\n",
    "\n",
    "calegfr_mod_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "calegfr_mod_filename = \"TOPMed_MESA_RenalPhenotypes_16March2018_egfr_mod.tsv\"\n",
    "calegfr_mod_dir_filename = os.path.join(calegfr_mod_dir, calegfr_mod_filename)\n",
    "calegfr_mod_df = pd.read_csv(calegfr_mod_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "calegfr_mod = calegfr_mod_df[\"EGFR\"].values\n",
    "\n",
    "cor_cepgfr1c_calegfr = pearsonr(cepgfr1c, calegfr)[0]\n",
    "cor_cepgfr1c_calegfr_mod = pearsonr(cepgfr1c, calegfr_mod)[0]\n",
    "print(cor_cepgfr1c_calegfr, cor_cepgfr1c_calegfr_mod)\n",
    "\n",
    "mse_cepgfr1c_calegfr = mean_squared_error(cepgfr1c, calegfr)\n",
    "mse_cepgfr1c_calegfr_mod = mean_squared_error(cepgfr1c, calegfr_mod)\n",
    "print(mse_cepgfr1c_calegfr)\n",
    "print(mse_cepgfr1c_calegfr_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959972669338772 0.9999999999999826\n",
      "17.445292821842674\n",
      "8.525105559709774e-12\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_df.rename(columns = {\"EGFR\":\"EGFR_freeze5\"}, inplace = True)\n",
    "calegfr_df.rename(columns = {\"EGFR\":\"EGFR_calegfr\"}, inplace = True)\n",
    "calegfr_mod_df.rename(columns = {\"EGFR\":\"EGFR_calegfr_mod\"}, inplace = True)\n",
    "egfr_list = [freeze5_egfr_df, calegfr_df, calegfr_mod_df]\n",
    "merge_egfr = merge_df_list(egfr_list, \"id\", merge_method='merge', how = 'inner')\n",
    "common_calegfr = merge_egfr[\"EGFR_calegfr\"].values\n",
    "common_freeze5_egfr = merge_egfr[\"EGFR_freeze5\"].values\n",
    "common_calegfr_mod = merge_egfr[\"EGFR_calegfr_mod\"].values\n",
    "\n",
    "cor_common_freeze5_calegfr = pearsonr(common_freeze5_egfr, common_calegfr)[0]\n",
    "cor_common_freeze5_calegfr_mod = pearsonr(common_freeze5_egfr, common_calegfr_mod)[0]\n",
    "print(cor_common_freeze5_calegfr, cor_common_freeze5_calegfr_mod)\n",
    "\n",
    "mse_common_freeze5_calegfr = mean_squared_error(common_freeze5_egfr, common_calegfr)\n",
    "mse_common_freeze5_calegfr_mod = mean_squared_error(common_freeze5_egfr, common_calegfr_mod)\n",
    "print(mse_common_freeze5_calegfr)\n",
    "print(mse_common_freeze5_calegfr_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949210599819995\n",
      "33.930496703190656\n"
     ]
    }
   ],
   "source": [
    "freeze5_egfr_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"egfr_freeze5\")\n",
    "freeze5_egfr_filename = \"eGFR_Jan2019_MESA.tsv\"\n",
    "freeze5_egfr_dir_filename = os.path.join(freeze5_egfr_dir, freeze5_egfr_filename)\n",
    "freeze5_egfr_df = pd.read_csv(freeze5_egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "cepgfr1c_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "cepgfr1c_filename = \"TOPMed_kidney_phenotype_MESA_exam1.tsv\"\n",
    "cepgfr1c_dir_filename = os.path.join(cepgfr1c_dir, cepgfr1c_filename)\n",
    "cepgfr1c_df = pd.read_csv(cepgfr1c_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "freeze5_egfr_df.rename(columns = {\"EGFR\":\"EGFR_freeze5\"}, inplace = True)\n",
    "cepgfr1c_df.rename(columns = {\"EGFR\":\"EGFR_cepgfr1c\"}, inplace = True)\n",
    "freeze5_cepgfr1c = freeze5_egfr_df.merge(cepgfr1c_df, left_on = \"id\", right_on = \"sidno\", how = \"inner\")\n",
    "common_freeze5_egfr = freeze5_cepgfr1c[\"EGFR_freeze5\"].values\n",
    "common_cepgfr1c = freeze5_cepgfr1c[\"EGFR_cepgfr1c\"].values\n",
    "\n",
    "cor_common_freeze5_cepgfr1c = pearsonr(common_freeze5_egfr, common_cepgfr1c)[0]\n",
    "print(cor_common_freeze5_cepgfr1c)\n",
    "\n",
    "mse_common_freeze5_cepgfr1c = mean_squared_error(common_freeze5_egfr, common_cepgfr1c)\n",
    "print(mse_common_freeze5_cepgfr1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process DHS Exsited eGFR Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"dhs\")\n",
    "dhs_filename = \"phs001412_AF_20190705_nda.csv\"\n",
    "dhs_dir_filename = os.path.join(dhs_dir, dhs_filename)\n",
    "dhs = pd.read_csv(dhs_dir_filename, sep = \",\", header = 0, index_col = None)\n",
    "dhs.dropna(axis = 0, subset = [\"CKD_eGFR\", \"Subject_ID\", \"Study\"], inplace = True, how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check race composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'Black'}\n"
     ]
    }
   ],
   "source": [
    "race_set = set(dhs[\"Race\"].values.tolist())\n",
    "print(len(race_set), race_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs.rename(columns = {\"Subject_ID\":\"id\", \"Sex\":\"male\", \"Age\":\"age\", \"Race\":\"race\", \"CKD_eGFR\":\"EGFR\" }, inplace = True)\n",
    "race_dict = {\"Black\":2}\n",
    "male_dict = {\"F\":0, \"M\":1}\n",
    "dhs.replace({\"male\":male_dict, \"race\":race_dict}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_egfr_sample_num = dhs.shape[0]\n",
    "dhs_egfr_col_list = [\"id\", \"scr\", \"race\", \"age\", \"male\", \"EGFR\", \"case.control\"]\n",
    "dhs_egfr = pd.DataFrame(data = np.zeros((dhs_egfr_sample_num, len(dhs_egfr_col_list))), columns = dhs_egfr_col_list)\n",
    "dhs_egfr[\"id\"] = dhs[\"id\"]\n",
    "dhs_egfr[\"race\"] = dhs[\"race\"]\n",
    "dhs_egfr[\"age\"] = dhs[\"age\"]\n",
    "dhs_egfr[\"male\"] = dhs[\"male\"]\n",
    "dhs_egfr[\"EGFR\"] = dhs[\"EGFR\"]\n",
    "dhs_egfr_dir = os.path.join(\"..\", \"raw_data\", \"egfr\")\n",
    "dhs_egfr_filename = \"TopMed_Kidney_Phenotype_DHS_egfr.tsv\"\n",
    "dhs_egfr_dir_filename = os.path.join(dhs_egfr_dir, dhs_egfr_filename)\n",
    "dhs_egfr.to_csv(dhs_egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving FHS # old not in new negative issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "116\n",
      "Total number of all duplicates are two times of number of samples with duplicates, so each sample with duplicate having only one duplicate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "dup_num = duplicates_num(egfr_new, \"id\")\n",
    "print(dup_num)\n",
    "boolean_series = egfr_new[[\"id\"]].duplicated(keep=False)\n",
    "egfr_new_dup = egfr_new.loc[boolean_series, :]\n",
    "egfr_new_dup.sort_values(axis = 0, by = \"id\", inplace = True)\n",
    "#display(egfr_new_dup)\n",
    "duplicates_list = egfr_new_dup[\"id\"].values.tolist()\n",
    "print(len(duplicates_list))\n",
    "print(\"Total number of all duplicates are two times of number of samples with duplicates, so each sample with duplicate having only one duplicate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to check if egfr of each pair of duplicates are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "All egfr values of pairs of duplicates are identical, so I will drop any of them in each pair.\n"
     ]
    }
   ],
   "source": [
    "egfr_list = egfr_new_dup[\"EGFR\"].values.tolist()\n",
    "duplicate_egfr_diff_list = []\n",
    "egfr_num = len(egfr_list)\n",
    "for egfr_i in range(0, egfr_num, 2):\n",
    "    duplicate_egfr_diff = egfr_list[egfr_i + 1] - egfr_list[egfr_i]\n",
    "    duplicate_egfr_diff_list.append(duplicate_egfr_diff)\n",
    "print(sum(duplicate_egfr_diff_list))\n",
    "print(\"All egfr values of pairs of duplicates are identical, so I will drop any of them in each pair.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_new_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS_raw.tsv\")\n",
    "egfr_new = pd.read_csv(egfr_new_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr_new.drop_duplicates(subset = \"id\", keep = \"first\", inplace = True)\n",
    "egfr_new_nodup_dir_filename = os.path.join(load_dir, \"egfr_calculated_FHS.tsv\")\n",
    "egfr_new.to_csv(egfr_new_nodup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess CHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHS SCr file did not include sex info which is needed for calculating CKS-eGFR. So I map the individual ID to the annotation file to get the sex info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_filename = \"freeze8_anno05_af02_unique02.tsv\"\n",
    "annotation_dir = os.path.join(\"..\", \"raw_data\", \"annotation\")\n",
    "annotation_dir_filename = os.path.join(annotation_dir, annotation_filename)\n",
    "annotation = pd.read_csv(annotation_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "chs_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"chs\")\n",
    "chs_filename = \"TOPMed_kidney_phenotype_CHS.txt\"\n",
    "chs_dir_filename = os.path.join(chs_dir, chs_filename)\n",
    "chs = pd.read_csv(chs_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "chs['ID'] = chs['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "subject_id_sex = annotation[[\"subject_id\", \"sex\"]]\n",
    "subject_id_sex.rename(columns = {\"subject_id\":\"ID\", \"sex\":\"MALE\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs_sex = chs.merge(subject_id_sex, on = \"ID\", how = \"inner\")\n",
    "chs_sex_filename = \"TOPMed_kidney_phenotype_CHS_sex.txt\"\n",
    "chs_sex_dir = chs_dir\n",
    "chs_sex_dir_filename = os.path.join(chs_sex_dir, chs_sex_filename)\n",
    "chs_sex.to_csv(chs_sex_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess CARDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_dir = os.path.join(\"..\", \"raw_data\", \"kidney_pheno\", \"cardia\")\n",
    "cardia_dcc_filename = \"cardia_dcc_demographic_v3.txt\"\n",
    "cardia_scr_filename = \"cardia_scr_exam6.txt\"\n",
    "cardia_race_age_filename = \"cardia_race_age_exam6.txt\"\n",
    "\n",
    "cardia_dcc_dir_filename = os.path.join(cardia_dir, cardia_dcc_filename)\n",
    "cardia_scr_dir_filename = os.path.join(cardia_dir, cardia_scr_filename)\n",
    "cardia_race_age_dir_filename = os.path.join(cardia_dir, cardia_race_age_filename)\n",
    "\n",
    "cardia_dcc = pd.read_csv(cardia_dcc_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cardia_scr = pd.read_csv(cardia_scr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "cardia_race_age = pd.read_csv(cardia_race_age_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_dcc_scr_race_age_list = [cardia_dcc, cardia_scr, cardia_race_age]\n",
    "common_col = \"ID\"\n",
    "cardia = merge_df_list(cardia_dcc_scr_race_age_list, common_col, merge_method='merge', how = 'inner')\n",
    "\n",
    "cardia.loc[cardia.loc[:, \"MALE\"] == \"female\", \"MALE\"] = 0\n",
    "cardia.loc[cardia.loc[:, \"MALE\"] == \"male\", \"MALE\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardia_filename = \"cardia_exam6.txt\"\n",
    "cardia_dir_filename = os.path.join(cardia_dir, cardia_filename)\n",
    "cardia.to_csv(cardia_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating All eGFR Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMISH completed.\n",
      "ARIC completed.\n",
      "CARDIA completed.\n",
      "CHS completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DHS completed.\n",
      "FHS completed.\n",
      "GENOA completed.\n",
      "GenSalt completed.\n",
      "GeneSTAR completed.\n",
      "JHS completed.\n",
      "MESA completed.\n",
      "WHI completed.\n",
      "HCHS-SOL completed.\n",
      "HyperGEN completed.\n"
     ]
    }
   ],
   "source": [
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_calculated\")\n",
    "cohort_list = [\"AMISH\", \"ARIC\", \"CARDIA\", \"CHS\", \"DHS\", \"FHS\", \"GENOA\", \"GenSalt\", \"GeneSTAR\", \"JHS\", \"MESA\", \"WHI\", \"HCHS-SOL\", \"HyperGEN\"]\n",
    "header_selection = [\"id\", \"age\", \"race\", \"male\", \"ethnicity\", \"EGFR\"]\n",
    "egfr_full = concat_egfr(egfr_dir, cohort_list, header_selection)\n",
    "egfr_full_filename = \"egfr03-01.tsv\"\n",
    "egfr_full_dir_filename = os.path.join(egfr_dir, egfr_full_filename)\n",
    "egfr_full.to_csv(egfr_full_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing and Recording Duplicates in Comprehensive eGFR Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 8)\n"
     ]
    }
   ],
   "source": [
    "egfr_dup = df_extraction_duplicates(egfr_full, \"unique_subject_key\")\n",
    "print(egfr_dup.shape)\n",
    "egfr_dup = egfr_dup.sort_values(by=['unique_subject_key'])\n",
    "egfr_dup_dir = os.path.join(\"..\", \"data_summary\")\n",
    "egfr_dup_filename = \"egfr03-01_dup.tsv\"\n",
    "egfr_dup_dir_filename = os.path.join(egfr_dup_dir, egfr_dup_filename)\n",
    "egfr_dup.to_csv(egfr_dup_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/minzhi/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "invar_subsets = [\"race\", \"male\", \"cohort\"]\n",
    "egfr_unique = remove_dup_anno(egfr_full, invar_subsets)\n",
    "egfr_unique_filename = \"egfr03-01_unique.tsv\"\n",
    "egfr_unique_dir_filename = os.path.join(egfr_dir, egfr_unique_filename)\n",
    "egfr_unique.to_csv(egfr_unique_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating eGFR to phenotype table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223838, 37)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_filename = \"coh03_pre.tsv\"\n",
    "pheno_dir_filename = os.path.join(pheno_dir, pheno_filename)\n",
    "pheno = pd.read_csv(pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "pheno[\"SUBJECT_ID\"] = pheno[\"SUBJECT_ID\"].astype(str)\n",
    "\n",
    "egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\", \"egfr_useful\")\n",
    "egfr_filename = \"egfr01.tsv\"\n",
    "egfr_dir_filename = os.path.join(egfr_dir, egfr_filename)\n",
    "egfr = pd.read_csv(egfr_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "egfr[\"SUBJECT_ID\"] = egfr[\"SUBJECT_ID\"].astype(str)\n",
    "\n",
    "pheno_egfr = pheno.merge(egfr, on = [\"SUBJECT_ID\", \"unique_subject_key\", \"cohort\"], how = \"outer\")\n",
    "print(pheno_egfr.shape)\n",
    "print(duplicates_num(pheno_egfr, \"unique_subject_key\"))\n",
    "pheno_egfr_filename = \"coh03_pheno01_pre.tsv\"\n",
    "pheno_egfr_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "pheno_egfr_dir_filename = os.path.join(pheno_egfr_dir, pheno_egfr_filename)\n",
    "pheno_egfr.to_csv(pheno_egfr_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Deprecated) Merging JHS blood cell traits and ddimer with processed results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_noex_adad_noex_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filename = \"freeze8_anno04_af02_btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex.tsv\"\n",
    "pheno_noex_adad_noex_dir_filename = os.path.join(pheno_noex_adad_noex_dir, pheno_noex_adad_noex_filename)\n",
    "pheno_noex_adad_noex = pd.read_csv(pheno_noex_adad_noex_dir_filename, sep = \"\\t\", header = 0, index_col = None)\n",
    "\n",
    "pheno_dir = os.path.join(\"..\", \"raw_data\", \"phenotype\")\n",
    "jhs_useful_filename = \"jhs_usefule_phenotype_05072019.tsv\"\n",
    "jhs_useful_dir_filename = os.path.join(pheno_dir, jhs_useful_filename)\n",
    "jhs_pheno_useful = pd.read_csv(jhs_useful_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_col = \"NWDID\"\n",
    "df1_col = \"NWDID\"\n",
    "pheno_noex_adad_noex_filljhs = merge_replace_nan(df0_col, df1_col, pheno_noex_adad_noex, jhs_pheno_useful)\n",
    "pheno_noex_adad_noex_filljhs_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "pheno_noex_adad_noex_filljhs_filename = \"freeze8_anno04_af02_btc02-coh03_ddimer-noex_egfr03-ckd_adad01-noex_filljhs.tsv\"\n",
    "pheno_noex_adad_noex_filljhs_dir_filename = os.path.join(pheno_noex_adad_noex_filljhs_dir, pheno_noex_adad_noex_filljhs_filename)\n",
    "pheno_noex_adad_noex_filljhs.to_csv(pheno_noex_adad_noex_filljhs_dir_filename, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D. Preprocess CN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_dir = os.path.join(\"..\", \"raw_data\", \"cn\")\n",
    "cn_filename = \"alpha_globin_calls.txt\"\n",
    "cn_dir_filename = os.path.join(cn_dir, cn_filename)\n",
    "cn = pd.read_csv(cn_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131823, 8)\n",
      "(131003, 8)\n",
      "(130032, 8)\n"
     ]
    }
   ],
   "source": [
    "print(cn.shape)\n",
    "cn_pass = cn.loc[cn.loc[:, \"QC_FAIL\"] == \"QC_PASS\", :]\n",
    "print(cn_pass.shape)\n",
    "cn_pass = cn_pass.loc[cn_pass.loc[:, \"QC_FLAGGED\"] == \"QC_PASS\", :]\n",
    "print(cn_pass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cn_pass)\n",
    "alpha_globin_calls_pass_useful.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_num(cn_pass, \"SAMPLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE</th>\n",
       "      <th>QC_FAIL</th>\n",
       "      <th>QC_FLAGGED</th>\n",
       "      <th>CORRECTED_MEAN</th>\n",
       "      <th>CN_CONFIDENCE</th>\n",
       "      <th>CN</th>\n",
       "      <th>CN_HQ</th>\n",
       "      <th>HYBRID_CN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NWD100011</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.031077</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NWD100014</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.899950</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NWD100018</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.268556</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NWD100027</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.196768</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NWD100028</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>0.066836</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131818</th>\n",
       "      <td>NWD999961</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>0.098204</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131819</th>\n",
       "      <td>NWD999969</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131820</th>\n",
       "      <td>NWD999974</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>0.682218</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131821</th>\n",
       "      <td>NWD999977</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.922117</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131822</th>\n",
       "      <td>NWD999984</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>QC_PASS</td>\n",
       "      <td>-0.092649</td>\n",
       "      <td>HIGHER_CONFIDENCE</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130032 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SAMPLE  QC_FAIL QC_FLAGGED  CORRECTED_MEAN      CN_CONFIDENCE  CN  \\\n",
       "0       NWD100011  QC_PASS    QC_PASS       -0.031077  HIGHER_CONFIDENCE   2   \n",
       "1       NWD100014  QC_PASS    QC_PASS       -0.899950  HIGHER_CONFIDENCE   1   \n",
       "2       NWD100018  QC_PASS    QC_PASS       -0.268556  HIGHER_CONFIDENCE   2   \n",
       "3       NWD100027  QC_PASS    QC_PASS       -0.196768  HIGHER_CONFIDENCE   2   \n",
       "4       NWD100028  QC_PASS    QC_PASS        0.066836  HIGHER_CONFIDENCE   2   \n",
       "...           ...      ...        ...             ...                ...  ..   \n",
       "131818  NWD999961  QC_PASS    QC_PASS        0.098204  HIGHER_CONFIDENCE   2   \n",
       "131819  NWD999969  QC_PASS    QC_PASS       -0.013932  HIGHER_CONFIDENCE   2   \n",
       "131820  NWD999974  QC_PASS    QC_PASS        0.682218  HIGHER_CONFIDENCE   3   \n",
       "131821  NWD999977  QC_PASS    QC_PASS       -0.922117  HIGHER_CONFIDENCE   1   \n",
       "131822  NWD999984  QC_PASS    QC_PASS       -0.092649  HIGHER_CONFIDENCE   2   \n",
       "\n",
       "        CN_HQ  HYBRID_CN  \n",
       "0         2.0        2.0  \n",
       "1         1.0        1.0  \n",
       "2         2.0        2.0  \n",
       "3         2.0        2.0  \n",
       "4         2.0        2.0  \n",
       "...       ...        ...  \n",
       "131818    2.0        2.0  \n",
       "131819    2.0        2.0  \n",
       "131820    3.0        3.0  \n",
       "131821    1.0        1.0  \n",
       "131822    2.0        2.0  \n",
       "\n",
       "[130032 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cn_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_4 = cn_pass.loc[cn_pass.loc[:, \"CN\"] == 4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_pheno_dir = os.path.join(\"..\", \"prepro_data\", \"phenotype\")\n",
    "anno_pheno_filename = \"freeze8_anno05_af02_btc03-coh04_egfr03-ckd_adad01_race_ethnicity_0-2.tsv\"\n",
    "anno_pheno_dir_filename = os.path.join(anno_pheno_dir, anno_pheno_filename)\n",
    "anno_pheno = pd.read_csv(anno_pheno_dir_filename, sep = \"\\t\", header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 66)\n"
     ]
    }
   ],
   "source": [
    "anno_pheno_cn_4 = anno_pheno.merge(cn_4, left_on = \"NWDID\", right_on = \"SAMPLE\", how = \"inner\")\n",
    "print(anno_pheno_cn_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25233, 66)\n"
     ]
    }
   ],
   "source": [
    "anno_pheno_cn = anno_pheno.merge(cn_pass, left_on = \"NWDID\", right_on = \"SAMPLE\", how = \"inner\")\n",
    "print(anno_pheno_cn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWDID</th>\n",
       "      <th>unique_subject_key</th>\n",
       "      <th>consent</th>\n",
       "      <th>exclude</th>\n",
       "      <th>age_at_DDIMER</th>\n",
       "      <th>DDIMER</th>\n",
       "      <th>sample_remove_DDIMER</th>\n",
       "      <th>age_at_hemoglobin_mcnc_bld_1</th>\n",
       "      <th>age_at_hematocrit_vfr_bld_1</th>\n",
       "      <th>age_at_rbc_ncnc_bld_1</th>\n",
       "      <th>...</th>\n",
       "      <th>age_at_CKD</th>\n",
       "      <th>microcytosis</th>\n",
       "      <th>age_at_microcytosis</th>\n",
       "      <th>anemia</th>\n",
       "      <th>age_at_anemia</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>AA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NWD112649</td>\n",
       "      <td>HCHS_SOL_SoL809397</td>\n",
       "      <td>HMB</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.00</td>\n",
       "      <td>SoL809397</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NWD278543</td>\n",
       "      <td>HCHS_SOL_SoL129930</td>\n",
       "      <td>HMB-NPU</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>SoL129930</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NWD579469</td>\n",
       "      <td>HCHS_SOL_SoL379011</td>\n",
       "      <td>HMB-NPU</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>SoL379011</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NWD784564</td>\n",
       "      <td>HCHS_SOL_SoL818760</td>\n",
       "      <td>HMB-NPU</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>SoL818760</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NWD777183</td>\n",
       "      <td>HCHS_SOL_SoL220081</td>\n",
       "      <td>HMB</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SoL220081</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32750</th>\n",
       "      <td>NWD999830</td>\n",
       "      <td>HCHS_SOL_SoL943941</td>\n",
       "      <td>HMB</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>SoL943941</td>\n",
       "      <td>HCHS_SOL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32751</th>\n",
       "      <td>NWD999882</td>\n",
       "      <td>SAFS_DI00071</td>\n",
       "      <td>DS-DHD-IRB-PUB-MDS-RD</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.03</td>\n",
       "      <td>47.03</td>\n",
       "      <td>47.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.03</td>\n",
       "      <td>DI00071</td>\n",
       "      <td>SAFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32752</th>\n",
       "      <td>NWD999922</td>\n",
       "      <td>BioMe_TPMCCDG17054</td>\n",
       "      <td>HMB-NPU</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.00</td>\n",
       "      <td>TPMCCDG17054</td>\n",
       "      <td>BioMe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32753</th>\n",
       "      <td>NWD999925</td>\n",
       "      <td>WHI_700450</td>\n",
       "      <td>HMB-IRB-NPU</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.07</td>\n",
       "      <td>71.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.07</td>\n",
       "      <td>700450</td>\n",
       "      <td>WHI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32754</th>\n",
       "      <td>NWD999984</td>\n",
       "      <td>COPDGene_COPDGene_K96789</td>\n",
       "      <td>HMB</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>57.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.00</td>\n",
       "      <td>COPDGene_K96789</td>\n",
       "      <td>COPDGene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32755 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NWDID        unique_subject_key                consent  exclude  \\\n",
       "0      NWD112649        HCHS_SOL_SoL809397                    HMB    False   \n",
       "1      NWD278543        HCHS_SOL_SoL129930                HMB-NPU    False   \n",
       "2      NWD579469        HCHS_SOL_SoL379011                HMB-NPU    False   \n",
       "3      NWD784564        HCHS_SOL_SoL818760                HMB-NPU    False   \n",
       "4      NWD777183        HCHS_SOL_SoL220081                    HMB    False   \n",
       "...          ...                       ...                    ...      ...   \n",
       "32750  NWD999830        HCHS_SOL_SoL943941                    HMB    False   \n",
       "32751  NWD999882              SAFS_DI00071  DS-DHD-IRB-PUB-MDS-RD    False   \n",
       "32752  NWD999922        BioMe_TPMCCDG17054                HMB-NPU    False   \n",
       "32753  NWD999925                WHI_700450            HMB-IRB-NPU    False   \n",
       "32754  NWD999984  COPDGene_COPDGene_K96789                    HMB    False   \n",
       "\n",
       "       age_at_DDIMER  DDIMER  sample_remove_DDIMER  \\\n",
       "0                NaN     NaN                   NaN   \n",
       "1                NaN     NaN                   NaN   \n",
       "2                NaN     NaN                   NaN   \n",
       "3                NaN     NaN                   NaN   \n",
       "4                NaN     NaN                   NaN   \n",
       "...              ...     ...                   ...   \n",
       "32750            NaN     NaN                   NaN   \n",
       "32751            NaN     NaN                   NaN   \n",
       "32752            NaN     NaN                   NaN   \n",
       "32753            NaN     NaN                   NaN   \n",
       "32754            NaN     NaN                   NaN   \n",
       "\n",
       "       age_at_hemoglobin_mcnc_bld_1  age_at_hematocrit_vfr_bld_1  \\\n",
       "0                             54.00                        54.00   \n",
       "1                             22.00                        22.00   \n",
       "2                             33.00                        33.00   \n",
       "3                             23.00                        23.00   \n",
       "4                             57.00                        57.00   \n",
       "...                             ...                          ...   \n",
       "32750                         57.00                        57.00   \n",
       "32751                         47.03                        47.03   \n",
       "32752                         72.00                        72.00   \n",
       "32753                         71.07                        71.07   \n",
       "32754                         57.00                        57.00   \n",
       "\n",
       "       age_at_rbc_ncnc_bld_1  ...  age_at_CKD  microcytosis  \\\n",
       "0                      54.00  ...        54.0           0.0   \n",
       "1                      22.00  ...        22.0           0.0   \n",
       "2                      33.00  ...        33.0           0.0   \n",
       "3                      23.00  ...        23.0           0.0   \n",
       "4                      57.00  ...        57.0           0.0   \n",
       "...                      ...  ...         ...           ...   \n",
       "32750                  57.00  ...        57.0           0.0   \n",
       "32751                  47.03  ...         NaN           0.0   \n",
       "32752                  72.00  ...         NaN           0.0   \n",
       "32753                    NaN  ...        71.0           NaN   \n",
       "32754                  57.00  ...         NaN           0.0   \n",
       "\n",
       "       age_at_microcytosis  anemia  age_at_anemia       subject_id     study  \\\n",
       "0                    54.00     0.0          54.00        SoL809397  HCHS_SOL   \n",
       "1                    22.00     0.0          22.00        SoL129930  HCHS_SOL   \n",
       "2                    33.00     0.0          33.00        SoL379011  HCHS_SOL   \n",
       "3                    23.00     0.0          23.00        SoL818760  HCHS_SOL   \n",
       "4                    57.00     0.0          57.00        SoL220081  HCHS_SOL   \n",
       "...                    ...     ...            ...              ...       ...   \n",
       "32750                57.00     0.0          57.00        SoL943941  HCHS_SOL   \n",
       "32751                47.03     NaN          47.03          DI00071      SAFS   \n",
       "32752                72.00     NaN          72.00     TPMCCDG17054     BioMe   \n",
       "32753                  NaN     1.0          71.07           700450       WHI   \n",
       "32754                57.00     NaN          57.00  COPDGene_K96789  COPDGene   \n",
       "\n",
       "       sex  ethnicity   AA  \n",
       "0      0.0        1.0  0.0  \n",
       "1      0.0        1.0  0.0  \n",
       "2      1.0        1.0  0.0  \n",
       "3      0.0        1.0  0.0  \n",
       "4      0.0        1.0  0.0  \n",
       "...    ...        ...  ...  \n",
       "32750  0.0        1.0  0.0  \n",
       "32751  0.0        1.0  0.0  \n",
       "32752  0.0        0.0  1.0  \n",
       "32753  0.0        0.0  1.0  \n",
       "32754  1.0        0.0  1.0  \n",
       "\n",
       "[32755 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(anno_pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
